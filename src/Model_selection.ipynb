{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score,make_scorer\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from lightgbm import LGBMClassifier,early_stopping, log_evaluation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier,StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import xgboost as xgb\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"../data\\OUT\\diabetes_train.csv\")\n",
    "df_val = pd.read_csv(r\"../data\\OUT\\diabetes_valid.csv\")\n",
    "df_test = pd.read_csv(r\"../data\\OUT\\diabetes_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pregnancies', 'Glucose', 'SkinThickness', 'Insulin', 'BMI',\n",
       "       'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train-Val-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop('Outcome', axis=1)\n",
    "y_train = df_train['Outcome']\n",
    "\n",
    "X_val = df_val.drop('Outcome', axis=1)\n",
    "y_val = df_val['Outcome']\n",
    "\n",
    "X_test = df_test.drop('Outcome', axis=1)\n",
    "y_test = df_test['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_metrics(model, X_val, y_val, model_name=None, df_metrics=None):\n",
    "    # Prédire sur le jeu de validation\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='weighted')\n",
    "    recall = recall_score(y_val, y_pred, average='weighted')\n",
    "    f1_class1 = f1_score(y_val, y_pred, pos_label=1)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "    # Créer un DataFrame pour les résultats\n",
    "    results = pd.DataFrame([{\n",
    "        'Model': model_name,  # Utiliser le nom du modèle\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score class 1': f1_class1,\n",
    "        'F1 Score global weighted': f1,\n",
    "    }])\n",
    "\n",
    "    # Ajouter les résultats au DataFrame existant (en utilisant pd.concat)\n",
    "    if df_metrics is None:\n",
    "        df_metrics = results\n",
    "    else:\n",
    "        df_metrics = pd.concat([df_metrics, results], ignore_index=True)\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train without balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_val= pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score class 1', 'F1 Score global weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0  logistic_reg  0.804878    0.80196  0.804878          0.707317   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.802499  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\AppData\\Local\\Temp\\ipykernel_36364\\679556832.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metrics = pd.concat([df_metrics, results], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Autre modèle, par exemple un modèle LogisticRegression\n",
    "logistic_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_reg.fit(X_train, y_train)\n",
    "\n",
    "# Appeler à nouveau la fonction pour ajouter les métriques du modèle LogisticRegression\n",
    "df_metrics_val = dataframe_metrics(logistic_reg,X_val, y_val, model_name=\"logistic_reg\", df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0   logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1  decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.802499  \n",
      "1                  0.718675  \n"
     ]
    }
   ],
   "source": [
    "# Premier modèle : Decision Tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Appeler la fonction pour ajouter les métriques du premier modèle\n",
    "df_metrics_val = dataframe_metrics(decision_tree,X_val, y_val, model_name=\"decision_tree\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0   logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1  decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2  random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.802499  \n",
      "1                  0.718675  \n",
      "2                  0.778541  \n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(random_forest,X_val, y_val, model_name=\"random_forest\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0   logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1  decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2  random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3       adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.802499  \n",
      "1                  0.718675  \n",
      "2                  0.778541  \n",
      "3                  0.758723  \n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoostClassifier()\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(adaboost, X_val, y_val, model_name=\"adaboost\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0   logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1  decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2  random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3       adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4        xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.802499  \n",
      "1                  0.718675  \n",
      "2                  0.778541  \n",
      "3                  0.758723  \n",
      "4                  0.729328  \n"
     ]
    }
   ],
   "source": [
    "xgboost = XGBClassifier(eval_metric='logloss')\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(xgboost,X_val, y_val, model_name=\"xgboost\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 171, number of negative: 320\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 522\n",
      "[LightGBM] [Info] Number of data points in the train set: 491, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.348269 -> initscore=-0.626657\n",
      "[LightGBM] [Info] Start training from score -0.626657\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "           Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0   logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1  decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2  random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3       adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4        xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5           lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.802499  \n",
      "1                  0.718675  \n",
      "2                  0.778541  \n",
      "3                  0.758723  \n",
      "4                  0.729328  \n",
      "5                  0.738346  \n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(lgbm, X_val, y_val, model_name=\"lgbm\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0   logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1  decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2  random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3       adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4        xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5           lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6     linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.802499  \n",
      "1                  0.718675  \n",
      "2                  0.778541  \n",
      "3                  0.758723  \n",
      "4                  0.729328  \n",
      "5                  0.738346  \n",
      "6                  0.823837  \n"
     ]
    }
   ],
   "source": [
    "linear_svm = SVC(kernel='linear', probability=True)\n",
    "linear_svm.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(linear_svm, X_val, y_val, model_name=\"linear_svm\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0   logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1  decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2  random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3       adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4        xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5           lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6     linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7            svm  0.780488   0.775918  0.780488          0.649351   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.802499  \n",
      "1                  0.718675  \n",
      "2                  0.778541  \n",
      "3                  0.758723  \n",
      "4                  0.729328  \n",
      "5                  0.738346  \n",
      "6                  0.823837  \n",
      "7                  0.773504  \n"
     ]
    }
   ],
   "source": [
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(svm,X_val, y_val, model_name=\"svm\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0   logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1  decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2  random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3       adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4        xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5           lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6     linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7            svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8            KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.802499  \n",
      "1                  0.718675  \n",
      "2                  0.778541  \n",
      "3                  0.758723  \n",
      "4                  0.729328  \n",
      "5                  0.738346  \n",
      "6                  0.823837  \n",
      "7                  0.773504  \n",
      "8                  0.721993  \n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(KNN,X_val, y_val, model_name=\"KNN\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score class 1</th>\n",
       "      <th>F1 Score global weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear_svm</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.823837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_reg</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.801960</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.802499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.775918</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.773504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.758613</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.758723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.737260</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.738346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.724255</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.718675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.727850</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.729328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.720801</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.721993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
       "6     linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
       "0   logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
       "2  random_forest  0.780488   0.777579  0.780488          0.674699   \n",
       "7            svm  0.780488   0.775918  0.780488          0.649351   \n",
       "3       adaboost  0.764228   0.758613  0.764228          0.632911   \n",
       "5           lgbm  0.739837   0.737260  0.739837          0.619048   \n",
       "1  decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
       "4        xgboost  0.731707   0.727850  0.731707          0.602410   \n",
       "8            KNN  0.723577   0.720801  0.723577          0.595238   \n",
       "\n",
       "   F1 Score global weighted  \n",
       "6                  0.823837  \n",
       "0                  0.802499  \n",
       "2                  0.778541  \n",
       "7                  0.773504  \n",
       "3                  0.758723  \n",
       "5                  0.738346  \n",
       "1                  0.718675  \n",
       "4                  0.729328  \n",
       "8                  0.721993  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_val= df_metrics_val.reset_index(drop=True)\n",
    "df_metrics_val.sort_values(by='F1 Score class 1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0           logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1          decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2          random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3               adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                   lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6             linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                    svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                    KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9  logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.802499  \n",
      "1                  0.718675  \n",
      "2                  0.778541  \n",
      "3                  0.758723  \n",
      "4                  0.729328  \n",
      "5                  0.738346  \n",
      "6                  0.823837  \n",
      "7                  0.773504  \n",
      "8                  0.721993  \n",
      "9                  0.783727  \n"
     ]
    }
   ],
   "source": [
    "# Entraîner une régression logistique avec des poids équilibrés\n",
    "logistic_reg_balanced = LogisticRegression(class_weight='balanced')\n",
    "logistic_reg_balanced.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(logistic_reg_balanced,X_val, y_val, model_name=\"logistic_reg_balanced\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
      "\n",
      "    F1 Score global weighted  \n",
      "0                   0.802499  \n",
      "1                   0.718675  \n",
      "2                   0.778541  \n",
      "3                   0.758723  \n",
      "4                   0.729328  \n",
      "5                   0.738346  \n",
      "6                   0.823837  \n",
      "7                   0.773504  \n",
      "8                   0.721993  \n",
      "9                   0.783727  \n",
      "10                  0.753123  \n"
     ]
    }
   ],
   "source": [
    "decision_tree_balanced = DecisionTreeClassifier(class_weight='balanced')\n",
    "decision_tree_balanced.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(decision_tree_balanced,X_val, y_val, model_name=\"decision_tree_balanced\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
      "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
      "\n",
      "    F1 Score global weighted  \n",
      "0                   0.802499  \n",
      "1                   0.718675  \n",
      "2                   0.778541  \n",
      "3                   0.758723  \n",
      "4                   0.729328  \n",
      "5                   0.738346  \n",
      "6                   0.823837  \n",
      "7                   0.773504  \n",
      "8                   0.721993  \n",
      "9                   0.783727  \n",
      "10                  0.753123  \n",
      "11                  0.778541  \n"
     ]
    }
   ],
   "source": [
    "random_forest_balanced = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "random_forest_balanced.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(random_forest_balanced,X_val, y_val, model_name=\"random_forest_balanced\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
      "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
      "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
      "\n",
      "    F1 Score global weighted  \n",
      "0                   0.802499  \n",
      "1                   0.718675  \n",
      "2                   0.778541  \n",
      "3                   0.758723  \n",
      "4                   0.729328  \n",
      "5                   0.738346  \n",
      "6                   0.823837  \n",
      "7                   0.773504  \n",
      "8                   0.721993  \n",
      "9                   0.783727  \n",
      "10                  0.753123  \n",
      "11                  0.778541  \n",
      "12                  0.756098  \n"
     ]
    }
   ],
   "source": [
    "scale_pos_weight = len(y_train) / (2 * np.bincount(y_train)[1])\n",
    "\n",
    "# Créer un modèle XGBoost avec ajustement des poids de classe\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Classification binaire\n",
    "    'eval_metric': 'logloss',        # Fonction de perte\n",
    "    'scale_pos_weight': scale_pos_weight # Ajustement du poids de la classe positive\n",
    "}\n",
    "\n",
    "xgboost_balanced = XGBClassifier(eval_metric='logloss', scale_pos_weight=scale_pos_weight)\n",
    "xgboost_balanced.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "df_metrics_val = dataframe_metrics(xgboost_balanced,X_val, y_val, model_name=\"xgboost_balanced\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
      "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
      "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
      "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
      "\n",
      "    F1 Score global weighted  \n",
      "0                   0.802499  \n",
      "1                   0.718675  \n",
      "2                   0.778541  \n",
      "3                   0.758723  \n",
      "4                   0.729328  \n",
      "5                   0.738346  \n",
      "6                   0.823837  \n",
      "7                   0.773504  \n",
      "8                   0.721993  \n",
      "9                   0.783727  \n",
      "10                  0.753123  \n",
      "11                  0.778541  \n",
      "12                  0.756098  \n",
      "13                  0.776626  \n"
     ]
    }
   ],
   "source": [
    "sample_weight = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "adaboost_balanced = AdaBoostClassifier(n_estimators=50)\n",
    "\n",
    "# Entraîner le modèle avec les poids des échantillons\n",
    "adaboost_balanced.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(adaboost_balanced,X_val, y_val, model_name=\"adaboost_balanced\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 171, number of negative: 320\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 522\n",
      "[LightGBM] [Info] Number of data points in the train set: 491, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
      "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
      "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
      "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
      "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
      "\n",
      "    F1 Score global weighted  \n",
      "0                   0.802499  \n",
      "1                   0.718675  \n",
      "2                   0.778541  \n",
      "3                   0.758723  \n",
      "4                   0.729328  \n",
      "5                   0.738346  \n",
      "6                   0.823837  \n",
      "7                   0.773504  \n",
      "8                   0.721993  \n",
      "9                   0.783727  \n",
      "10                  0.753123  \n",
      "11                  0.778541  \n",
      "12                  0.756098  \n",
      "13                  0.776626  \n",
      "14                  0.739837  \n"
     ]
    }
   ],
   "source": [
    "lgbm_balanced = LGBMClassifier(class_weight='balanced', n_estimators=100)\n",
    "\n",
    "# Entraîner le modèle\n",
    "lgbm_balanced.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(lgbm_balanced,X_val, y_val, model_name=\"lgbm_balanced\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
      "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
      "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
      "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
      "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
      "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
      "\n",
      "    F1 Score global weighted  \n",
      "0                   0.802499  \n",
      "1                   0.718675  \n",
      "2                   0.778541  \n",
      "3                   0.758723  \n",
      "4                   0.729328  \n",
      "5                   0.738346  \n",
      "6                   0.823837  \n",
      "7                   0.773504  \n",
      "8                   0.721993  \n",
      "9                   0.783727  \n",
      "10                  0.753123  \n",
      "11                  0.778541  \n",
      "12                  0.756098  \n",
      "13                  0.776626  \n",
      "14                  0.739837  \n",
      "15                  0.777565  \n"
     ]
    }
   ],
   "source": [
    "svm_balanced = SVC(class_weight='balanced', probability=True)\n",
    "\n",
    "# Entraîner le modèle\n",
    "svm_balanced.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(svm_balanced,X_val, y_val, model_name=\"svm_balanced\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
      "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
      "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
      "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
      "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
      "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
      "16            knn_balanced  0.715447   0.711274  0.715447          0.578313   \n",
      "\n",
      "    F1 Score global weighted  \n",
      "0                   0.802499  \n",
      "1                   0.718675  \n",
      "2                   0.778541  \n",
      "3                   0.758723  \n",
      "4                   0.729328  \n",
      "5                   0.738346  \n",
      "6                   0.823837  \n",
      "7                   0.773504  \n",
      "8                   0.721993  \n",
      "9                   0.783727  \n",
      "10                  0.753123  \n",
      "11                  0.778541  \n",
      "12                  0.756098  \n",
      "13                  0.776626  \n",
      "14                  0.739837  \n",
      "15                  0.777565  \n",
      "16                  0.712923  \n"
     ]
    }
   ],
   "source": [
    "knn_balanced = KNeighborsClassifier(weights='distance')\n",
    "\n",
    "# Entraîner le modèle\n",
    "knn_balanced.fit(X_train, y_train)\n",
    "\n",
    "df_metrics_val = dataframe_metrics(knn_balanced,X_val, y_val, model_name=\"knn_balanced\",df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score class 1</th>\n",
       "      <th>F1 Score global weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear_svm</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.823837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svm_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.808667</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.777565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logistic_reg_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.792041</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.783727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adaboost_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.791050</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.776626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_reg</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.801960</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.802499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>random_forest_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgboost_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.756098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.775918</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.773504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision_tree_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.751839</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.753123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.758613</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.758723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm_balanced</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.739837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.737260</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.738346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.724255</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.718675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.727850</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.729328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.720801</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.721993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>knn_balanced</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.712923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
       "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
       "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
       "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
       "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
       "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
       "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
       "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
       "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
       "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
       "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
       "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
       "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
       "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
       "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
       "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
       "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
       "16            knn_balanced  0.715447   0.711274  0.715447          0.578313   \n",
       "\n",
       "    F1 Score global weighted  \n",
       "6                   0.823837  \n",
       "15                  0.777565  \n",
       "9                   0.783727  \n",
       "13                  0.776626  \n",
       "0                   0.802499  \n",
       "11                  0.778541  \n",
       "2                   0.778541  \n",
       "12                  0.756098  \n",
       "7                   0.773504  \n",
       "10                  0.753123  \n",
       "3                   0.758723  \n",
       "14                  0.739837  \n",
       "5                   0.738346  \n",
       "1                   0.718675  \n",
       "4                   0.729328  \n",
       "8                   0.721993  \n",
       "16                  0.712923  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_val.sort_values(by='F1 Score class 1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with the best weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids 1: 1.000 - F1 Score: 0.7073\n",
      "Poids 1: 1.001 - F1 Score: 0.7073\n",
      "Poids 1: 1.002 - F1 Score: 0.7073\n",
      "Poids 1: 1.003 - F1 Score: 0.7073\n",
      "Poids 1: 1.004 - F1 Score: 0.7073\n",
      "Poids 1: 1.005 - F1 Score: 0.7073\n",
      "Poids 1: 1.006 - F1 Score: 0.7073\n",
      "Poids 1: 1.007 - F1 Score: 0.7073\n",
      "Poids 1: 1.008 - F1 Score: 0.7073\n",
      "Poids 1: 1.009 - F1 Score: 0.7073\n",
      "Poids 1: 1.010 - F1 Score: 0.7073\n",
      "Poids 1: 1.011 - F1 Score: 0.7073\n",
      "Poids 1: 1.012 - F1 Score: 0.7073\n",
      "Poids 1: 1.013 - F1 Score: 0.7073\n",
      "Poids 1: 1.014 - F1 Score: 0.7073\n",
      "Poids 1: 1.015 - F1 Score: 0.7073\n",
      "Poids 1: 1.016 - F1 Score: 0.7073\n",
      "Poids 1: 1.017 - F1 Score: 0.7073\n",
      "Poids 1: 1.018 - F1 Score: 0.7073\n",
      "Poids 1: 1.019 - F1 Score: 0.7073\n",
      "Poids 1: 1.020 - F1 Score: 0.7073\n",
      "Poids 1: 1.021 - F1 Score: 0.7073\n",
      "Poids 1: 1.022 - F1 Score: 0.7073\n",
      "Poids 1: 1.023 - F1 Score: 0.7073\n",
      "Poids 1: 1.024 - F1 Score: 0.7073\n",
      "Poids 1: 1.025 - F1 Score: 0.7073\n",
      "Poids 1: 1.026 - F1 Score: 0.7073\n",
      "Poids 1: 1.027 - F1 Score: 0.7073\n",
      "Poids 1: 1.028 - F1 Score: 0.7073\n",
      "Poids 1: 1.029 - F1 Score: 0.7073\n",
      "Poids 1: 1.030 - F1 Score: 0.7073\n",
      "Poids 1: 1.031 - F1 Score: 0.7073\n",
      "Poids 1: 1.032 - F1 Score: 0.7073\n",
      "Poids 1: 1.033 - F1 Score: 0.7073\n",
      "Poids 1: 1.034 - F1 Score: 0.7073\n",
      "Poids 1: 1.035 - F1 Score: 0.7073\n",
      "Poids 1: 1.036 - F1 Score: 0.7073\n",
      "Poids 1: 1.037 - F1 Score: 0.7073\n",
      "Poids 1: 1.038 - F1 Score: 0.7073\n",
      "Poids 1: 1.039 - F1 Score: 0.7073\n",
      "Poids 1: 1.040 - F1 Score: 0.7073\n",
      "Poids 1: 1.041 - F1 Score: 0.7073\n",
      "Poids 1: 1.042 - F1 Score: 0.7073\n",
      "Poids 1: 1.043 - F1 Score: 0.7073\n",
      "Poids 1: 1.044 - F1 Score: 0.7073\n",
      "Poids 1: 1.045 - F1 Score: 0.7073\n",
      "Poids 1: 1.046 - F1 Score: 0.7073\n",
      "Poids 1: 1.047 - F1 Score: 0.7073\n",
      "Poids 1: 1.048 - F1 Score: 0.7073\n",
      "Poids 1: 1.049 - F1 Score: 0.7073\n",
      "Poids 1: 1.050 - F1 Score: 0.7073\n",
      "Poids 1: 1.051 - F1 Score: 0.7073\n",
      "Poids 1: 1.052 - F1 Score: 0.7073\n",
      "Poids 1: 1.053 - F1 Score: 0.7073\n",
      "Poids 1: 1.054 - F1 Score: 0.7073\n",
      "Poids 1: 1.055 - F1 Score: 0.7073\n",
      "Poids 1: 1.056 - F1 Score: 0.7073\n",
      "Poids 1: 1.057 - F1 Score: 0.7073\n",
      "Poids 1: 1.058 - F1 Score: 0.7073\n",
      "Poids 1: 1.059 - F1 Score: 0.7073\n",
      "Poids 1: 1.060 - F1 Score: 0.7073\n",
      "Poids 1: 1.061 - F1 Score: 0.7073\n",
      "Poids 1: 1.062 - F1 Score: 0.7073\n",
      "Poids 1: 1.063 - F1 Score: 0.7073\n",
      "Poids 1: 1.064 - F1 Score: 0.7073\n",
      "Poids 1: 1.065 - F1 Score: 0.7073\n",
      "Poids 1: 1.066 - F1 Score: 0.7073\n",
      "Poids 1: 1.067 - F1 Score: 0.7073\n",
      "Poids 1: 1.068 - F1 Score: 0.7073\n",
      "Poids 1: 1.069 - F1 Score: 0.7073\n",
      "Poids 1: 1.070 - F1 Score: 0.7073\n",
      "Poids 1: 1.071 - F1 Score: 0.7073\n",
      "Poids 1: 1.072 - F1 Score: 0.7073\n",
      "Poids 1: 1.073 - F1 Score: 0.7073\n",
      "Poids 1: 1.074 - F1 Score: 0.7073\n",
      "Poids 1: 1.075 - F1 Score: 0.7073\n",
      "Poids 1: 1.076 - F1 Score: 0.7073\n",
      "Poids 1: 1.077 - F1 Score: 0.7073\n",
      "Poids 1: 1.078 - F1 Score: 0.7073\n",
      "Poids 1: 1.079 - F1 Score: 0.7073\n",
      "Poids 1: 1.080 - F1 Score: 0.7073\n",
      "Poids 1: 1.081 - F1 Score: 0.7073\n",
      "Poids 1: 1.082 - F1 Score: 0.7073\n",
      "Poids 1: 1.083 - F1 Score: 0.7073\n",
      "Poids 1: 1.084 - F1 Score: 0.7073\n",
      "Poids 1: 1.085 - F1 Score: 0.7073\n",
      "Poids 1: 1.086 - F1 Score: 0.7073\n",
      "Poids 1: 1.087 - F1 Score: 0.7073\n",
      "Poids 1: 1.088 - F1 Score: 0.7073\n",
      "Poids 1: 1.089 - F1 Score: 0.7073\n",
      "Poids 1: 1.090 - F1 Score: 0.7073\n",
      "Poids 1: 1.091 - F1 Score: 0.7073\n",
      "Poids 1: 1.092 - F1 Score: 0.7073\n",
      "Poids 1: 1.093 - F1 Score: 0.7073\n",
      "Poids 1: 1.094 - F1 Score: 0.7073\n",
      "Poids 1: 1.095 - F1 Score: 0.7073\n",
      "Poids 1: 1.096 - F1 Score: 0.7073\n",
      "Poids 1: 1.097 - F1 Score: 0.7073\n",
      "Poids 1: 1.098 - F1 Score: 0.7073\n",
      "Poids 1: 1.099 - F1 Score: 0.7073\n",
      "Poids 1: 1.100 - F1 Score: 0.7073\n",
      "Poids 1: 1.101 - F1 Score: 0.7229\n",
      "Poids 1: 1.102 - F1 Score: 0.7229\n",
      "Poids 1: 1.103 - F1 Score: 0.7229\n",
      "Poids 1: 1.104 - F1 Score: 0.7229\n",
      "Poids 1: 1.105 - F1 Score: 0.7229\n",
      "Poids 1: 1.106 - F1 Score: 0.7229\n",
      "Poids 1: 1.107 - F1 Score: 0.7229\n",
      "Poids 1: 1.108 - F1 Score: 0.7229\n",
      "Poids 1: 1.109 - F1 Score: 0.7229\n",
      "Poids 1: 1.110 - F1 Score: 0.7229\n",
      "Poids 1: 1.111 - F1 Score: 0.7229\n",
      "Poids 1: 1.112 - F1 Score: 0.7229\n",
      "Poids 1: 1.113 - F1 Score: 0.7229\n",
      "Poids 1: 1.114 - F1 Score: 0.7229\n",
      "Poids 1: 1.115 - F1 Score: 0.7229\n",
      "Poids 1: 1.116 - F1 Score: 0.7229\n",
      "Poids 1: 1.117 - F1 Score: 0.7229\n",
      "Poids 1: 1.118 - F1 Score: 0.7229\n",
      "Poids 1: 1.119 - F1 Score: 0.7229\n",
      "Poids 1: 1.120 - F1 Score: 0.7229\n",
      "Poids 1: 1.121 - F1 Score: 0.7229\n",
      "Poids 1: 1.122 - F1 Score: 0.7229\n",
      "Poids 1: 1.123 - F1 Score: 0.7229\n",
      "Poids 1: 1.124 - F1 Score: 0.7229\n",
      "Poids 1: 1.125 - F1 Score: 0.7229\n",
      "Poids 1: 1.126 - F1 Score: 0.7229\n",
      "Poids 1: 1.127 - F1 Score: 0.7229\n",
      "Poids 1: 1.128 - F1 Score: 0.7229\n",
      "Poids 1: 1.129 - F1 Score: 0.7229\n",
      "Poids 1: 1.130 - F1 Score: 0.7229\n",
      "Poids 1: 1.131 - F1 Score: 0.7229\n",
      "Poids 1: 1.132 - F1 Score: 0.7229\n",
      "Poids 1: 1.133 - F1 Score: 0.7229\n",
      "Poids 1: 1.134 - F1 Score: 0.7229\n",
      "Poids 1: 1.135 - F1 Score: 0.7229\n",
      "Poids 1: 1.136 - F1 Score: 0.7229\n",
      "Poids 1: 1.137 - F1 Score: 0.7229\n",
      "Poids 1: 1.138 - F1 Score: 0.7229\n",
      "Poids 1: 1.139 - F1 Score: 0.7229\n",
      "Poids 1: 1.140 - F1 Score: 0.7229\n",
      "Poids 1: 1.141 - F1 Score: 0.7229\n",
      "Poids 1: 1.142 - F1 Score: 0.7229\n",
      "Poids 1: 1.143 - F1 Score: 0.7229\n",
      "Poids 1: 1.144 - F1 Score: 0.7229\n",
      "Poids 1: 1.145 - F1 Score: 0.7229\n",
      "Poids 1: 1.146 - F1 Score: 0.7229\n",
      "Poids 1: 1.147 - F1 Score: 0.7229\n",
      "Poids 1: 1.148 - F1 Score: 0.7229\n",
      "Poids 1: 1.149 - F1 Score: 0.7229\n",
      "Poids 1: 1.150 - F1 Score: 0.7229\n",
      "Poids 1: 1.151 - F1 Score: 0.7229\n",
      "Poids 1: 1.152 - F1 Score: 0.7229\n",
      "Poids 1: 1.153 - F1 Score: 0.7229\n",
      "Poids 1: 1.154 - F1 Score: 0.7229\n",
      "Poids 1: 1.155 - F1 Score: 0.7229\n",
      "Poids 1: 1.156 - F1 Score: 0.7229\n",
      "Poids 1: 1.157 - F1 Score: 0.7229\n",
      "Poids 1: 1.158 - F1 Score: 0.7229\n",
      "Poids 1: 1.159 - F1 Score: 0.7229\n",
      "Poids 1: 1.160 - F1 Score: 0.7229\n",
      "Poids 1: 1.161 - F1 Score: 0.7229\n",
      "Poids 1: 1.162 - F1 Score: 0.7229\n",
      "Poids 1: 1.163 - F1 Score: 0.7229\n",
      "Poids 1: 1.164 - F1 Score: 0.7229\n",
      "Poids 1: 1.165 - F1 Score: 0.7229\n",
      "Poids 1: 1.166 - F1 Score: 0.7229\n",
      "Poids 1: 1.167 - F1 Score: 0.7229\n",
      "Poids 1: 1.168 - F1 Score: 0.7229\n",
      "Poids 1: 1.169 - F1 Score: 0.7229\n",
      "Poids 1: 1.170 - F1 Score: 0.7229\n",
      "Poids 1: 1.171 - F1 Score: 0.7229\n",
      "Poids 1: 1.172 - F1 Score: 0.7229\n",
      "Poids 1: 1.173 - F1 Score: 0.7229\n",
      "Poids 1: 1.174 - F1 Score: 0.7229\n",
      "Poids 1: 1.175 - F1 Score: 0.7229\n",
      "Poids 1: 1.176 - F1 Score: 0.7229\n",
      "Poids 1: 1.177 - F1 Score: 0.7229\n",
      "Poids 1: 1.178 - F1 Score: 0.7229\n",
      "Poids 1: 1.179 - F1 Score: 0.7229\n",
      "Poids 1: 1.180 - F1 Score: 0.7229\n",
      "Poids 1: 1.181 - F1 Score: 0.7229\n",
      "Poids 1: 1.182 - F1 Score: 0.7229\n",
      "Poids 1: 1.183 - F1 Score: 0.7229\n",
      "Poids 1: 1.184 - F1 Score: 0.7229\n",
      "Poids 1: 1.185 - F1 Score: 0.7229\n",
      "Poids 1: 1.186 - F1 Score: 0.7229\n",
      "Poids 1: 1.187 - F1 Score: 0.7229\n",
      "Poids 1: 1.188 - F1 Score: 0.7229\n",
      "Poids 1: 1.189 - F1 Score: 0.7229\n",
      "Poids 1: 1.190 - F1 Score: 0.7229\n",
      "Poids 1: 1.191 - F1 Score: 0.7229\n",
      "Poids 1: 1.192 - F1 Score: 0.7229\n",
      "Poids 1: 1.193 - F1 Score: 0.7229\n",
      "Poids 1: 1.194 - F1 Score: 0.7229\n",
      "Poids 1: 1.195 - F1 Score: 0.7229\n",
      "Poids 1: 1.196 - F1 Score: 0.7229\n",
      "Poids 1: 1.197 - F1 Score: 0.7229\n",
      "Poids 1: 1.198 - F1 Score: 0.7229\n",
      "Poids 1: 1.199 - F1 Score: 0.7229\n",
      "Poids 1: 1.200 - F1 Score: 0.7229\n",
      "Poids 1: 1.201 - F1 Score: 0.7229\n",
      "Poids 1: 1.202 - F1 Score: 0.7229\n",
      "Poids 1: 1.203 - F1 Score: 0.7229\n",
      "Poids 1: 1.204 - F1 Score: 0.7229\n",
      "Poids 1: 1.205 - F1 Score: 0.7229\n",
      "Poids 1: 1.206 - F1 Score: 0.7229\n",
      "Poids 1: 1.207 - F1 Score: 0.7229\n",
      "Poids 1: 1.208 - F1 Score: 0.7229\n",
      "Poids 1: 1.209 - F1 Score: 0.7229\n",
      "Poids 1: 1.210 - F1 Score: 0.7229\n",
      "Poids 1: 1.211 - F1 Score: 0.7229\n",
      "Poids 1: 1.212 - F1 Score: 0.7229\n",
      "Poids 1: 1.213 - F1 Score: 0.7229\n",
      "Poids 1: 1.214 - F1 Score: 0.7229\n",
      "Poids 1: 1.215 - F1 Score: 0.7229\n",
      "Poids 1: 1.216 - F1 Score: 0.7229\n",
      "Poids 1: 1.217 - F1 Score: 0.7229\n",
      "Poids 1: 1.218 - F1 Score: 0.7229\n",
      "Poids 1: 1.219 - F1 Score: 0.7229\n",
      "Poids 1: 1.220 - F1 Score: 0.7229\n",
      "Poids 1: 1.221 - F1 Score: 0.7229\n",
      "Poids 1: 1.222 - F1 Score: 0.7229\n",
      "Poids 1: 1.223 - F1 Score: 0.7229\n",
      "Poids 1: 1.224 - F1 Score: 0.7229\n",
      "Poids 1: 1.225 - F1 Score: 0.7229\n",
      "Poids 1: 1.226 - F1 Score: 0.7229\n",
      "Poids 1: 1.227 - F1 Score: 0.7229\n",
      "Poids 1: 1.228 - F1 Score: 0.7229\n",
      "Poids 1: 1.229 - F1 Score: 0.7229\n",
      "Poids 1: 1.230 - F1 Score: 0.7229\n",
      "Poids 1: 1.231 - F1 Score: 0.7229\n",
      "Poids 1: 1.232 - F1 Score: 0.7229\n",
      "Poids 1: 1.233 - F1 Score: 0.7229\n",
      "Poids 1: 1.234 - F1 Score: 0.7229\n",
      "Poids 1: 1.235 - F1 Score: 0.7229\n",
      "Poids 1: 1.236 - F1 Score: 0.7229\n",
      "Poids 1: 1.237 - F1 Score: 0.7229\n",
      "Poids 1: 1.238 - F1 Score: 0.7229\n",
      "Poids 1: 1.239 - F1 Score: 0.7229\n",
      "Poids 1: 1.240 - F1 Score: 0.7229\n",
      "Poids 1: 1.241 - F1 Score: 0.7229\n",
      "Poids 1: 1.242 - F1 Score: 0.7229\n",
      "Poids 1: 1.243 - F1 Score: 0.7229\n",
      "Poids 1: 1.244 - F1 Score: 0.7229\n",
      "Poids 1: 1.245 - F1 Score: 0.7229\n",
      "Poids 1: 1.246 - F1 Score: 0.7229\n",
      "Poids 1: 1.247 - F1 Score: 0.7229\n",
      "Poids 1: 1.248 - F1 Score: 0.7229\n",
      "Poids 1: 1.249 - F1 Score: 0.7229\n",
      "Poids 1: 1.250 - F1 Score: 0.7229\n",
      "Poids 1: 1.251 - F1 Score: 0.7229\n",
      "Poids 1: 1.252 - F1 Score: 0.7381\n",
      "Poids 1: 1.253 - F1 Score: 0.7381\n",
      "Poids 1: 1.254 - F1 Score: 0.7381\n",
      "Poids 1: 1.255 - F1 Score: 0.7381\n",
      "Poids 1: 1.256 - F1 Score: 0.7381\n",
      "Poids 1: 1.257 - F1 Score: 0.7381\n",
      "Poids 1: 1.258 - F1 Score: 0.7381\n",
      "Poids 1: 1.259 - F1 Score: 0.7381\n",
      "Poids 1: 1.260 - F1 Score: 0.7381\n",
      "Poids 1: 1.261 - F1 Score: 0.7381\n",
      "Poids 1: 1.262 - F1 Score: 0.7381\n",
      "Poids 1: 1.263 - F1 Score: 0.7381\n",
      "Poids 1: 1.264 - F1 Score: 0.7381\n",
      "Poids 1: 1.265 - F1 Score: 0.7381\n",
      "Poids 1: 1.266 - F1 Score: 0.7381\n",
      "Poids 1: 1.267 - F1 Score: 0.7381\n",
      "Poids 1: 1.268 - F1 Score: 0.7381\n",
      "Poids 1: 1.269 - F1 Score: 0.7381\n",
      "Poids 1: 1.270 - F1 Score: 0.7381\n",
      "Poids 1: 1.271 - F1 Score: 0.7381\n",
      "Poids 1: 1.272 - F1 Score: 0.7381\n",
      "Poids 1: 1.273 - F1 Score: 0.7381\n",
      "Poids 1: 1.274 - F1 Score: 0.7381\n",
      "Poids 1: 1.275 - F1 Score: 0.7381\n",
      "Poids 1: 1.276 - F1 Score: 0.7381\n",
      "Poids 1: 1.277 - F1 Score: 0.7381\n",
      "Poids 1: 1.278 - F1 Score: 0.7381\n",
      "Poids 1: 1.279 - F1 Score: 0.7381\n",
      "Poids 1: 1.280 - F1 Score: 0.7381\n",
      "Poids 1: 1.281 - F1 Score: 0.7381\n",
      "Poids 1: 1.282 - F1 Score: 0.7381\n",
      "Poids 1: 1.283 - F1 Score: 0.7381\n",
      "Poids 1: 1.284 - F1 Score: 0.7381\n",
      "Poids 1: 1.285 - F1 Score: 0.7381\n",
      "Poids 1: 1.286 - F1 Score: 0.7381\n",
      "Poids 1: 1.287 - F1 Score: 0.7381\n",
      "Poids 1: 1.288 - F1 Score: 0.7381\n",
      "Poids 1: 1.289 - F1 Score: 0.7381\n",
      "Poids 1: 1.290 - F1 Score: 0.7381\n",
      "Poids 1: 1.291 - F1 Score: 0.7381\n",
      "Poids 1: 1.292 - F1 Score: 0.7381\n",
      "Poids 1: 1.293 - F1 Score: 0.7381\n",
      "Poids 1: 1.294 - F1 Score: 0.7381\n",
      "Poids 1: 1.295 - F1 Score: 0.7381\n",
      "Poids 1: 1.296 - F1 Score: 0.7381\n",
      "Poids 1: 1.297 - F1 Score: 0.7381\n",
      "Poids 1: 1.298 - F1 Score: 0.7381\n",
      "Poids 1: 1.299 - F1 Score: 0.7381\n",
      "Poids 1: 1.300 - F1 Score: 0.7381\n",
      "Poids 1: 1.301 - F1 Score: 0.7381\n",
      "Poids 1: 1.302 - F1 Score: 0.7381\n",
      "Poids 1: 1.303 - F1 Score: 0.7381\n",
      "Poids 1: 1.304 - F1 Score: 0.7381\n",
      "Poids 1: 1.305 - F1 Score: 0.7381\n",
      "Poids 1: 1.306 - F1 Score: 0.7381\n",
      "Poids 1: 1.307 - F1 Score: 0.7381\n",
      "Poids 1: 1.308 - F1 Score: 0.7381\n",
      "Poids 1: 1.309 - F1 Score: 0.7381\n",
      "Poids 1: 1.310 - F1 Score: 0.7381\n",
      "Poids 1: 1.311 - F1 Score: 0.7381\n",
      "Poids 1: 1.312 - F1 Score: 0.7381\n",
      "Poids 1: 1.313 - F1 Score: 0.7381\n",
      "Poids 1: 1.314 - F1 Score: 0.7381\n",
      "Poids 1: 1.315 - F1 Score: 0.7381\n",
      "Poids 1: 1.316 - F1 Score: 0.7381\n",
      "Poids 1: 1.317 - F1 Score: 0.7381\n",
      "Poids 1: 1.318 - F1 Score: 0.7294\n",
      "Poids 1: 1.319 - F1 Score: 0.7294\n",
      "Poids 1: 1.320 - F1 Score: 0.7294\n",
      "Poids 1: 1.321 - F1 Score: 0.7294\n",
      "Poids 1: 1.322 - F1 Score: 0.7294\n",
      "Poids 1: 1.323 - F1 Score: 0.7294\n",
      "Poids 1: 1.324 - F1 Score: 0.7294\n",
      "Poids 1: 1.325 - F1 Score: 0.7294\n",
      "Poids 1: 1.326 - F1 Score: 0.7294\n",
      "Poids 1: 1.327 - F1 Score: 0.7294\n",
      "Poids 1: 1.328 - F1 Score: 0.7294\n",
      "Poids 1: 1.329 - F1 Score: 0.7294\n",
      "Poids 1: 1.330 - F1 Score: 0.7294\n",
      "Poids 1: 1.331 - F1 Score: 0.7294\n",
      "Poids 1: 1.332 - F1 Score: 0.7294\n",
      "Poids 1: 1.333 - F1 Score: 0.7294\n",
      "Poids 1: 1.334 - F1 Score: 0.7294\n",
      "Poids 1: 1.335 - F1 Score: 0.7294\n",
      "Poids 1: 1.336 - F1 Score: 0.7294\n",
      "Poids 1: 1.337 - F1 Score: 0.7294\n",
      "Poids 1: 1.338 - F1 Score: 0.7294\n",
      "Poids 1: 1.339 - F1 Score: 0.7294\n",
      "Poids 1: 1.340 - F1 Score: 0.7294\n",
      "Poids 1: 1.341 - F1 Score: 0.7294\n",
      "Poids 1: 1.342 - F1 Score: 0.7294\n",
      "Poids 1: 1.343 - F1 Score: 0.7294\n",
      "Poids 1: 1.344 - F1 Score: 0.7294\n",
      "Poids 1: 1.345 - F1 Score: 0.7294\n",
      "Poids 1: 1.346 - F1 Score: 0.7294\n",
      "Poids 1: 1.347 - F1 Score: 0.7294\n",
      "Poids 1: 1.348 - F1 Score: 0.7294\n",
      "Poids 1: 1.349 - F1 Score: 0.7294\n",
      "Poids 1: 1.350 - F1 Score: 0.7294\n",
      "Poids 1: 1.351 - F1 Score: 0.7294\n",
      "Poids 1: 1.352 - F1 Score: 0.7294\n",
      "Poids 1: 1.353 - F1 Score: 0.7294\n",
      "Poids 1: 1.354 - F1 Score: 0.7294\n",
      "Poids 1: 1.355 - F1 Score: 0.7294\n",
      "Poids 1: 1.356 - F1 Score: 0.7294\n",
      "Poids 1: 1.357 - F1 Score: 0.7294\n",
      "Poids 1: 1.358 - F1 Score: 0.7294\n",
      "Poids 1: 1.359 - F1 Score: 0.7294\n",
      "Poids 1: 1.360 - F1 Score: 0.7294\n",
      "Poids 1: 1.361 - F1 Score: 0.7294\n",
      "Poids 1: 1.362 - F1 Score: 0.7294\n",
      "Poids 1: 1.363 - F1 Score: 0.7294\n",
      "Poids 1: 1.364 - F1 Score: 0.7294\n",
      "Poids 1: 1.365 - F1 Score: 0.7294\n",
      "Poids 1: 1.366 - F1 Score: 0.7294\n",
      "Poids 1: 1.367 - F1 Score: 0.7294\n",
      "Poids 1: 1.368 - F1 Score: 0.7294\n",
      "Poids 1: 1.369 - F1 Score: 0.7294\n",
      "Poids 1: 1.370 - F1 Score: 0.7294\n",
      "Poids 1: 1.371 - F1 Score: 0.7294\n",
      "Poids 1: 1.372 - F1 Score: 0.7294\n",
      "Poids 1: 1.373 - F1 Score: 0.7294\n",
      "Poids 1: 1.374 - F1 Score: 0.7294\n",
      "Poids 1: 1.375 - F1 Score: 0.7294\n",
      "Poids 1: 1.376 - F1 Score: 0.7294\n",
      "Poids 1: 1.377 - F1 Score: 0.7294\n",
      "Poids 1: 1.378 - F1 Score: 0.7294\n",
      "Poids 1: 1.379 - F1 Score: 0.7294\n",
      "Poids 1: 1.380 - F1 Score: 0.7294\n",
      "Poids 1: 1.381 - F1 Score: 0.7294\n",
      "Poids 1: 1.382 - F1 Score: 0.7294\n",
      "Poids 1: 1.383 - F1 Score: 0.7294\n",
      "Poids 1: 1.384 - F1 Score: 0.7294\n",
      "Poids 1: 1.385 - F1 Score: 0.7294\n",
      "Poids 1: 1.386 - F1 Score: 0.7294\n",
      "Poids 1: 1.387 - F1 Score: 0.7209\n",
      "Poids 1: 1.388 - F1 Score: 0.7209\n",
      "Poids 1: 1.389 - F1 Score: 0.7356\n",
      "Poids 1: 1.390 - F1 Score: 0.7356\n",
      "Poids 1: 1.391 - F1 Score: 0.7356\n",
      "Poids 1: 1.392 - F1 Score: 0.7356\n",
      "Poids 1: 1.393 - F1 Score: 0.7356\n",
      "Poids 1: 1.394 - F1 Score: 0.7356\n",
      "Poids 1: 1.395 - F1 Score: 0.7356\n",
      "Poids 1: 1.396 - F1 Score: 0.7356\n",
      "Poids 1: 1.397 - F1 Score: 0.7356\n",
      "Poids 1: 1.398 - F1 Score: 0.7356\n",
      "Poids 1: 1.399 - F1 Score: 0.7356\n",
      "Poids 1: 1.400 - F1 Score: 0.7356\n",
      "Poids 1: 1.401 - F1 Score: 0.7356\n",
      "Poids 1: 1.402 - F1 Score: 0.7356\n",
      "Poids 1: 1.403 - F1 Score: 0.7356\n",
      "Poids 1: 1.404 - F1 Score: 0.7356\n",
      "Poids 1: 1.405 - F1 Score: 0.7356\n",
      "Poids 1: 1.406 - F1 Score: 0.7356\n",
      "Poids 1: 1.407 - F1 Score: 0.7356\n",
      "Poids 1: 1.408 - F1 Score: 0.7356\n",
      "Poids 1: 1.409 - F1 Score: 0.7356\n",
      "Poids 1: 1.410 - F1 Score: 0.7356\n",
      "Poids 1: 1.411 - F1 Score: 0.7356\n",
      "Poids 1: 1.412 - F1 Score: 0.7356\n",
      "Poids 1: 1.413 - F1 Score: 0.7356\n",
      "Poids 1: 1.414 - F1 Score: 0.7356\n",
      "Poids 1: 1.415 - F1 Score: 0.7356\n",
      "Poids 1: 1.416 - F1 Score: 0.7356\n",
      "Poids 1: 1.417 - F1 Score: 0.7356\n",
      "Poids 1: 1.418 - F1 Score: 0.7356\n",
      "Poids 1: 1.419 - F1 Score: 0.7356\n",
      "Poids 1: 1.420 - F1 Score: 0.7356\n",
      "Poids 1: 1.421 - F1 Score: 0.7356\n",
      "Poids 1: 1.422 - F1 Score: 0.7356\n",
      "Poids 1: 1.423 - F1 Score: 0.7356\n",
      "Poids 1: 1.424 - F1 Score: 0.7356\n",
      "Poids 1: 1.425 - F1 Score: 0.7356\n",
      "Poids 1: 1.426 - F1 Score: 0.7273\n",
      "Poids 1: 1.427 - F1 Score: 0.7273\n",
      "Poids 1: 1.428 - F1 Score: 0.7273\n",
      "Poids 1: 1.429 - F1 Score: 0.7273\n",
      "Poids 1: 1.430 - F1 Score: 0.7273\n",
      "Poids 1: 1.431 - F1 Score: 0.7273\n",
      "Poids 1: 1.432 - F1 Score: 0.7273\n",
      "Poids 1: 1.433 - F1 Score: 0.7273\n",
      "Poids 1: 1.434 - F1 Score: 0.7273\n",
      "Poids 1: 1.435 - F1 Score: 0.7273\n",
      "Poids 1: 1.436 - F1 Score: 0.7273\n",
      "Poids 1: 1.437 - F1 Score: 0.7273\n",
      "Poids 1: 1.438 - F1 Score: 0.7273\n",
      "Poids 1: 1.439 - F1 Score: 0.7273\n",
      "Poids 1: 1.440 - F1 Score: 0.7273\n",
      "Poids 1: 1.441 - F1 Score: 0.7273\n",
      "Poids 1: 1.442 - F1 Score: 0.7273\n",
      "Poids 1: 1.443 - F1 Score: 0.7273\n",
      "Poids 1: 1.444 - F1 Score: 0.7273\n",
      "Poids 1: 1.445 - F1 Score: 0.7273\n",
      "Poids 1: 1.446 - F1 Score: 0.7273\n",
      "Poids 1: 1.447 - F1 Score: 0.7273\n",
      "Poids 1: 1.448 - F1 Score: 0.7273\n",
      "Poids 1: 1.449 - F1 Score: 0.7273\n",
      "Poids 1: 1.450 - F1 Score: 0.7273\n",
      "Poids 1: 1.451 - F1 Score: 0.7273\n",
      "Poids 1: 1.452 - F1 Score: 0.7273\n",
      "Poids 1: 1.453 - F1 Score: 0.7273\n",
      "Poids 1: 1.454 - F1 Score: 0.7273\n",
      "Poids 1: 1.455 - F1 Score: 0.7273\n",
      "Poids 1: 1.456 - F1 Score: 0.7273\n",
      "Poids 1: 1.457 - F1 Score: 0.7273\n",
      "Poids 1: 1.458 - F1 Score: 0.7273\n",
      "Poids 1: 1.459 - F1 Score: 0.7273\n",
      "Poids 1: 1.460 - F1 Score: 0.7273\n",
      "Poids 1: 1.461 - F1 Score: 0.7273\n",
      "Poids 1: 1.462 - F1 Score: 0.7273\n",
      "Poids 1: 1.463 - F1 Score: 0.7273\n",
      "Poids 1: 1.464 - F1 Score: 0.7273\n",
      "Poids 1: 1.465 - F1 Score: 0.7273\n",
      "Poids 1: 1.466 - F1 Score: 0.7273\n",
      "Poids 1: 1.467 - F1 Score: 0.7273\n",
      "Poids 1: 1.468 - F1 Score: 0.7273\n",
      "Poids 1: 1.469 - F1 Score: 0.7273\n",
      "Poids 1: 1.470 - F1 Score: 0.7273\n",
      "Poids 1: 1.471 - F1 Score: 0.7191\n",
      "Poids 1: 1.472 - F1 Score: 0.7191\n",
      "Poids 1: 1.473 - F1 Score: 0.7191\n",
      "Poids 1: 1.474 - F1 Score: 0.7191\n",
      "Poids 1: 1.475 - F1 Score: 0.7191\n",
      "Poids 1: 1.476 - F1 Score: 0.7191\n",
      "Poids 1: 1.477 - F1 Score: 0.7191\n",
      "Poids 1: 1.478 - F1 Score: 0.7191\n",
      "Poids 1: 1.479 - F1 Score: 0.7191\n",
      "Poids 1: 1.480 - F1 Score: 0.7191\n",
      "Poids 1: 1.481 - F1 Score: 0.7191\n",
      "Poids 1: 1.482 - F1 Score: 0.7191\n",
      "Poids 1: 1.483 - F1 Score: 0.7191\n",
      "Poids 1: 1.484 - F1 Score: 0.7191\n",
      "Poids 1: 1.485 - F1 Score: 0.7191\n",
      "Poids 1: 1.486 - F1 Score: 0.7191\n",
      "Poids 1: 1.487 - F1 Score: 0.7191\n",
      "Poids 1: 1.488 - F1 Score: 0.7191\n",
      "Poids 1: 1.489 - F1 Score: 0.7191\n",
      "Poids 1: 1.490 - F1 Score: 0.7191\n",
      "Poids 1: 1.491 - F1 Score: 0.7191\n",
      "Poids 1: 1.492 - F1 Score: 0.7191\n",
      "Poids 1: 1.493 - F1 Score: 0.7191\n",
      "Poids 1: 1.494 - F1 Score: 0.7191\n",
      "Poids 1: 1.495 - F1 Score: 0.7191\n",
      "Poids 1: 1.496 - F1 Score: 0.7191\n",
      "Poids 1: 1.497 - F1 Score: 0.7191\n",
      "Poids 1: 1.498 - F1 Score: 0.7191\n",
      "Poids 1: 1.499 - F1 Score: 0.7191\n",
      "Poids 1: 1.500 - F1 Score: 0.7191\n",
      "Poids 1: 1.501 - F1 Score: 0.7191\n",
      "Poids 1: 1.502 - F1 Score: 0.7191\n",
      "Poids 1: 1.503 - F1 Score: 0.7191\n",
      "Poids 1: 1.504 - F1 Score: 0.7191\n",
      "Poids 1: 1.505 - F1 Score: 0.7191\n",
      "Poids 1: 1.506 - F1 Score: 0.7191\n",
      "Poids 1: 1.507 - F1 Score: 0.7191\n",
      "Poids 1: 1.508 - F1 Score: 0.7191\n",
      "Poids 1: 1.509 - F1 Score: 0.7191\n",
      "Poids 1: 1.510 - F1 Score: 0.7191\n",
      "Poids 1: 1.511 - F1 Score: 0.7191\n",
      "Poids 1: 1.512 - F1 Score: 0.7191\n",
      "Poids 1: 1.513 - F1 Score: 0.7191\n",
      "Poids 1: 1.514 - F1 Score: 0.7191\n",
      "Poids 1: 1.515 - F1 Score: 0.7191\n",
      "Poids 1: 1.516 - F1 Score: 0.7191\n",
      "Poids 1: 1.517 - F1 Score: 0.7191\n",
      "Poids 1: 1.518 - F1 Score: 0.7191\n",
      "Poids 1: 1.519 - F1 Score: 0.7191\n",
      "Poids 1: 1.520 - F1 Score: 0.7191\n",
      "Poids 1: 1.521 - F1 Score: 0.7191\n",
      "Poids 1: 1.522 - F1 Score: 0.7191\n",
      "Poids 1: 1.523 - F1 Score: 0.7191\n",
      "Poids 1: 1.524 - F1 Score: 0.7191\n",
      "Poids 1: 1.525 - F1 Score: 0.7191\n",
      "Poids 1: 1.526 - F1 Score: 0.7191\n",
      "Poids 1: 1.527 - F1 Score: 0.7191\n",
      "Poids 1: 1.528 - F1 Score: 0.7191\n",
      "Poids 1: 1.529 - F1 Score: 0.7191\n",
      "Poids 1: 1.530 - F1 Score: 0.7191\n",
      "Poids 1: 1.531 - F1 Score: 0.7191\n",
      "Poids 1: 1.532 - F1 Score: 0.7191\n",
      "Poids 1: 1.533 - F1 Score: 0.7191\n",
      "Poids 1: 1.534 - F1 Score: 0.7191\n",
      "Poids 1: 1.535 - F1 Score: 0.7191\n",
      "Poids 1: 1.536 - F1 Score: 0.7191\n",
      "Poids 1: 1.537 - F1 Score: 0.7191\n",
      "Poids 1: 1.538 - F1 Score: 0.7191\n",
      "Poids 1: 1.539 - F1 Score: 0.7191\n",
      "Poids 1: 1.540 - F1 Score: 0.7191\n",
      "Poids 1: 1.541 - F1 Score: 0.7191\n",
      "Poids 1: 1.542 - F1 Score: 0.7191\n",
      "Poids 1: 1.543 - F1 Score: 0.7191\n",
      "Poids 1: 1.544 - F1 Score: 0.7191\n",
      "Poids 1: 1.545 - F1 Score: 0.7191\n",
      "Poids 1: 1.546 - F1 Score: 0.7191\n",
      "Poids 1: 1.547 - F1 Score: 0.7191\n",
      "Poids 1: 1.548 - F1 Score: 0.7191\n",
      "Poids 1: 1.549 - F1 Score: 0.7191\n",
      "Poids 1: 1.550 - F1 Score: 0.7191\n",
      "Poids 1: 1.551 - F1 Score: 0.7191\n",
      "Poids 1: 1.552 - F1 Score: 0.7191\n",
      "Poids 1: 1.553 - F1 Score: 0.7191\n",
      "Poids 1: 1.554 - F1 Score: 0.7191\n",
      "Poids 1: 1.555 - F1 Score: 0.7191\n",
      "Poids 1: 1.556 - F1 Score: 0.7191\n",
      "Poids 1: 1.557 - F1 Score: 0.7191\n",
      "Poids 1: 1.558 - F1 Score: 0.7191\n",
      "Poids 1: 1.559 - F1 Score: 0.7191\n",
      "Poids 1: 1.560 - F1 Score: 0.7191\n",
      "Poids 1: 1.561 - F1 Score: 0.7191\n",
      "Poids 1: 1.562 - F1 Score: 0.7191\n",
      "Poids 1: 1.563 - F1 Score: 0.7191\n",
      "Poids 1: 1.564 - F1 Score: 0.7191\n",
      "Poids 1: 1.565 - F1 Score: 0.7191\n",
      "Poids 1: 1.566 - F1 Score: 0.7191\n",
      "Poids 1: 1.567 - F1 Score: 0.7191\n",
      "Poids 1: 1.568 - F1 Score: 0.7191\n",
      "Poids 1: 1.569 - F1 Score: 0.7191\n",
      "Poids 1: 1.570 - F1 Score: 0.7191\n",
      "Poids 1: 1.571 - F1 Score: 0.7191\n",
      "Poids 1: 1.572 - F1 Score: 0.7191\n",
      "Poids 1: 1.573 - F1 Score: 0.7191\n",
      "Poids 1: 1.574 - F1 Score: 0.7191\n",
      "Poids 1: 1.575 - F1 Score: 0.7191\n",
      "Poids 1: 1.576 - F1 Score: 0.7191\n",
      "Poids 1: 1.577 - F1 Score: 0.7191\n",
      "Poids 1: 1.578 - F1 Score: 0.7191\n",
      "Poids 1: 1.579 - F1 Score: 0.7111\n",
      "Poids 1: 1.580 - F1 Score: 0.7111\n",
      "Poids 1: 1.581 - F1 Score: 0.7033\n",
      "Poids 1: 1.582 - F1 Score: 0.7033\n",
      "Poids 1: 1.583 - F1 Score: 0.7033\n",
      "Poids 1: 1.584 - F1 Score: 0.7033\n",
      "Poids 1: 1.585 - F1 Score: 0.7033\n",
      "Poids 1: 1.586 - F1 Score: 0.7033\n",
      "Poids 1: 1.587 - F1 Score: 0.7033\n",
      "Poids 1: 1.588 - F1 Score: 0.7033\n",
      "Poids 1: 1.589 - F1 Score: 0.7033\n",
      "Poids 1: 1.590 - F1 Score: 0.7033\n",
      "Poids 1: 1.591 - F1 Score: 0.7033\n",
      "Poids 1: 1.592 - F1 Score: 0.7033\n",
      "Poids 1: 1.593 - F1 Score: 0.7033\n",
      "Poids 1: 1.594 - F1 Score: 0.7033\n",
      "Poids 1: 1.595 - F1 Score: 0.7033\n",
      "Poids 1: 1.596 - F1 Score: 0.7033\n",
      "Poids 1: 1.597 - F1 Score: 0.7033\n",
      "Poids 1: 1.598 - F1 Score: 0.7033\n",
      "Poids 1: 1.599 - F1 Score: 0.7033\n",
      "Poids 1: 1.600 - F1 Score: 0.7033\n",
      "Poids 1: 1.601 - F1 Score: 0.7033\n",
      "Poids 1: 1.602 - F1 Score: 0.7033\n",
      "Poids 1: 1.603 - F1 Score: 0.7033\n",
      "Poids 1: 1.604 - F1 Score: 0.7033\n",
      "Poids 1: 1.605 - F1 Score: 0.7033\n",
      "Poids 1: 1.606 - F1 Score: 0.7033\n",
      "Poids 1: 1.607 - F1 Score: 0.7033\n",
      "Poids 1: 1.608 - F1 Score: 0.6957\n",
      "Poids 1: 1.609 - F1 Score: 0.6957\n",
      "Poids 1: 1.610 - F1 Score: 0.6957\n",
      "Poids 1: 1.611 - F1 Score: 0.6957\n",
      "Poids 1: 1.612 - F1 Score: 0.6957\n",
      "Poids 1: 1.613 - F1 Score: 0.6957\n",
      "Poids 1: 1.614 - F1 Score: 0.6957\n",
      "Poids 1: 1.615 - F1 Score: 0.6957\n",
      "Poids 1: 1.616 - F1 Score: 0.6957\n",
      "Poids 1: 1.617 - F1 Score: 0.6957\n",
      "Poids 1: 1.618 - F1 Score: 0.6957\n",
      "Poids 1: 1.619 - F1 Score: 0.6957\n",
      "Poids 1: 1.620 - F1 Score: 0.6957\n",
      "Poids 1: 1.621 - F1 Score: 0.6957\n",
      "Poids 1: 1.622 - F1 Score: 0.6957\n",
      "Poids 1: 1.623 - F1 Score: 0.6957\n",
      "Poids 1: 1.624 - F1 Score: 0.6957\n",
      "Poids 1: 1.625 - F1 Score: 0.6957\n",
      "Poids 1: 1.626 - F1 Score: 0.6957\n",
      "Poids 1: 1.627 - F1 Score: 0.6957\n",
      "Poids 1: 1.628 - F1 Score: 0.6957\n",
      "Poids 1: 1.629 - F1 Score: 0.6957\n",
      "Poids 1: 1.630 - F1 Score: 0.6957\n",
      "Poids 1: 1.631 - F1 Score: 0.6957\n",
      "Poids 1: 1.632 - F1 Score: 0.6957\n",
      "Poids 1: 1.633 - F1 Score: 0.6957\n",
      "Poids 1: 1.634 - F1 Score: 0.7097\n",
      "Poids 1: 1.635 - F1 Score: 0.7097\n",
      "Poids 1: 1.636 - F1 Score: 0.7097\n",
      "Poids 1: 1.637 - F1 Score: 0.7097\n",
      "Poids 1: 1.638 - F1 Score: 0.7097\n",
      "Poids 1: 1.639 - F1 Score: 0.7097\n",
      "Poids 1: 1.640 - F1 Score: 0.7097\n",
      "Poids 1: 1.641 - F1 Score: 0.7097\n",
      "Poids 1: 1.642 - F1 Score: 0.7097\n",
      "Poids 1: 1.643 - F1 Score: 0.7097\n",
      "Poids 1: 1.644 - F1 Score: 0.7097\n",
      "Poids 1: 1.645 - F1 Score: 0.7097\n",
      "Poids 1: 1.646 - F1 Score: 0.7097\n",
      "Poids 1: 1.647 - F1 Score: 0.7097\n",
      "Poids 1: 1.648 - F1 Score: 0.7097\n",
      "Poids 1: 1.649 - F1 Score: 0.7097\n",
      "Poids 1: 1.650 - F1 Score: 0.7097\n",
      "Poids 1: 1.651 - F1 Score: 0.7097\n",
      "Poids 1: 1.652 - F1 Score: 0.7097\n",
      "Poids 1: 1.653 - F1 Score: 0.7097\n",
      "Poids 1: 1.654 - F1 Score: 0.7097\n",
      "Poids 1: 1.655 - F1 Score: 0.7097\n",
      "Poids 1: 1.656 - F1 Score: 0.7097\n",
      "Poids 1: 1.657 - F1 Score: 0.7097\n",
      "Poids 1: 1.658 - F1 Score: 0.7097\n",
      "Poids 1: 1.659 - F1 Score: 0.7097\n",
      "Poids 1: 1.660 - F1 Score: 0.7097\n",
      "Poids 1: 1.661 - F1 Score: 0.7097\n",
      "Poids 1: 1.662 - F1 Score: 0.7097\n",
      "Poids 1: 1.663 - F1 Score: 0.7097\n",
      "Poids 1: 1.664 - F1 Score: 0.7097\n",
      "Poids 1: 1.665 - F1 Score: 0.7097\n",
      "Poids 1: 1.666 - F1 Score: 0.7097\n",
      "Poids 1: 1.667 - F1 Score: 0.7097\n",
      "Poids 1: 1.668 - F1 Score: 0.7097\n",
      "Poids 1: 1.669 - F1 Score: 0.7097\n",
      "Poids 1: 1.670 - F1 Score: 0.7097\n",
      "Poids 1: 1.671 - F1 Score: 0.7097\n",
      "Poids 1: 1.672 - F1 Score: 0.7097\n",
      "Poids 1: 1.673 - F1 Score: 0.7097\n",
      "Poids 1: 1.674 - F1 Score: 0.7097\n",
      "Poids 1: 1.675 - F1 Score: 0.7097\n",
      "Poids 1: 1.676 - F1 Score: 0.7097\n",
      "Poids 1: 1.677 - F1 Score: 0.7097\n",
      "Poids 1: 1.678 - F1 Score: 0.7097\n",
      "Poids 1: 1.679 - F1 Score: 0.7097\n",
      "Poids 1: 1.680 - F1 Score: 0.7097\n",
      "Poids 1: 1.681 - F1 Score: 0.7097\n",
      "Poids 1: 1.682 - F1 Score: 0.7097\n",
      "Poids 1: 1.683 - F1 Score: 0.7097\n",
      "Poids 1: 1.684 - F1 Score: 0.7097\n",
      "Poids 1: 1.685 - F1 Score: 0.7097\n",
      "Poids 1: 1.686 - F1 Score: 0.7097\n",
      "Poids 1: 1.687 - F1 Score: 0.7097\n",
      "Poids 1: 1.688 - F1 Score: 0.7097\n",
      "Poids 1: 1.689 - F1 Score: 0.7097\n",
      "Poids 1: 1.690 - F1 Score: 0.7097\n",
      "Poids 1: 1.691 - F1 Score: 0.7097\n",
      "Poids 1: 1.692 - F1 Score: 0.7097\n",
      "Poids 1: 1.693 - F1 Score: 0.7097\n",
      "Poids 1: 1.694 - F1 Score: 0.7097\n",
      "Poids 1: 1.695 - F1 Score: 0.7097\n",
      "Poids 1: 1.696 - F1 Score: 0.7097\n",
      "Poids 1: 1.697 - F1 Score: 0.7097\n",
      "Poids 1: 1.698 - F1 Score: 0.7097\n",
      "Poids 1: 1.699 - F1 Score: 0.7097\n",
      "Poids 1: 1.700 - F1 Score: 0.7097\n",
      "Poids 1: 1.701 - F1 Score: 0.7097\n",
      "Poids 1: 1.702 - F1 Score: 0.7097\n",
      "Poids 1: 1.703 - F1 Score: 0.7097\n",
      "Poids 1: 1.704 - F1 Score: 0.7097\n",
      "Poids 1: 1.705 - F1 Score: 0.7097\n",
      "Poids 1: 1.706 - F1 Score: 0.7097\n",
      "Poids 1: 1.707 - F1 Score: 0.7097\n",
      "Poids 1: 1.708 - F1 Score: 0.7097\n",
      "Poids 1: 1.709 - F1 Score: 0.7097\n",
      "Poids 1: 1.710 - F1 Score: 0.7097\n",
      "Poids 1: 1.711 - F1 Score: 0.7097\n",
      "Poids 1: 1.712 - F1 Score: 0.7097\n",
      "Poids 1: 1.713 - F1 Score: 0.7097\n",
      "Poids 1: 1.714 - F1 Score: 0.7097\n",
      "Poids 1: 1.715 - F1 Score: 0.7097\n",
      "Poids 1: 1.716 - F1 Score: 0.7097\n",
      "Poids 1: 1.717 - F1 Score: 0.7097\n",
      "Poids 1: 1.718 - F1 Score: 0.7097\n",
      "Poids 1: 1.719 - F1 Score: 0.7097\n",
      "Poids 1: 1.720 - F1 Score: 0.7097\n",
      "Poids 1: 1.721 - F1 Score: 0.7097\n",
      "Poids 1: 1.722 - F1 Score: 0.7097\n",
      "Poids 1: 1.723 - F1 Score: 0.7097\n",
      "Poids 1: 1.724 - F1 Score: 0.7097\n",
      "Poids 1: 1.725 - F1 Score: 0.7097\n",
      "Poids 1: 1.726 - F1 Score: 0.7097\n",
      "Poids 1: 1.727 - F1 Score: 0.7097\n",
      "Poids 1: 1.728 - F1 Score: 0.7097\n",
      "Poids 1: 1.729 - F1 Score: 0.7097\n",
      "Poids 1: 1.730 - F1 Score: 0.7097\n",
      "Poids 1: 1.731 - F1 Score: 0.7097\n",
      "Poids 1: 1.732 - F1 Score: 0.7097\n",
      "Poids 1: 1.733 - F1 Score: 0.7097\n",
      "Poids 1: 1.734 - F1 Score: 0.7097\n",
      "Poids 1: 1.735 - F1 Score: 0.7097\n",
      "Poids 1: 1.736 - F1 Score: 0.7097\n",
      "Poids 1: 1.737 - F1 Score: 0.7097\n",
      "Poids 1: 1.738 - F1 Score: 0.7097\n",
      "Poids 1: 1.739 - F1 Score: 0.7097\n",
      "Poids 1: 1.740 - F1 Score: 0.7097\n",
      "Poids 1: 1.741 - F1 Score: 0.7097\n",
      "Poids 1: 1.742 - F1 Score: 0.7097\n",
      "Poids 1: 1.743 - F1 Score: 0.7097\n",
      "Poids 1: 1.744 - F1 Score: 0.7097\n",
      "Poids 1: 1.745 - F1 Score: 0.7097\n",
      "Poids 1: 1.746 - F1 Score: 0.7097\n",
      "Poids 1: 1.747 - F1 Score: 0.7097\n",
      "Poids 1: 1.748 - F1 Score: 0.7097\n",
      "Poids 1: 1.749 - F1 Score: 0.7097\n",
      "Poids 1: 1.750 - F1 Score: 0.7097\n",
      "Poids 1: 1.751 - F1 Score: 0.7097\n",
      "Poids 1: 1.752 - F1 Score: 0.7097\n",
      "Poids 1: 1.753 - F1 Score: 0.7097\n",
      "Poids 1: 1.754 - F1 Score: 0.7097\n",
      "Poids 1: 1.755 - F1 Score: 0.7097\n",
      "Poids 1: 1.756 - F1 Score: 0.7097\n",
      "Poids 1: 1.757 - F1 Score: 0.7097\n",
      "Poids 1: 1.758 - F1 Score: 0.7097\n",
      "Poids 1: 1.759 - F1 Score: 0.7097\n",
      "Poids 1: 1.760 - F1 Score: 0.7097\n",
      "Poids 1: 1.761 - F1 Score: 0.7097\n",
      "Poids 1: 1.762 - F1 Score: 0.7097\n",
      "Poids 1: 1.763 - F1 Score: 0.7097\n",
      "Poids 1: 1.764 - F1 Score: 0.7097\n",
      "Poids 1: 1.765 - F1 Score: 0.7097\n",
      "Poids 1: 1.766 - F1 Score: 0.7097\n",
      "Poids 1: 1.767 - F1 Score: 0.7097\n",
      "Poids 1: 1.768 - F1 Score: 0.7097\n",
      "Poids 1: 1.769 - F1 Score: 0.7097\n",
      "Poids 1: 1.770 - F1 Score: 0.7097\n",
      "Poids 1: 1.771 - F1 Score: 0.7097\n",
      "Poids 1: 1.772 - F1 Score: 0.7097\n",
      "Poids 1: 1.773 - F1 Score: 0.7097\n",
      "Poids 1: 1.774 - F1 Score: 0.7097\n",
      "Poids 1: 1.775 - F1 Score: 0.7097\n",
      "Poids 1: 1.776 - F1 Score: 0.7097\n",
      "Poids 1: 1.777 - F1 Score: 0.7097\n",
      "Poids 1: 1.778 - F1 Score: 0.7097\n",
      "Poids 1: 1.779 - F1 Score: 0.7097\n",
      "Poids 1: 1.780 - F1 Score: 0.7097\n",
      "Poids 1: 1.781 - F1 Score: 0.7097\n",
      "Poids 1: 1.782 - F1 Score: 0.7097\n",
      "Poids 1: 1.783 - F1 Score: 0.7097\n",
      "Poids 1: 1.784 - F1 Score: 0.7097\n",
      "Poids 1: 1.785 - F1 Score: 0.7097\n",
      "Poids 1: 1.786 - F1 Score: 0.7097\n",
      "Poids 1: 1.787 - F1 Score: 0.7097\n",
      "Poids 1: 1.788 - F1 Score: 0.7097\n",
      "Poids 1: 1.789 - F1 Score: 0.7097\n",
      "Poids 1: 1.790 - F1 Score: 0.7097\n",
      "Poids 1: 1.791 - F1 Score: 0.7097\n",
      "Poids 1: 1.792 - F1 Score: 0.7097\n",
      "Poids 1: 1.793 - F1 Score: 0.7097\n",
      "Poids 1: 1.794 - F1 Score: 0.7097\n",
      "Poids 1: 1.795 - F1 Score: 0.7097\n",
      "Poids 1: 1.796 - F1 Score: 0.7097\n",
      "Poids 1: 1.797 - F1 Score: 0.7097\n",
      "Poids 1: 1.798 - F1 Score: 0.7097\n",
      "Poids 1: 1.799 - F1 Score: 0.7097\n",
      "Poids 1: 1.800 - F1 Score: 0.7097\n",
      "Poids 1: 1.801 - F1 Score: 0.7097\n",
      "Poids 1: 1.802 - F1 Score: 0.7097\n",
      "Poids 1: 1.803 - F1 Score: 0.7097\n",
      "Poids 1: 1.804 - F1 Score: 0.7097\n",
      "Poids 1: 1.805 - F1 Score: 0.7097\n",
      "Poids 1: 1.806 - F1 Score: 0.7097\n",
      "Poids 1: 1.807 - F1 Score: 0.7097\n",
      "Poids 1: 1.808 - F1 Score: 0.7097\n",
      "Poids 1: 1.809 - F1 Score: 0.7097\n",
      "Poids 1: 1.810 - F1 Score: 0.7097\n",
      "Poids 1: 1.811 - F1 Score: 0.7097\n",
      "Poids 1: 1.812 - F1 Score: 0.7097\n",
      "Poids 1: 1.813 - F1 Score: 0.7097\n",
      "Poids 1: 1.814 - F1 Score: 0.7097\n",
      "Poids 1: 1.815 - F1 Score: 0.7097\n",
      "Poids 1: 1.816 - F1 Score: 0.7097\n",
      "Poids 1: 1.817 - F1 Score: 0.7097\n",
      "Poids 1: 1.818 - F1 Score: 0.7097\n",
      "Poids 1: 1.819 - F1 Score: 0.7097\n",
      "Poids 1: 1.820 - F1 Score: 0.7097\n",
      "Poids 1: 1.821 - F1 Score: 0.7097\n",
      "Poids 1: 1.822 - F1 Score: 0.7097\n",
      "Poids 1: 1.823 - F1 Score: 0.7097\n",
      "Poids 1: 1.824 - F1 Score: 0.7097\n",
      "Poids 1: 1.825 - F1 Score: 0.7097\n",
      "Poids 1: 1.826 - F1 Score: 0.7097\n",
      "Poids 1: 1.827 - F1 Score: 0.7097\n",
      "Poids 1: 1.828 - F1 Score: 0.7097\n",
      "Poids 1: 1.829 - F1 Score: 0.7097\n",
      "Poids 1: 1.830 - F1 Score: 0.7097\n",
      "Poids 1: 1.831 - F1 Score: 0.7097\n",
      "Poids 1: 1.832 - F1 Score: 0.7097\n",
      "Poids 1: 1.833 - F1 Score: 0.7097\n",
      "Poids 1: 1.834 - F1 Score: 0.7097\n",
      "Poids 1: 1.835 - F1 Score: 0.7097\n",
      "Poids 1: 1.836 - F1 Score: 0.7097\n",
      "Poids 1: 1.837 - F1 Score: 0.7097\n",
      "Poids 1: 1.838 - F1 Score: 0.7097\n",
      "Poids 1: 1.839 - F1 Score: 0.7097\n",
      "Poids 1: 1.840 - F1 Score: 0.7097\n",
      "Poids 1: 1.841 - F1 Score: 0.7097\n",
      "Poids 1: 1.842 - F1 Score: 0.7097\n",
      "Poids 1: 1.843 - F1 Score: 0.7097\n",
      "Poids 1: 1.844 - F1 Score: 0.7097\n",
      "Poids 1: 1.845 - F1 Score: 0.7097\n",
      "Poids 1: 1.846 - F1 Score: 0.7097\n",
      "Poids 1: 1.847 - F1 Score: 0.7097\n",
      "Poids 1: 1.848 - F1 Score: 0.7097\n",
      "Poids 1: 1.849 - F1 Score: 0.7097\n",
      "Poids 1: 1.850 - F1 Score: 0.7097\n",
      "Poids 1: 1.851 - F1 Score: 0.7097\n",
      "Poids 1: 1.852 - F1 Score: 0.7097\n",
      "Poids 1: 1.853 - F1 Score: 0.7097\n",
      "Poids 1: 1.854 - F1 Score: 0.7097\n",
      "Poids 1: 1.855 - F1 Score: 0.7097\n",
      "Poids 1: 1.856 - F1 Score: 0.7097\n",
      "Poids 1: 1.857 - F1 Score: 0.7097\n",
      "Poids 1: 1.858 - F1 Score: 0.7097\n",
      "Poids 1: 1.859 - F1 Score: 0.7097\n",
      "Poids 1: 1.860 - F1 Score: 0.7097\n",
      "Poids 1: 1.861 - F1 Score: 0.7097\n",
      "Poids 1: 1.862 - F1 Score: 0.7097\n",
      "Poids 1: 1.863 - F1 Score: 0.7097\n",
      "Poids 1: 1.864 - F1 Score: 0.7097\n",
      "Poids 1: 1.865 - F1 Score: 0.7097\n",
      "Poids 1: 1.866 - F1 Score: 0.7097\n",
      "Poids 1: 1.867 - F1 Score: 0.7097\n",
      "Poids 1: 1.868 - F1 Score: 0.7097\n",
      "Poids 1: 1.869 - F1 Score: 0.7097\n",
      "Poids 1: 1.870 - F1 Score: 0.7097\n",
      "Poids 1: 1.871 - F1 Score: 0.7097\n",
      "Poids 1: 1.872 - F1 Score: 0.7097\n",
      "Poids 1: 1.873 - F1 Score: 0.7097\n",
      "Poids 1: 1.874 - F1 Score: 0.7097\n",
      "Poids 1: 1.875 - F1 Score: 0.7097\n",
      "Poids 1: 1.876 - F1 Score: 0.7097\n",
      "Poids 1: 1.877 - F1 Score: 0.7097\n",
      "Poids 1: 1.878 - F1 Score: 0.7021\n",
      "Poids 1: 1.879 - F1 Score: 0.7021\n",
      "Poids 1: 1.880 - F1 Score: 0.6947\n",
      "Poids 1: 1.881 - F1 Score: 0.6947\n",
      "Poids 1: 1.882 - F1 Score: 0.6947\n",
      "Poids 1: 1.883 - F1 Score: 0.6947\n",
      "Poids 1: 1.884 - F1 Score: 0.6947\n",
      "Poids 1: 1.885 - F1 Score: 0.6947\n",
      "Poids 1: 1.886 - F1 Score: 0.6947\n",
      "Poids 1: 1.887 - F1 Score: 0.6947\n",
      "Poids 1: 1.888 - F1 Score: 0.6947\n",
      "Poids 1: 1.889 - F1 Score: 0.6947\n",
      "Poids 1: 1.890 - F1 Score: 0.6947\n",
      "Poids 1: 1.891 - F1 Score: 0.6947\n",
      "Poids 1: 1.892 - F1 Score: 0.6947\n",
      "Poids 1: 1.893 - F1 Score: 0.6947\n",
      "Poids 1: 1.894 - F1 Score: 0.6947\n",
      "Poids 1: 1.895 - F1 Score: 0.6947\n",
      "Poids 1: 1.896 - F1 Score: 0.6947\n",
      "Poids 1: 1.897 - F1 Score: 0.6947\n",
      "Poids 1: 1.898 - F1 Score: 0.6947\n",
      "Poids 1: 1.899 - F1 Score: 0.6947\n",
      "Poids 1: 1.900 - F1 Score: 0.6947\n",
      "Poids 1: 1.901 - F1 Score: 0.6947\n",
      "Poids 1: 1.902 - F1 Score: 0.6947\n",
      "Poids 1: 1.903 - F1 Score: 0.6947\n",
      "Poids 1: 1.904 - F1 Score: 0.6947\n",
      "Poids 1: 1.905 - F1 Score: 0.6947\n",
      "Poids 1: 1.906 - F1 Score: 0.6947\n",
      "Poids 1: 1.907 - F1 Score: 0.6947\n",
      "Poids 1: 1.908 - F1 Score: 0.6947\n",
      "Poids 1: 1.909 - F1 Score: 0.6947\n",
      "Poids 1: 1.910 - F1 Score: 0.6947\n",
      "Poids 1: 1.911 - F1 Score: 0.6947\n",
      "Poids 1: 1.912 - F1 Score: 0.6947\n",
      "Poids 1: 1.913 - F1 Score: 0.6947\n",
      "Poids 1: 1.914 - F1 Score: 0.6947\n",
      "Poids 1: 1.915 - F1 Score: 0.6947\n",
      "Poids 1: 1.916 - F1 Score: 0.6947\n",
      "Poids 1: 1.917 - F1 Score: 0.6947\n",
      "Poids 1: 1.918 - F1 Score: 0.6947\n",
      "Poids 1: 1.919 - F1 Score: 0.6947\n",
      "Poids 1: 1.920 - F1 Score: 0.6947\n",
      "Poids 1: 1.921 - F1 Score: 0.6947\n",
      "Poids 1: 1.922 - F1 Score: 0.6947\n",
      "Poids 1: 1.923 - F1 Score: 0.6947\n",
      "Poids 1: 1.924 - F1 Score: 0.6947\n",
      "Poids 1: 1.925 - F1 Score: 0.6947\n",
      "Poids 1: 1.926 - F1 Score: 0.6947\n",
      "Poids 1: 1.927 - F1 Score: 0.6947\n",
      "Poids 1: 1.928 - F1 Score: 0.6947\n",
      "Poids 1: 1.929 - F1 Score: 0.6947\n",
      "Poids 1: 1.930 - F1 Score: 0.6947\n",
      "Poids 1: 1.931 - F1 Score: 0.6947\n",
      "Poids 1: 1.932 - F1 Score: 0.7083\n",
      "Poids 1: 1.933 - F1 Score: 0.7083\n",
      "Poids 1: 1.934 - F1 Score: 0.7083\n",
      "Poids 1: 1.935 - F1 Score: 0.7083\n",
      "Poids 1: 1.936 - F1 Score: 0.7083\n",
      "Poids 1: 1.937 - F1 Score: 0.7083\n",
      "Poids 1: 1.938 - F1 Score: 0.7083\n",
      "Poids 1: 1.939 - F1 Score: 0.7083\n",
      "Poids 1: 1.940 - F1 Score: 0.7083\n",
      "Poids 1: 1.941 - F1 Score: 0.7083\n",
      "Poids 1: 1.942 - F1 Score: 0.7083\n",
      "Poids 1: 1.943 - F1 Score: 0.7010\n",
      "Poids 1: 1.944 - F1 Score: 0.7010\n",
      "Poids 1: 1.945 - F1 Score: 0.7010\n",
      "Poids 1: 1.946 - F1 Score: 0.7010\n",
      "Poids 1: 1.947 - F1 Score: 0.7010\n",
      "Poids 1: 1.948 - F1 Score: 0.7010\n",
      "Poids 1: 1.949 - F1 Score: 0.7010\n",
      "Poids 1: 1.950 - F1 Score: 0.7010\n",
      "Poids 1: 1.951 - F1 Score: 0.7010\n",
      "Poids 1: 1.952 - F1 Score: 0.7010\n",
      "Poids 1: 1.953 - F1 Score: 0.7010\n",
      "Poids 1: 1.954 - F1 Score: 0.7010\n",
      "Poids 1: 1.955 - F1 Score: 0.7010\n",
      "Poids 1: 1.956 - F1 Score: 0.7010\n",
      "Poids 1: 1.957 - F1 Score: 0.7010\n",
      "Poids 1: 1.958 - F1 Score: 0.7010\n",
      "Poids 1: 1.959 - F1 Score: 0.7010\n",
      "Poids 1: 1.960 - F1 Score: 0.7010\n",
      "Poids 1: 1.961 - F1 Score: 0.7010\n",
      "Poids 1: 1.962 - F1 Score: 0.7010\n",
      "Poids 1: 1.963 - F1 Score: 0.7010\n",
      "Poids 1: 1.964 - F1 Score: 0.7010\n",
      "Poids 1: 1.965 - F1 Score: 0.7010\n",
      "Poids 1: 1.966 - F1 Score: 0.7010\n",
      "Poids 1: 1.967 - F1 Score: 0.7010\n",
      "Poids 1: 1.968 - F1 Score: 0.7010\n",
      "Poids 1: 1.969 - F1 Score: 0.7010\n",
      "Poids 1: 1.970 - F1 Score: 0.7010\n",
      "Poids 1: 1.971 - F1 Score: 0.7010\n",
      "Poids 1: 1.972 - F1 Score: 0.7010\n",
      "Poids 1: 1.973 - F1 Score: 0.7010\n",
      "Poids 1: 1.974 - F1 Score: 0.7143\n",
      "Poids 1: 1.975 - F1 Score: 0.7143\n",
      "Poids 1: 1.976 - F1 Score: 0.7143\n",
      "Poids 1: 1.977 - F1 Score: 0.7143\n",
      "Poids 1: 1.978 - F1 Score: 0.7143\n",
      "Poids 1: 1.979 - F1 Score: 0.7143\n",
      "Poids 1: 1.980 - F1 Score: 0.7143\n",
      "Poids 1: 1.981 - F1 Score: 0.7143\n",
      "Poids 1: 1.982 - F1 Score: 0.7143\n",
      "Poids 1: 1.983 - F1 Score: 0.7143\n",
      "Poids 1: 1.984 - F1 Score: 0.7143\n",
      "Poids 1: 1.985 - F1 Score: 0.7143\n",
      "Poids 1: 1.986 - F1 Score: 0.7143\n",
      "Poids 1: 1.987 - F1 Score: 0.7143\n",
      "Poids 1: 1.988 - F1 Score: 0.7143\n",
      "Poids 1: 1.989 - F1 Score: 0.7143\n",
      "Poids 1: 1.990 - F1 Score: 0.7143\n",
      "Poids 1: 1.991 - F1 Score: 0.7143\n",
      "Poids 1: 1.992 - F1 Score: 0.7143\n",
      "Poids 1: 1.993 - F1 Score: 0.7143\n",
      "Poids 1: 1.994 - F1 Score: 0.7143\n",
      "Poids 1: 1.995 - F1 Score: 0.7143\n",
      "Poids 1: 1.996 - F1 Score: 0.7143\n",
      "Poids 1: 1.997 - F1 Score: 0.7143\n",
      "Poids 1: 1.998 - F1 Score: 0.7143\n",
      "Poids 1: 1.999 - F1 Score: 0.7143\n",
      "Poids 1: 2.000 - F1 Score: 0.7143\n",
      "Poids 1: 2.001 - F1 Score: 0.7143\n",
      "Poids 1: 2.002 - F1 Score: 0.7143\n",
      "Poids 1: 2.003 - F1 Score: 0.7143\n",
      "Poids 1: 2.004 - F1 Score: 0.7143\n",
      "Poids 1: 2.005 - F1 Score: 0.7143\n",
      "Poids 1: 2.006 - F1 Score: 0.7143\n",
      "Poids 1: 2.007 - F1 Score: 0.7143\n",
      "Poids 1: 2.008 - F1 Score: 0.7143\n",
      "Poids 1: 2.009 - F1 Score: 0.7143\n",
      "Poids 1: 2.010 - F1 Score: 0.7143\n",
      "Poids 1: 2.011 - F1 Score: 0.7143\n",
      "Poids 1: 2.012 - F1 Score: 0.7143\n",
      "Poids 1: 2.013 - F1 Score: 0.7071\n",
      "Poids 1: 2.014 - F1 Score: 0.7071\n",
      "Poids 1: 2.015 - F1 Score: 0.7071\n",
      "Poids 1: 2.016 - F1 Score: 0.7071\n",
      "Poids 1: 2.017 - F1 Score: 0.7071\n",
      "Poids 1: 2.018 - F1 Score: 0.7071\n",
      "Poids 1: 2.019 - F1 Score: 0.7071\n",
      "Poids 1: 2.020 - F1 Score: 0.7071\n",
      "Poids 1: 2.021 - F1 Score: 0.7071\n",
      "Poids 1: 2.022 - F1 Score: 0.7071\n",
      "Poids 1: 2.023 - F1 Score: 0.7071\n",
      "Poids 1: 2.024 - F1 Score: 0.7071\n",
      "Poids 1: 2.025 - F1 Score: 0.7071\n",
      "Poids 1: 2.026 - F1 Score: 0.7071\n",
      "Poids 1: 2.027 - F1 Score: 0.7071\n",
      "Poids 1: 2.028 - F1 Score: 0.7071\n",
      "Poids 1: 2.029 - F1 Score: 0.7071\n",
      "Poids 1: 2.030 - F1 Score: 0.7071\n",
      "Poids 1: 2.031 - F1 Score: 0.7071\n",
      "Poids 1: 2.032 - F1 Score: 0.7071\n",
      "Poids 1: 2.033 - F1 Score: 0.7071\n",
      "Poids 1: 2.034 - F1 Score: 0.7071\n",
      "Poids 1: 2.035 - F1 Score: 0.7071\n",
      "Poids 1: 2.036 - F1 Score: 0.7071\n",
      "Poids 1: 2.037 - F1 Score: 0.7071\n",
      "Poids 1: 2.038 - F1 Score: 0.7071\n",
      "Poids 1: 2.039 - F1 Score: 0.7071\n",
      "Poids 1: 2.040 - F1 Score: 0.7071\n",
      "Poids 1: 2.041 - F1 Score: 0.7071\n",
      "Poids 1: 2.042 - F1 Score: 0.7071\n",
      "Poids 1: 2.043 - F1 Score: 0.7071\n",
      "Poids 1: 2.044 - F1 Score: 0.7071\n",
      "Poids 1: 2.045 - F1 Score: 0.7071\n",
      "Poids 1: 2.046 - F1 Score: 0.7071\n",
      "Poids 1: 2.047 - F1 Score: 0.7071\n",
      "Poids 1: 2.048 - F1 Score: 0.7071\n",
      "Poids 1: 2.049 - F1 Score: 0.7071\n",
      "Poids 1: 2.050 - F1 Score: 0.7071\n",
      "Poids 1: 2.051 - F1 Score: 0.7071\n",
      "Poids 1: 2.052 - F1 Score: 0.7071\n",
      "Poids 1: 2.053 - F1 Score: 0.7071\n",
      "Poids 1: 2.054 - F1 Score: 0.7071\n",
      "Poids 1: 2.055 - F1 Score: 0.7071\n",
      "Poids 1: 2.056 - F1 Score: 0.7071\n",
      "Poids 1: 2.057 - F1 Score: 0.7071\n",
      "Poids 1: 2.058 - F1 Score: 0.7071\n",
      "Poids 1: 2.059 - F1 Score: 0.7071\n",
      "Poids 1: 2.060 - F1 Score: 0.7071\n",
      "Poids 1: 2.061 - F1 Score: 0.7071\n",
      "Poids 1: 2.062 - F1 Score: 0.7071\n",
      "Poids 1: 2.063 - F1 Score: 0.7071\n",
      "Poids 1: 2.064 - F1 Score: 0.7071\n",
      "Poids 1: 2.065 - F1 Score: 0.7071\n",
      "Poids 1: 2.066 - F1 Score: 0.7071\n",
      "Poids 1: 2.067 - F1 Score: 0.7071\n",
      "Poids 1: 2.068 - F1 Score: 0.7071\n",
      "Poids 1: 2.069 - F1 Score: 0.7071\n",
      "Poids 1: 2.070 - F1 Score: 0.7071\n",
      "Poids 1: 2.071 - F1 Score: 0.7071\n",
      "Poids 1: 2.072 - F1 Score: 0.7071\n",
      "Poids 1: 2.073 - F1 Score: 0.7071\n",
      "Poids 1: 2.074 - F1 Score: 0.7071\n",
      "Poids 1: 2.075 - F1 Score: 0.7071\n",
      "Poids 1: 2.076 - F1 Score: 0.7071\n",
      "Poids 1: 2.077 - F1 Score: 0.7071\n",
      "Poids 1: 2.078 - F1 Score: 0.7071\n",
      "Poids 1: 2.079 - F1 Score: 0.7071\n",
      "Poids 1: 2.080 - F1 Score: 0.7071\n",
      "Poids 1: 2.081 - F1 Score: 0.7071\n",
      "Poids 1: 2.082 - F1 Score: 0.7071\n",
      "Poids 1: 2.083 - F1 Score: 0.7071\n",
      "Poids 1: 2.084 - F1 Score: 0.7071\n",
      "Poids 1: 2.085 - F1 Score: 0.7071\n",
      "Poids 1: 2.086 - F1 Score: 0.7071\n",
      "Poids 1: 2.087 - F1 Score: 0.7071\n",
      "Poids 1: 2.088 - F1 Score: 0.7071\n",
      "Poids 1: 2.089 - F1 Score: 0.7071\n",
      "Poids 1: 2.090 - F1 Score: 0.7071\n",
      "Poids 1: 2.091 - F1 Score: 0.7071\n",
      "Poids 1: 2.092 - F1 Score: 0.7071\n",
      "Poids 1: 2.093 - F1 Score: 0.7071\n",
      "Poids 1: 2.094 - F1 Score: 0.7071\n",
      "Poids 1: 2.095 - F1 Score: 0.7071\n",
      "Poids 1: 2.096 - F1 Score: 0.7071\n",
      "Poids 1: 2.097 - F1 Score: 0.7071\n",
      "Poids 1: 2.098 - F1 Score: 0.7071\n",
      "Poids 1: 2.099 - F1 Score: 0.7071\n",
      "Poids 1: 2.100 - F1 Score: 0.7071\n",
      "Poids 1: 2.101 - F1 Score: 0.7071\n",
      "Poids 1: 2.102 - F1 Score: 0.7071\n",
      "Poids 1: 2.103 - F1 Score: 0.7071\n",
      "Poids 1: 2.104 - F1 Score: 0.7071\n",
      "Poids 1: 2.105 - F1 Score: 0.7071\n",
      "Poids 1: 2.106 - F1 Score: 0.7071\n",
      "Poids 1: 2.107 - F1 Score: 0.7071\n",
      "Poids 1: 2.108 - F1 Score: 0.7071\n",
      "Poids 1: 2.109 - F1 Score: 0.7071\n",
      "Poids 1: 2.110 - F1 Score: 0.7071\n",
      "Poids 1: 2.111 - F1 Score: 0.7071\n",
      "Poids 1: 2.112 - F1 Score: 0.7071\n",
      "Poids 1: 2.113 - F1 Score: 0.7071\n",
      "Poids 1: 2.114 - F1 Score: 0.7071\n",
      "Poids 1: 2.115 - F1 Score: 0.7071\n",
      "Poids 1: 2.116 - F1 Score: 0.7071\n",
      "Poids 1: 2.117 - F1 Score: 0.7071\n",
      "Poids 1: 2.118 - F1 Score: 0.7071\n",
      "Poids 1: 2.119 - F1 Score: 0.7071\n",
      "Poids 1: 2.120 - F1 Score: 0.7071\n",
      "Poids 1: 2.121 - F1 Score: 0.7071\n",
      "Poids 1: 2.122 - F1 Score: 0.7071\n",
      "Poids 1: 2.123 - F1 Score: 0.7071\n",
      "Poids 1: 2.124 - F1 Score: 0.7071\n",
      "Poids 1: 2.125 - F1 Score: 0.7071\n",
      "Poids 1: 2.126 - F1 Score: 0.7071\n",
      "Poids 1: 2.127 - F1 Score: 0.7071\n",
      "Poids 1: 2.128 - F1 Score: 0.7071\n",
      "Poids 1: 2.129 - F1 Score: 0.7071\n",
      "Poids 1: 2.130 - F1 Score: 0.7071\n",
      "Poids 1: 2.131 - F1 Score: 0.7071\n",
      "Poids 1: 2.132 - F1 Score: 0.7071\n",
      "Poids 1: 2.133 - F1 Score: 0.7071\n",
      "Poids 1: 2.134 - F1 Score: 0.7071\n",
      "Poids 1: 2.135 - F1 Score: 0.7071\n",
      "Poids 1: 2.136 - F1 Score: 0.7071\n",
      "Poids 1: 2.137 - F1 Score: 0.7071\n",
      "Poids 1: 2.138 - F1 Score: 0.7071\n",
      "Poids 1: 2.139 - F1 Score: 0.7071\n",
      "Poids 1: 2.140 - F1 Score: 0.7071\n",
      "Poids 1: 2.141 - F1 Score: 0.7071\n",
      "Poids 1: 2.142 - F1 Score: 0.7071\n",
      "Poids 1: 2.143 - F1 Score: 0.7071\n",
      "Poids 1: 2.144 - F1 Score: 0.7071\n",
      "Poids 1: 2.145 - F1 Score: 0.7071\n",
      "Poids 1: 2.146 - F1 Score: 0.7071\n",
      "Poids 1: 2.147 - F1 Score: 0.7071\n",
      "Poids 1: 2.148 - F1 Score: 0.7071\n",
      "Poids 1: 2.149 - F1 Score: 0.7071\n",
      "Poids 1: 2.150 - F1 Score: 0.7071\n",
      "Poids 1: 2.151 - F1 Score: 0.7071\n",
      "Poids 1: 2.152 - F1 Score: 0.7071\n",
      "Poids 1: 2.153 - F1 Score: 0.7071\n",
      "Poids 1: 2.154 - F1 Score: 0.7071\n",
      "Poids 1: 2.155 - F1 Score: 0.7071\n",
      "Poids 1: 2.156 - F1 Score: 0.7071\n",
      "Poids 1: 2.157 - F1 Score: 0.7071\n",
      "Poids 1: 2.158 - F1 Score: 0.7071\n",
      "Poids 1: 2.159 - F1 Score: 0.7071\n",
      "Poids 1: 2.160 - F1 Score: 0.7071\n",
      "Poids 1: 2.161 - F1 Score: 0.7071\n",
      "Poids 1: 2.162 - F1 Score: 0.7071\n",
      "Poids 1: 2.163 - F1 Score: 0.7071\n",
      "Poids 1: 2.164 - F1 Score: 0.7071\n",
      "Poids 1: 2.165 - F1 Score: 0.7071\n",
      "Poids 1: 2.166 - F1 Score: 0.7071\n",
      "Poids 1: 2.167 - F1 Score: 0.7071\n",
      "Poids 1: 2.168 - F1 Score: 0.7071\n",
      "Poids 1: 2.169 - F1 Score: 0.7071\n",
      "Poids 1: 2.170 - F1 Score: 0.7071\n",
      "Poids 1: 2.171 - F1 Score: 0.7071\n",
      "Poids 1: 2.172 - F1 Score: 0.7071\n",
      "Poids 1: 2.173 - F1 Score: 0.7071\n",
      "Poids 1: 2.174 - F1 Score: 0.7071\n",
      "Poids 1: 2.175 - F1 Score: 0.7071\n",
      "Poids 1: 2.176 - F1 Score: 0.7071\n",
      "Poids 1: 2.177 - F1 Score: 0.7071\n",
      "Poids 1: 2.178 - F1 Score: 0.7071\n",
      "Poids 1: 2.179 - F1 Score: 0.7071\n",
      "Poids 1: 2.180 - F1 Score: 0.7071\n",
      "Poids 1: 2.181 - F1 Score: 0.7071\n",
      "Poids 1: 2.182 - F1 Score: 0.7071\n",
      "Poids 1: 2.183 - F1 Score: 0.7071\n",
      "Poids 1: 2.184 - F1 Score: 0.7071\n",
      "Poids 1: 2.185 - F1 Score: 0.7071\n",
      "Poids 1: 2.186 - F1 Score: 0.7071\n",
      "Poids 1: 2.187 - F1 Score: 0.7071\n",
      "Poids 1: 2.188 - F1 Score: 0.7071\n",
      "Poids 1: 2.189 - F1 Score: 0.7071\n",
      "Poids 1: 2.190 - F1 Score: 0.7071\n",
      "Poids 1: 2.191 - F1 Score: 0.7071\n",
      "Poids 1: 2.192 - F1 Score: 0.7071\n",
      "Poids 1: 2.193 - F1 Score: 0.7000\n",
      "Poids 1: 2.194 - F1 Score: 0.7000\n",
      "Poids 1: 2.195 - F1 Score: 0.7000\n",
      "Poids 1: 2.196 - F1 Score: 0.7000\n",
      "Poids 1: 2.197 - F1 Score: 0.7000\n",
      "Poids 1: 2.198 - F1 Score: 0.7000\n",
      "Poids 1: 2.199 - F1 Score: 0.7000\n",
      "Poids 1: 2.200 - F1 Score: 0.7000\n",
      "Poids 1: 2.201 - F1 Score: 0.7000\n",
      "Poids 1: 2.202 - F1 Score: 0.7000\n",
      "Poids 1: 2.203 - F1 Score: 0.7000\n",
      "Poids 1: 2.204 - F1 Score: 0.7000\n",
      "Poids 1: 2.205 - F1 Score: 0.7000\n",
      "Poids 1: 2.206 - F1 Score: 0.7000\n",
      "Poids 1: 2.207 - F1 Score: 0.7000\n",
      "Poids 1: 2.208 - F1 Score: 0.7000\n",
      "Poids 1: 2.209 - F1 Score: 0.7000\n",
      "Poids 1: 2.210 - F1 Score: 0.7000\n",
      "Poids 1: 2.211 - F1 Score: 0.7000\n",
      "Poids 1: 2.212 - F1 Score: 0.7000\n",
      "Poids 1: 2.213 - F1 Score: 0.7000\n",
      "Poids 1: 2.214 - F1 Score: 0.7000\n",
      "Poids 1: 2.215 - F1 Score: 0.7000\n",
      "Poids 1: 2.216 - F1 Score: 0.7000\n",
      "Poids 1: 2.217 - F1 Score: 0.7000\n",
      "Poids 1: 2.218 - F1 Score: 0.7000\n",
      "Poids 1: 2.219 - F1 Score: 0.7000\n",
      "Poids 1: 2.220 - F1 Score: 0.7000\n",
      "Poids 1: 2.221 - F1 Score: 0.7000\n",
      "Poids 1: 2.222 - F1 Score: 0.7000\n",
      "Poids 1: 2.223 - F1 Score: 0.7000\n",
      "Poids 1: 2.224 - F1 Score: 0.7000\n",
      "Poids 1: 2.225 - F1 Score: 0.7000\n",
      "Poids 1: 2.226 - F1 Score: 0.7000\n",
      "Poids 1: 2.227 - F1 Score: 0.7000\n",
      "Poids 1: 2.228 - F1 Score: 0.7000\n",
      "Poids 1: 2.229 - F1 Score: 0.7000\n",
      "Poids 1: 2.230 - F1 Score: 0.7000\n",
      "Poids 1: 2.231 - F1 Score: 0.7000\n",
      "Poids 1: 2.232 - F1 Score: 0.7000\n",
      "Poids 1: 2.233 - F1 Score: 0.7000\n",
      "Poids 1: 2.234 - F1 Score: 0.7000\n",
      "Poids 1: 2.235 - F1 Score: 0.7000\n",
      "Poids 1: 2.236 - F1 Score: 0.7000\n",
      "Poids 1: 2.237 - F1 Score: 0.7000\n",
      "Poids 1: 2.238 - F1 Score: 0.7000\n",
      "Poids 1: 2.239 - F1 Score: 0.7000\n",
      "Poids 1: 2.240 - F1 Score: 0.7000\n",
      "Poids 1: 2.241 - F1 Score: 0.7000\n",
      "Poids 1: 2.242 - F1 Score: 0.7000\n",
      "Poids 1: 2.243 - F1 Score: 0.7000\n",
      "Poids 1: 2.244 - F1 Score: 0.7000\n",
      "Poids 1: 2.245 - F1 Score: 0.7000\n",
      "Poids 1: 2.246 - F1 Score: 0.7000\n",
      "Poids 1: 2.247 - F1 Score: 0.7000\n",
      "Poids 1: 2.248 - F1 Score: 0.7000\n",
      "Poids 1: 2.249 - F1 Score: 0.7000\n",
      "Poids 1: 2.250 - F1 Score: 0.7000\n",
      "Poids 1: 2.251 - F1 Score: 0.7000\n",
      "Poids 1: 2.252 - F1 Score: 0.7000\n",
      "Poids 1: 2.253 - F1 Score: 0.7000\n",
      "Poids 1: 2.254 - F1 Score: 0.7000\n",
      "Poids 1: 2.255 - F1 Score: 0.7000\n",
      "Poids 1: 2.256 - F1 Score: 0.7000\n",
      "Poids 1: 2.257 - F1 Score: 0.7000\n",
      "Poids 1: 2.258 - F1 Score: 0.7000\n",
      "Poids 1: 2.259 - F1 Score: 0.7000\n",
      "Poids 1: 2.260 - F1 Score: 0.7000\n",
      "Poids 1: 2.261 - F1 Score: 0.7000\n",
      "Poids 1: 2.262 - F1 Score: 0.7000\n",
      "Poids 1: 2.263 - F1 Score: 0.7000\n",
      "Poids 1: 2.264 - F1 Score: 0.7000\n",
      "Poids 1: 2.265 - F1 Score: 0.7000\n",
      "Poids 1: 2.266 - F1 Score: 0.7000\n",
      "Poids 1: 2.267 - F1 Score: 0.7000\n",
      "Poids 1: 2.268 - F1 Score: 0.7000\n",
      "Poids 1: 2.269 - F1 Score: 0.6931\n",
      "Poids 1: 2.270 - F1 Score: 0.6931\n",
      "Poids 1: 2.271 - F1 Score: 0.6931\n",
      "Poids 1: 2.272 - F1 Score: 0.6931\n",
      "Poids 1: 2.273 - F1 Score: 0.6931\n",
      "Poids 1: 2.274 - F1 Score: 0.6931\n",
      "Poids 1: 2.275 - F1 Score: 0.6931\n",
      "Poids 1: 2.276 - F1 Score: 0.6931\n",
      "Poids 1: 2.277 - F1 Score: 0.6931\n",
      "Poids 1: 2.278 - F1 Score: 0.6931\n",
      "Poids 1: 2.279 - F1 Score: 0.6931\n",
      "Poids 1: 2.280 - F1 Score: 0.6931\n",
      "Poids 1: 2.281 - F1 Score: 0.6931\n",
      "Poids 1: 2.282 - F1 Score: 0.6931\n",
      "Poids 1: 2.283 - F1 Score: 0.6931\n",
      "Poids 1: 2.284 - F1 Score: 0.6931\n",
      "Poids 1: 2.285 - F1 Score: 0.6931\n",
      "Poids 1: 2.286 - F1 Score: 0.6931\n",
      "Poids 1: 2.287 - F1 Score: 0.6931\n",
      "Poids 1: 2.288 - F1 Score: 0.6931\n",
      "Poids 1: 2.289 - F1 Score: 0.6931\n",
      "Poids 1: 2.290 - F1 Score: 0.6931\n",
      "Poids 1: 2.291 - F1 Score: 0.6931\n",
      "Poids 1: 2.292 - F1 Score: 0.6931\n",
      "Poids 1: 2.293 - F1 Score: 0.6931\n",
      "Poids 1: 2.294 - F1 Score: 0.6931\n",
      "Poids 1: 2.295 - F1 Score: 0.6931\n",
      "Poids 1: 2.296 - F1 Score: 0.6931\n",
      "Poids 1: 2.297 - F1 Score: 0.6931\n",
      "Poids 1: 2.298 - F1 Score: 0.6931\n",
      "Poids 1: 2.299 - F1 Score: 0.6931\n",
      "Poids 1: 2.300 - F1 Score: 0.6931\n",
      "Poids 1: 2.301 - F1 Score: 0.6931\n",
      "Poids 1: 2.302 - F1 Score: 0.6931\n",
      "Poids 1: 2.303 - F1 Score: 0.6931\n",
      "Poids 1: 2.304 - F1 Score: 0.6931\n",
      "Poids 1: 2.305 - F1 Score: 0.6931\n",
      "Poids 1: 2.306 - F1 Score: 0.6931\n",
      "Poids 1: 2.307 - F1 Score: 0.6931\n",
      "Poids 1: 2.308 - F1 Score: 0.6931\n",
      "Poids 1: 2.309 - F1 Score: 0.6931\n",
      "Poids 1: 2.310 - F1 Score: 0.6931\n",
      "Poids 1: 2.311 - F1 Score: 0.6931\n",
      "Poids 1: 2.312 - F1 Score: 0.6931\n",
      "Poids 1: 2.313 - F1 Score: 0.6931\n",
      "Poids 1: 2.314 - F1 Score: 0.6931\n",
      "Poids 1: 2.315 - F1 Score: 0.6931\n",
      "Poids 1: 2.316 - F1 Score: 0.6931\n",
      "Poids 1: 2.317 - F1 Score: 0.6931\n",
      "Poids 1: 2.318 - F1 Score: 0.6931\n",
      "Poids 1: 2.319 - F1 Score: 0.6931\n",
      "Poids 1: 2.320 - F1 Score: 0.6931\n",
      "Poids 1: 2.321 - F1 Score: 0.6931\n",
      "Poids 1: 2.322 - F1 Score: 0.6931\n",
      "Poids 1: 2.323 - F1 Score: 0.7059\n",
      "Poids 1: 2.324 - F1 Score: 0.7059\n",
      "Poids 1: 2.325 - F1 Score: 0.7059\n",
      "Poids 1: 2.326 - F1 Score: 0.7059\n",
      "Poids 1: 2.327 - F1 Score: 0.7059\n",
      "Poids 1: 2.328 - F1 Score: 0.7059\n",
      "Poids 1: 2.329 - F1 Score: 0.7059\n",
      "Poids 1: 2.330 - F1 Score: 0.7059\n",
      "Poids 1: 2.331 - F1 Score: 0.7059\n",
      "Poids 1: 2.332 - F1 Score: 0.7059\n",
      "Poids 1: 2.333 - F1 Score: 0.7059\n",
      "Poids 1: 2.334 - F1 Score: 0.7059\n",
      "Poids 1: 2.335 - F1 Score: 0.7059\n",
      "Poids 1: 2.336 - F1 Score: 0.7059\n",
      "Poids 1: 2.337 - F1 Score: 0.7059\n",
      "Poids 1: 2.338 - F1 Score: 0.7059\n",
      "Poids 1: 2.339 - F1 Score: 0.7059\n",
      "Poids 1: 2.340 - F1 Score: 0.7059\n",
      "Poids 1: 2.341 - F1 Score: 0.7059\n",
      "Poids 1: 2.342 - F1 Score: 0.7059\n",
      "Poids 1: 2.343 - F1 Score: 0.7059\n",
      "Poids 1: 2.344 - F1 Score: 0.7059\n",
      "Poids 1: 2.345 - F1 Score: 0.7059\n",
      "Poids 1: 2.346 - F1 Score: 0.7059\n",
      "Poids 1: 2.347 - F1 Score: 0.7059\n",
      "Poids 1: 2.348 - F1 Score: 0.7059\n",
      "Poids 1: 2.349 - F1 Score: 0.7059\n",
      "Poids 1: 2.350 - F1 Score: 0.7059\n",
      "Poids 1: 2.351 - F1 Score: 0.7059\n",
      "Poids 1: 2.352 - F1 Score: 0.7059\n",
      "Poids 1: 2.353 - F1 Score: 0.7059\n",
      "Poids 1: 2.354 - F1 Score: 0.7059\n",
      "Poids 1: 2.355 - F1 Score: 0.6990\n",
      "Poids 1: 2.356 - F1 Score: 0.6990\n",
      "Poids 1: 2.357 - F1 Score: 0.6990\n",
      "Poids 1: 2.358 - F1 Score: 0.6990\n",
      "Poids 1: 2.359 - F1 Score: 0.6990\n",
      "Poids 1: 2.360 - F1 Score: 0.6990\n",
      "Poids 1: 2.361 - F1 Score: 0.6990\n",
      "Poids 1: 2.362 - F1 Score: 0.6990\n",
      "Poids 1: 2.363 - F1 Score: 0.6990\n",
      "Poids 1: 2.364 - F1 Score: 0.6990\n",
      "Poids 1: 2.365 - F1 Score: 0.6990\n",
      "Poids 1: 2.366 - F1 Score: 0.6990\n",
      "Poids 1: 2.367 - F1 Score: 0.6990\n",
      "Poids 1: 2.368 - F1 Score: 0.6990\n",
      "Poids 1: 2.369 - F1 Score: 0.6990\n",
      "Poids 1: 2.370 - F1 Score: 0.6990\n",
      "Poids 1: 2.371 - F1 Score: 0.6990\n",
      "Poids 1: 2.372 - F1 Score: 0.6990\n",
      "Poids 1: 2.373 - F1 Score: 0.6990\n",
      "Poids 1: 2.374 - F1 Score: 0.6990\n",
      "Poids 1: 2.375 - F1 Score: 0.6990\n",
      "Poids 1: 2.376 - F1 Score: 0.6990\n",
      "Poids 1: 2.377 - F1 Score: 0.6990\n",
      "Poids 1: 2.378 - F1 Score: 0.6990\n",
      "Poids 1: 2.379 - F1 Score: 0.6990\n",
      "Poids 1: 2.380 - F1 Score: 0.6990\n",
      "Poids 1: 2.381 - F1 Score: 0.6990\n",
      "Poids 1: 2.382 - F1 Score: 0.6990\n",
      "Poids 1: 2.383 - F1 Score: 0.6990\n",
      "Poids 1: 2.384 - F1 Score: 0.6990\n",
      "Poids 1: 2.385 - F1 Score: 0.6990\n",
      "Poids 1: 2.386 - F1 Score: 0.6990\n",
      "Poids 1: 2.387 - F1 Score: 0.6990\n",
      "Poids 1: 2.388 - F1 Score: 0.6990\n",
      "Poids 1: 2.389 - F1 Score: 0.6990\n",
      "Poids 1: 2.390 - F1 Score: 0.6990\n",
      "Poids 1: 2.391 - F1 Score: 0.6990\n",
      "Poids 1: 2.392 - F1 Score: 0.6990\n",
      "Poids 1: 2.393 - F1 Score: 0.6990\n",
      "Poids 1: 2.394 - F1 Score: 0.6990\n",
      "Poids 1: 2.395 - F1 Score: 0.6990\n",
      "Poids 1: 2.396 - F1 Score: 0.6990\n",
      "Poids 1: 2.397 - F1 Score: 0.6990\n",
      "Poids 1: 2.398 - F1 Score: 0.6990\n",
      "Poids 1: 2.399 - F1 Score: 0.6990\n",
      "Poids 1: 2.400 - F1 Score: 0.6990\n",
      "Poids 1: 2.401 - F1 Score: 0.6990\n",
      "Poids 1: 2.402 - F1 Score: 0.6990\n",
      "Poids 1: 2.403 - F1 Score: 0.6990\n",
      "Poids 1: 2.404 - F1 Score: 0.6990\n",
      "Poids 1: 2.405 - F1 Score: 0.6990\n",
      "Poids 1: 2.406 - F1 Score: 0.6990\n",
      "Poids 1: 2.407 - F1 Score: 0.6990\n",
      "Poids 1: 2.408 - F1 Score: 0.6990\n",
      "Poids 1: 2.409 - F1 Score: 0.6990\n",
      "Poids 1: 2.410 - F1 Score: 0.6990\n",
      "Poids 1: 2.411 - F1 Score: 0.6990\n",
      "Poids 1: 2.412 - F1 Score: 0.6990\n",
      "Poids 1: 2.413 - F1 Score: 0.6990\n",
      "Poids 1: 2.414 - F1 Score: 0.6990\n",
      "Poids 1: 2.415 - F1 Score: 0.6990\n",
      "Poids 1: 2.416 - F1 Score: 0.6990\n",
      "Poids 1: 2.417 - F1 Score: 0.6990\n",
      "Poids 1: 2.418 - F1 Score: 0.6990\n",
      "Poids 1: 2.419 - F1 Score: 0.6990\n",
      "Poids 1: 2.420 - F1 Score: 0.6923\n",
      "Poids 1: 2.421 - F1 Score: 0.6923\n",
      "Poids 1: 2.422 - F1 Score: 0.6923\n",
      "Poids 1: 2.423 - F1 Score: 0.6923\n",
      "Poids 1: 2.424 - F1 Score: 0.6923\n",
      "Poids 1: 2.425 - F1 Score: 0.6923\n",
      "Poids 1: 2.426 - F1 Score: 0.6923\n",
      "Poids 1: 2.427 - F1 Score: 0.6923\n",
      "Poids 1: 2.428 - F1 Score: 0.6923\n",
      "Poids 1: 2.429 - F1 Score: 0.6923\n",
      "Poids 1: 2.430 - F1 Score: 0.6923\n",
      "Poids 1: 2.431 - F1 Score: 0.6923\n",
      "Poids 1: 2.432 - F1 Score: 0.6923\n",
      "Poids 1: 2.433 - F1 Score: 0.6923\n",
      "Poids 1: 2.434 - F1 Score: 0.6923\n",
      "Poids 1: 2.435 - F1 Score: 0.6923\n",
      "Poids 1: 2.436 - F1 Score: 0.6923\n",
      "Poids 1: 2.437 - F1 Score: 0.6857\n",
      "Poids 1: 2.438 - F1 Score: 0.6857\n",
      "Poids 1: 2.439 - F1 Score: 0.6857\n",
      "Poids 1: 2.440 - F1 Score: 0.6857\n",
      "Poids 1: 2.441 - F1 Score: 0.6857\n",
      "Poids 1: 2.442 - F1 Score: 0.6857\n",
      "Poids 1: 2.443 - F1 Score: 0.6857\n",
      "Poids 1: 2.444 - F1 Score: 0.6857\n",
      "Poids 1: 2.445 - F1 Score: 0.6857\n",
      "Poids 1: 2.446 - F1 Score: 0.6857\n",
      "Poids 1: 2.447 - F1 Score: 0.6857\n",
      "Poids 1: 2.448 - F1 Score: 0.6857\n",
      "Poids 1: 2.449 - F1 Score: 0.6857\n",
      "Poids 1: 2.450 - F1 Score: 0.6857\n",
      "Poids 1: 2.451 - F1 Score: 0.6857\n",
      "Poids 1: 2.452 - F1 Score: 0.6857\n",
      "Poids 1: 2.453 - F1 Score: 0.6857\n",
      "Poids 1: 2.454 - F1 Score: 0.6857\n",
      "Poids 1: 2.455 - F1 Score: 0.6857\n",
      "Poids 1: 2.456 - F1 Score: 0.6857\n",
      "Poids 1: 2.457 - F1 Score: 0.6857\n",
      "Poids 1: 2.458 - F1 Score: 0.6857\n",
      "Poids 1: 2.459 - F1 Score: 0.6857\n",
      "Poids 1: 2.460 - F1 Score: 0.6857\n",
      "Poids 1: 2.461 - F1 Score: 0.6857\n",
      "Poids 1: 2.462 - F1 Score: 0.6857\n",
      "Poids 1: 2.463 - F1 Score: 0.6857\n",
      "Poids 1: 2.464 - F1 Score: 0.6857\n",
      "Poids 1: 2.465 - F1 Score: 0.6857\n",
      "Poids 1: 2.466 - F1 Score: 0.6857\n",
      "Poids 1: 2.467 - F1 Score: 0.6857\n",
      "Poids 1: 2.468 - F1 Score: 0.6857\n",
      "Poids 1: 2.469 - F1 Score: 0.6857\n",
      "Poids 1: 2.470 - F1 Score: 0.6857\n",
      "Poids 1: 2.471 - F1 Score: 0.6857\n",
      "Poids 1: 2.472 - F1 Score: 0.6857\n",
      "Poids 1: 2.473 - F1 Score: 0.6857\n",
      "Poids 1: 2.474 - F1 Score: 0.6857\n",
      "Poids 1: 2.475 - F1 Score: 0.6857\n",
      "Poids 1: 2.476 - F1 Score: 0.6857\n",
      "Poids 1: 2.477 - F1 Score: 0.6857\n",
      "Poids 1: 2.478 - F1 Score: 0.6857\n",
      "Poids 1: 2.479 - F1 Score: 0.6857\n",
      "Poids 1: 2.480 - F1 Score: 0.6857\n",
      "Poids 1: 2.481 - F1 Score: 0.6857\n",
      "Poids 1: 2.482 - F1 Score: 0.6857\n",
      "Poids 1: 2.483 - F1 Score: 0.6857\n",
      "Poids 1: 2.484 - F1 Score: 0.6857\n",
      "Poids 1: 2.485 - F1 Score: 0.6857\n",
      "Poids 1: 2.486 - F1 Score: 0.6857\n",
      "Poids 1: 2.487 - F1 Score: 0.6857\n",
      "Poids 1: 2.488 - F1 Score: 0.6857\n",
      "Poids 1: 2.489 - F1 Score: 0.6857\n",
      "Poids 1: 2.490 - F1 Score: 0.6857\n",
      "Poids 1: 2.491 - F1 Score: 0.6857\n",
      "Poids 1: 2.492 - F1 Score: 0.6857\n",
      "Poids 1: 2.493 - F1 Score: 0.6857\n",
      "Poids 1: 2.494 - F1 Score: 0.6857\n",
      "Poids 1: 2.495 - F1 Score: 0.6857\n",
      "Poids 1: 2.496 - F1 Score: 0.6857\n",
      "Poids 1: 2.497 - F1 Score: 0.6857\n",
      "Poids 1: 2.498 - F1 Score: 0.6857\n",
      "Poids 1: 2.499 - F1 Score: 0.6857\n",
      "Poids 1: 2.500 - F1 Score: 0.6857\n",
      "Poids 1: 2.501 - F1 Score: 0.6857\n",
      "Poids 1: 2.502 - F1 Score: 0.6857\n",
      "Poids 1: 2.503 - F1 Score: 0.6857\n",
      "Poids 1: 2.504 - F1 Score: 0.6857\n",
      "Poids 1: 2.505 - F1 Score: 0.6857\n",
      "Poids 1: 2.506 - F1 Score: 0.6857\n",
      "Poids 1: 2.507 - F1 Score: 0.6857\n",
      "Poids 1: 2.508 - F1 Score: 0.6857\n",
      "Poids 1: 2.509 - F1 Score: 0.6857\n",
      "Poids 1: 2.510 - F1 Score: 0.6857\n",
      "Poids 1: 2.511 - F1 Score: 0.6857\n",
      "Poids 1: 2.512 - F1 Score: 0.6857\n",
      "Poids 1: 2.513 - F1 Score: 0.6857\n",
      "Poids 1: 2.514 - F1 Score: 0.6857\n",
      "Poids 1: 2.515 - F1 Score: 0.6857\n",
      "Poids 1: 2.516 - F1 Score: 0.6857\n",
      "Poids 1: 2.517 - F1 Score: 0.6857\n",
      "Poids 1: 2.518 - F1 Score: 0.6857\n",
      "Poids 1: 2.519 - F1 Score: 0.6857\n",
      "Poids 1: 2.520 - F1 Score: 0.6857\n",
      "Poids 1: 2.521 - F1 Score: 0.6857\n",
      "Poids 1: 2.522 - F1 Score: 0.6857\n",
      "Poids 1: 2.523 - F1 Score: 0.6857\n",
      "Poids 1: 2.524 - F1 Score: 0.6857\n",
      "Poids 1: 2.525 - F1 Score: 0.6857\n",
      "Poids 1: 2.526 - F1 Score: 0.6857\n",
      "Poids 1: 2.527 - F1 Score: 0.6857\n",
      "Poids 1: 2.528 - F1 Score: 0.6857\n",
      "Poids 1: 2.529 - F1 Score: 0.6857\n",
      "Poids 1: 2.530 - F1 Score: 0.6857\n",
      "Poids 1: 2.531 - F1 Score: 0.6857\n",
      "Poids 1: 2.532 - F1 Score: 0.6857\n",
      "Poids 1: 2.533 - F1 Score: 0.6857\n",
      "Poids 1: 2.534 - F1 Score: 0.6857\n",
      "Poids 1: 2.535 - F1 Score: 0.6857\n",
      "Poids 1: 2.536 - F1 Score: 0.6857\n",
      "Poids 1: 2.537 - F1 Score: 0.6857\n",
      "Poids 1: 2.538 - F1 Score: 0.6857\n",
      "Poids 1: 2.539 - F1 Score: 0.6857\n",
      "Poids 1: 2.540 - F1 Score: 0.6857\n",
      "Poids 1: 2.541 - F1 Score: 0.6857\n",
      "Poids 1: 2.542 - F1 Score: 0.6857\n",
      "Poids 1: 2.543 - F1 Score: 0.6857\n",
      "Poids 1: 2.544 - F1 Score: 0.6857\n",
      "Poids 1: 2.545 - F1 Score: 0.6857\n",
      "Poids 1: 2.546 - F1 Score: 0.6857\n",
      "Poids 1: 2.547 - F1 Score: 0.6857\n",
      "Poids 1: 2.548 - F1 Score: 0.6857\n",
      "Poids 1: 2.549 - F1 Score: 0.6857\n",
      "Poids 1: 2.550 - F1 Score: 0.6857\n",
      "Poids 1: 2.551 - F1 Score: 0.6857\n",
      "Poids 1: 2.552 - F1 Score: 0.6857\n",
      "Poids 1: 2.553 - F1 Score: 0.6857\n",
      "Poids 1: 2.554 - F1 Score: 0.6857\n",
      "Poids 1: 2.555 - F1 Score: 0.6857\n",
      "Poids 1: 2.556 - F1 Score: 0.6857\n",
      "Poids 1: 2.557 - F1 Score: 0.6857\n",
      "Poids 1: 2.558 - F1 Score: 0.6857\n",
      "Poids 1: 2.559 - F1 Score: 0.6857\n",
      "Poids 1: 2.560 - F1 Score: 0.6857\n",
      "Poids 1: 2.561 - F1 Score: 0.6857\n",
      "Poids 1: 2.562 - F1 Score: 0.6857\n",
      "Poids 1: 2.563 - F1 Score: 0.6857\n",
      "Poids 1: 2.564 - F1 Score: 0.6857\n",
      "Poids 1: 2.565 - F1 Score: 0.6857\n",
      "Poids 1: 2.566 - F1 Score: 0.6857\n",
      "Poids 1: 2.567 - F1 Score: 0.6857\n",
      "Poids 1: 2.568 - F1 Score: 0.6857\n",
      "Poids 1: 2.569 - F1 Score: 0.6857\n",
      "Poids 1: 2.570 - F1 Score: 0.6857\n",
      "Poids 1: 2.571 - F1 Score: 0.6857\n",
      "Poids 1: 2.572 - F1 Score: 0.6857\n",
      "Poids 1: 2.573 - F1 Score: 0.6857\n",
      "Poids 1: 2.574 - F1 Score: 0.6792\n",
      "Poids 1: 2.575 - F1 Score: 0.6792\n",
      "Poids 1: 2.576 - F1 Score: 0.6792\n",
      "Poids 1: 2.577 - F1 Score: 0.6792\n",
      "Poids 1: 2.578 - F1 Score: 0.6792\n",
      "Poids 1: 2.579 - F1 Score: 0.6792\n",
      "Poids 1: 2.580 - F1 Score: 0.6792\n",
      "Poids 1: 2.581 - F1 Score: 0.6792\n",
      "Poids 1: 2.582 - F1 Score: 0.6792\n",
      "Poids 1: 2.583 - F1 Score: 0.6792\n",
      "Poids 1: 2.584 - F1 Score: 0.6792\n",
      "Poids 1: 2.585 - F1 Score: 0.6792\n",
      "Poids 1: 2.586 - F1 Score: 0.6792\n",
      "Poids 1: 2.587 - F1 Score: 0.6792\n",
      "Poids 1: 2.588 - F1 Score: 0.6792\n",
      "Poids 1: 2.589 - F1 Score: 0.6792\n",
      "Poids 1: 2.590 - F1 Score: 0.6792\n",
      "Poids 1: 2.591 - F1 Score: 0.6792\n",
      "Poids 1: 2.592 - F1 Score: 0.6792\n",
      "Poids 1: 2.593 - F1 Score: 0.6792\n",
      "Poids 1: 2.594 - F1 Score: 0.6792\n",
      "Poids 1: 2.595 - F1 Score: 0.6792\n",
      "Poids 1: 2.596 - F1 Score: 0.6792\n",
      "Poids 1: 2.597 - F1 Score: 0.6792\n",
      "Poids 1: 2.598 - F1 Score: 0.6792\n",
      "Poids 1: 2.599 - F1 Score: 0.6792\n",
      "Poids 1: 2.600 - F1 Score: 0.6792\n",
      "Poids 1: 2.601 - F1 Score: 0.6792\n",
      "Poids 1: 2.602 - F1 Score: 0.6792\n",
      "Poids 1: 2.603 - F1 Score: 0.6792\n",
      "Poids 1: 2.604 - F1 Score: 0.6792\n",
      "Poids 1: 2.605 - F1 Score: 0.6792\n",
      "Poids 1: 2.606 - F1 Score: 0.6792\n",
      "Poids 1: 2.607 - F1 Score: 0.6792\n",
      "Poids 1: 2.608 - F1 Score: 0.6792\n",
      "Poids 1: 2.609 - F1 Score: 0.6792\n",
      "Poids 1: 2.610 - F1 Score: 0.6792\n",
      "Poids 1: 2.611 - F1 Score: 0.6792\n",
      "Poids 1: 2.612 - F1 Score: 0.6792\n",
      "Poids 1: 2.613 - F1 Score: 0.6792\n",
      "Poids 1: 2.614 - F1 Score: 0.6792\n",
      "Poids 1: 2.615 - F1 Score: 0.6792\n",
      "Poids 1: 2.616 - F1 Score: 0.6792\n",
      "Poids 1: 2.617 - F1 Score: 0.6792\n",
      "Poids 1: 2.618 - F1 Score: 0.6792\n",
      "Poids 1: 2.619 - F1 Score: 0.6792\n",
      "Poids 1: 2.620 - F1 Score: 0.6792\n",
      "Poids 1: 2.621 - F1 Score: 0.6792\n",
      "Poids 1: 2.622 - F1 Score: 0.6792\n",
      "Poids 1: 2.623 - F1 Score: 0.6792\n",
      "Poids 1: 2.624 - F1 Score: 0.6792\n",
      "Poids 1: 2.625 - F1 Score: 0.6792\n",
      "Poids 1: 2.626 - F1 Score: 0.6792\n",
      "Poids 1: 2.627 - F1 Score: 0.6792\n",
      "Poids 1: 2.628 - F1 Score: 0.6792\n",
      "Poids 1: 2.629 - F1 Score: 0.6792\n",
      "Poids 1: 2.630 - F1 Score: 0.6792\n",
      "Poids 1: 2.631 - F1 Score: 0.6792\n",
      "Poids 1: 2.632 - F1 Score: 0.6792\n",
      "Poids 1: 2.633 - F1 Score: 0.6792\n",
      "Poids 1: 2.634 - F1 Score: 0.6792\n",
      "Poids 1: 2.635 - F1 Score: 0.6792\n",
      "Poids 1: 2.636 - F1 Score: 0.6792\n",
      "Poids 1: 2.637 - F1 Score: 0.6792\n",
      "Poids 1: 2.638 - F1 Score: 0.6792\n",
      "Poids 1: 2.639 - F1 Score: 0.6792\n",
      "Poids 1: 2.640 - F1 Score: 0.6792\n",
      "Poids 1: 2.641 - F1 Score: 0.6792\n",
      "Poids 1: 2.642 - F1 Score: 0.6792\n",
      "Poids 1: 2.643 - F1 Score: 0.6792\n",
      "Poids 1: 2.644 - F1 Score: 0.6792\n",
      "Poids 1: 2.645 - F1 Score: 0.6792\n",
      "Poids 1: 2.646 - F1 Score: 0.6792\n",
      "Poids 1: 2.647 - F1 Score: 0.6792\n",
      "Poids 1: 2.648 - F1 Score: 0.6792\n",
      "Poids 1: 2.649 - F1 Score: 0.6792\n",
      "Poids 1: 2.650 - F1 Score: 0.6792\n",
      "Poids 1: 2.651 - F1 Score: 0.6792\n",
      "Poids 1: 2.652 - F1 Score: 0.6792\n",
      "Poids 1: 2.653 - F1 Score: 0.6792\n",
      "Poids 1: 2.654 - F1 Score: 0.6792\n",
      "Poids 1: 2.655 - F1 Score: 0.6792\n",
      "Poids 1: 2.656 - F1 Score: 0.6792\n",
      "Poids 1: 2.657 - F1 Score: 0.6792\n",
      "Poids 1: 2.658 - F1 Score: 0.6792\n",
      "Poids 1: 2.659 - F1 Score: 0.6792\n",
      "Poids 1: 2.660 - F1 Score: 0.6792\n",
      "Poids 1: 2.661 - F1 Score: 0.6792\n",
      "Poids 1: 2.662 - F1 Score: 0.6792\n",
      "Poids 1: 2.663 - F1 Score: 0.6792\n",
      "Poids 1: 2.664 - F1 Score: 0.6792\n",
      "Poids 1: 2.665 - F1 Score: 0.6792\n",
      "Poids 1: 2.666 - F1 Score: 0.6792\n",
      "Poids 1: 2.667 - F1 Score: 0.6792\n",
      "Poids 1: 2.668 - F1 Score: 0.6792\n",
      "Poids 1: 2.669 - F1 Score: 0.6792\n",
      "Poids 1: 2.670 - F1 Score: 0.6792\n",
      "Poids 1: 2.671 - F1 Score: 0.6792\n",
      "Poids 1: 2.672 - F1 Score: 0.6792\n",
      "Poids 1: 2.673 - F1 Score: 0.6792\n",
      "Poids 1: 2.674 - F1 Score: 0.6792\n",
      "Poids 1: 2.675 - F1 Score: 0.6792\n",
      "Poids 1: 2.676 - F1 Score: 0.6792\n",
      "Poids 1: 2.677 - F1 Score: 0.6792\n",
      "Poids 1: 2.678 - F1 Score: 0.6792\n",
      "Poids 1: 2.679 - F1 Score: 0.6792\n",
      "Poids 1: 2.680 - F1 Score: 0.6792\n",
      "Poids 1: 2.681 - F1 Score: 0.6792\n",
      "Poids 1: 2.682 - F1 Score: 0.6792\n",
      "Poids 1: 2.683 - F1 Score: 0.6792\n",
      "Poids 1: 2.684 - F1 Score: 0.6792\n",
      "Poids 1: 2.685 - F1 Score: 0.6792\n",
      "Poids 1: 2.686 - F1 Score: 0.6792\n",
      "Poids 1: 2.687 - F1 Score: 0.6792\n",
      "Poids 1: 2.688 - F1 Score: 0.6792\n",
      "Poids 1: 2.689 - F1 Score: 0.6792\n",
      "Poids 1: 2.690 - F1 Score: 0.6792\n",
      "Poids 1: 2.691 - F1 Score: 0.6792\n",
      "Poids 1: 2.692 - F1 Score: 0.6792\n",
      "Poids 1: 2.693 - F1 Score: 0.6792\n",
      "Poids 1: 2.694 - F1 Score: 0.6792\n",
      "Poids 1: 2.695 - F1 Score: 0.6792\n",
      "Poids 1: 2.696 - F1 Score: 0.6792\n",
      "Poids 1: 2.697 - F1 Score: 0.6792\n",
      "Poids 1: 2.698 - F1 Score: 0.6792\n",
      "Poids 1: 2.699 - F1 Score: 0.6792\n",
      "Poids 1: 2.700 - F1 Score: 0.6792\n",
      "Poids 1: 2.701 - F1 Score: 0.6792\n",
      "Poids 1: 2.702 - F1 Score: 0.6792\n",
      "Poids 1: 2.703 - F1 Score: 0.6792\n",
      "Poids 1: 2.704 - F1 Score: 0.6792\n",
      "Poids 1: 2.705 - F1 Score: 0.6792\n",
      "Poids 1: 2.706 - F1 Score: 0.6792\n",
      "Poids 1: 2.707 - F1 Score: 0.6792\n",
      "Poids 1: 2.708 - F1 Score: 0.6792\n",
      "Poids 1: 2.709 - F1 Score: 0.6792\n",
      "Poids 1: 2.710 - F1 Score: 0.6792\n",
      "Poids 1: 2.711 - F1 Score: 0.6792\n",
      "Poids 1: 2.712 - F1 Score: 0.6792\n",
      "Poids 1: 2.713 - F1 Score: 0.6792\n",
      "Poids 1: 2.714 - F1 Score: 0.6792\n",
      "Poids 1: 2.715 - F1 Score: 0.6792\n",
      "Poids 1: 2.716 - F1 Score: 0.6792\n",
      "Poids 1: 2.717 - F1 Score: 0.6792\n",
      "Poids 1: 2.718 - F1 Score: 0.6792\n",
      "Poids 1: 2.719 - F1 Score: 0.6792\n",
      "Poids 1: 2.720 - F1 Score: 0.6792\n",
      "Poids 1: 2.721 - F1 Score: 0.6792\n",
      "Poids 1: 2.722 - F1 Score: 0.6792\n",
      "Poids 1: 2.723 - F1 Score: 0.6792\n",
      "Poids 1: 2.724 - F1 Score: 0.6792\n",
      "Poids 1: 2.725 - F1 Score: 0.6792\n",
      "Poids 1: 2.726 - F1 Score: 0.6792\n",
      "Poids 1: 2.727 - F1 Score: 0.6792\n",
      "Poids 1: 2.728 - F1 Score: 0.6792\n",
      "Poids 1: 2.729 - F1 Score: 0.6792\n",
      "Poids 1: 2.730 - F1 Score: 0.6792\n",
      "Poids 1: 2.731 - F1 Score: 0.6792\n",
      "Poids 1: 2.732 - F1 Score: 0.6792\n",
      "Poids 1: 2.733 - F1 Score: 0.6792\n",
      "Poids 1: 2.734 - F1 Score: 0.6792\n",
      "Poids 1: 2.735 - F1 Score: 0.6792\n",
      "Poids 1: 2.736 - F1 Score: 0.6792\n",
      "Poids 1: 2.737 - F1 Score: 0.6792\n",
      "Poids 1: 2.738 - F1 Score: 0.6792\n",
      "Poids 1: 2.739 - F1 Score: 0.6792\n",
      "Poids 1: 2.740 - F1 Score: 0.6792\n",
      "Poids 1: 2.741 - F1 Score: 0.6792\n",
      "Poids 1: 2.742 - F1 Score: 0.6792\n",
      "Poids 1: 2.743 - F1 Score: 0.6792\n",
      "Poids 1: 2.744 - F1 Score: 0.6792\n",
      "Poids 1: 2.745 - F1 Score: 0.6792\n",
      "Poids 1: 2.746 - F1 Score: 0.6792\n",
      "Poids 1: 2.747 - F1 Score: 0.6792\n",
      "Poids 1: 2.748 - F1 Score: 0.6792\n",
      "Poids 1: 2.749 - F1 Score: 0.6792\n",
      "Poids 1: 2.750 - F1 Score: 0.6792\n",
      "Poids 1: 2.751 - F1 Score: 0.6792\n",
      "Poids 1: 2.752 - F1 Score: 0.6792\n",
      "Poids 1: 2.753 - F1 Score: 0.6792\n",
      "Poids 1: 2.754 - F1 Score: 0.6792\n",
      "Poids 1: 2.755 - F1 Score: 0.6792\n",
      "Poids 1: 2.756 - F1 Score: 0.6792\n",
      "Poids 1: 2.757 - F1 Score: 0.6792\n",
      "Poids 1: 2.758 - F1 Score: 0.6792\n",
      "Poids 1: 2.759 - F1 Score: 0.6792\n",
      "Poids 1: 2.760 - F1 Score: 0.6792\n",
      "Poids 1: 2.761 - F1 Score: 0.6792\n",
      "Poids 1: 2.762 - F1 Score: 0.6792\n",
      "Poids 1: 2.763 - F1 Score: 0.6792\n",
      "Poids 1: 2.764 - F1 Score: 0.6792\n",
      "Poids 1: 2.765 - F1 Score: 0.6792\n",
      "Poids 1: 2.766 - F1 Score: 0.6792\n",
      "Poids 1: 2.767 - F1 Score: 0.6792\n",
      "Poids 1: 2.768 - F1 Score: 0.6792\n",
      "Poids 1: 2.769 - F1 Score: 0.6792\n",
      "Poids 1: 2.770 - F1 Score: 0.6792\n",
      "Poids 1: 2.771 - F1 Score: 0.6792\n",
      "Poids 1: 2.772 - F1 Score: 0.6792\n",
      "Poids 1: 2.773 - F1 Score: 0.6792\n",
      "Poids 1: 2.774 - F1 Score: 0.6792\n",
      "Poids 1: 2.775 - F1 Score: 0.6792\n",
      "Poids 1: 2.776 - F1 Score: 0.6792\n",
      "Poids 1: 2.777 - F1 Score: 0.6792\n",
      "Poids 1: 2.778 - F1 Score: 0.6792\n",
      "Poids 1: 2.779 - F1 Score: 0.6792\n",
      "Poids 1: 2.780 - F1 Score: 0.6792\n",
      "Poids 1: 2.781 - F1 Score: 0.6792\n",
      "Poids 1: 2.782 - F1 Score: 0.6792\n",
      "Poids 1: 2.783 - F1 Score: 0.6792\n",
      "Poids 1: 2.784 - F1 Score: 0.6792\n",
      "Poids 1: 2.785 - F1 Score: 0.6792\n",
      "Poids 1: 2.786 - F1 Score: 0.6792\n",
      "Poids 1: 2.787 - F1 Score: 0.6792\n",
      "Poids 1: 2.788 - F1 Score: 0.6792\n",
      "Poids 1: 2.789 - F1 Score: 0.6792\n",
      "Poids 1: 2.790 - F1 Score: 0.6792\n",
      "Poids 1: 2.791 - F1 Score: 0.6792\n",
      "Poids 1: 2.792 - F1 Score: 0.6792\n",
      "Poids 1: 2.793 - F1 Score: 0.6792\n",
      "Poids 1: 2.794 - F1 Score: 0.6792\n",
      "Poids 1: 2.795 - F1 Score: 0.6792\n",
      "Poids 1: 2.796 - F1 Score: 0.6792\n",
      "Poids 1: 2.797 - F1 Score: 0.6792\n",
      "Poids 1: 2.798 - F1 Score: 0.6792\n",
      "Poids 1: 2.799 - F1 Score: 0.6792\n",
      "Poids 1: 2.800 - F1 Score: 0.6792\n",
      "Poids 1: 2.801 - F1 Score: 0.6792\n",
      "Poids 1: 2.802 - F1 Score: 0.6792\n",
      "Poids 1: 2.803 - F1 Score: 0.6792\n",
      "Poids 1: 2.804 - F1 Score: 0.6729\n",
      "Poids 1: 2.805 - F1 Score: 0.6729\n",
      "Poids 1: 2.806 - F1 Score: 0.6729\n",
      "Poids 1: 2.807 - F1 Score: 0.6729\n",
      "Poids 1: 2.808 - F1 Score: 0.6729\n",
      "Poids 1: 2.809 - F1 Score: 0.6729\n",
      "Poids 1: 2.810 - F1 Score: 0.6729\n",
      "Poids 1: 2.811 - F1 Score: 0.6729\n",
      "Poids 1: 2.812 - F1 Score: 0.6729\n",
      "Poids 1: 2.813 - F1 Score: 0.6729\n",
      "Poids 1: 2.814 - F1 Score: 0.6729\n",
      "Poids 1: 2.815 - F1 Score: 0.6729\n",
      "Poids 1: 2.816 - F1 Score: 0.6729\n",
      "Poids 1: 2.817 - F1 Score: 0.6729\n",
      "Poids 1: 2.818 - F1 Score: 0.6729\n",
      "Poids 1: 2.819 - F1 Score: 0.6729\n",
      "Poids 1: 2.820 - F1 Score: 0.6729\n",
      "Poids 1: 2.821 - F1 Score: 0.6729\n",
      "Poids 1: 2.822 - F1 Score: 0.6729\n",
      "Poids 1: 2.823 - F1 Score: 0.6729\n",
      "Poids 1: 2.824 - F1 Score: 0.6729\n",
      "Poids 1: 2.825 - F1 Score: 0.6729\n",
      "Poids 1: 2.826 - F1 Score: 0.6729\n",
      "Poids 1: 2.827 - F1 Score: 0.6729\n",
      "Poids 1: 2.828 - F1 Score: 0.6729\n",
      "Poids 1: 2.829 - F1 Score: 0.6729\n",
      "Poids 1: 2.830 - F1 Score: 0.6729\n",
      "Poids 1: 2.831 - F1 Score: 0.6729\n",
      "Poids 1: 2.832 - F1 Score: 0.6729\n",
      "Poids 1: 2.833 - F1 Score: 0.6729\n",
      "Poids 1: 2.834 - F1 Score: 0.6729\n",
      "Poids 1: 2.835 - F1 Score: 0.6667\n",
      "Poids 1: 2.836 - F1 Score: 0.6667\n",
      "Poids 1: 2.837 - F1 Score: 0.6667\n",
      "Poids 1: 2.838 - F1 Score: 0.6667\n",
      "Poids 1: 2.839 - F1 Score: 0.6667\n",
      "Poids 1: 2.840 - F1 Score: 0.6667\n",
      "Poids 1: 2.841 - F1 Score: 0.6667\n",
      "Poids 1: 2.842 - F1 Score: 0.6667\n",
      "Poids 1: 2.843 - F1 Score: 0.6667\n",
      "Poids 1: 2.844 - F1 Score: 0.6667\n",
      "Poids 1: 2.845 - F1 Score: 0.6667\n",
      "Poids 1: 2.846 - F1 Score: 0.6667\n",
      "Poids 1: 2.847 - F1 Score: 0.6667\n",
      "Poids 1: 2.848 - F1 Score: 0.6667\n",
      "Poids 1: 2.849 - F1 Score: 0.6667\n",
      "Poids 1: 2.850 - F1 Score: 0.6667\n",
      "Poids 1: 2.851 - F1 Score: 0.6667\n",
      "Poids 1: 2.852 - F1 Score: 0.6667\n",
      "Poids 1: 2.853 - F1 Score: 0.6667\n",
      "Poids 1: 2.854 - F1 Score: 0.6667\n",
      "Poids 1: 2.855 - F1 Score: 0.6667\n",
      "Poids 1: 2.856 - F1 Score: 0.6667\n",
      "Poids 1: 2.857 - F1 Score: 0.6667\n",
      "Poids 1: 2.858 - F1 Score: 0.6667\n",
      "Poids 1: 2.859 - F1 Score: 0.6667\n",
      "Poids 1: 2.860 - F1 Score: 0.6667\n",
      "Poids 1: 2.861 - F1 Score: 0.6789\n",
      "Poids 1: 2.862 - F1 Score: 0.6789\n",
      "Poids 1: 2.863 - F1 Score: 0.6789\n",
      "Poids 1: 2.864 - F1 Score: 0.6789\n",
      "Poids 1: 2.865 - F1 Score: 0.6789\n",
      "Poids 1: 2.866 - F1 Score: 0.6789\n",
      "Poids 1: 2.867 - F1 Score: 0.6789\n",
      "Poids 1: 2.868 - F1 Score: 0.6789\n",
      "Poids 1: 2.869 - F1 Score: 0.6789\n",
      "Poids 1: 2.870 - F1 Score: 0.6789\n",
      "Poids 1: 2.871 - F1 Score: 0.6789\n",
      "Poids 1: 2.872 - F1 Score: 0.6789\n",
      "Poids 1: 2.873 - F1 Score: 0.6789\n",
      "Poids 1: 2.874 - F1 Score: 0.6789\n",
      "Poids 1: 2.875 - F1 Score: 0.6789\n",
      "Poids 1: 2.876 - F1 Score: 0.6789\n",
      "Poids 1: 2.877 - F1 Score: 0.6789\n",
      "Poids 1: 2.878 - F1 Score: 0.6789\n",
      "Poids 1: 2.879 - F1 Score: 0.6789\n",
      "Poids 1: 2.880 - F1 Score: 0.6789\n",
      "Poids 1: 2.881 - F1 Score: 0.6789\n",
      "Poids 1: 2.882 - F1 Score: 0.6789\n",
      "Poids 1: 2.883 - F1 Score: 0.6789\n",
      "Poids 1: 2.884 - F1 Score: 0.6789\n",
      "Poids 1: 2.885 - F1 Score: 0.6789\n",
      "Poids 1: 2.886 - F1 Score: 0.6789\n",
      "Poids 1: 2.887 - F1 Score: 0.6789\n",
      "Poids 1: 2.888 - F1 Score: 0.6789\n",
      "Poids 1: 2.889 - F1 Score: 0.6789\n",
      "Poids 1: 2.890 - F1 Score: 0.6789\n",
      "Poids 1: 2.891 - F1 Score: 0.6789\n",
      "Poids 1: 2.892 - F1 Score: 0.6789\n",
      "Poids 1: 2.893 - F1 Score: 0.6789\n",
      "Poids 1: 2.894 - F1 Score: 0.6789\n",
      "Poids 1: 2.895 - F1 Score: 0.6789\n",
      "Poids 1: 2.896 - F1 Score: 0.6789\n",
      "Poids 1: 2.897 - F1 Score: 0.6789\n",
      "Poids 1: 2.898 - F1 Score: 0.6789\n",
      "Poids 1: 2.899 - F1 Score: 0.6789\n",
      "Poids 1: 2.900 - F1 Score: 0.6789\n",
      "Poids 1: 2.901 - F1 Score: 0.6789\n",
      "Poids 1: 2.902 - F1 Score: 0.6789\n",
      "Poids 1: 2.903 - F1 Score: 0.6789\n",
      "Poids 1: 2.904 - F1 Score: 0.6789\n",
      "Poids 1: 2.905 - F1 Score: 0.6789\n",
      "Poids 1: 2.906 - F1 Score: 0.6789\n",
      "Poids 1: 2.907 - F1 Score: 0.6789\n",
      "Poids 1: 2.908 - F1 Score: 0.6789\n",
      "Poids 1: 2.909 - F1 Score: 0.6789\n",
      "Poids 1: 2.910 - F1 Score: 0.6789\n",
      "Poids 1: 2.911 - F1 Score: 0.6789\n",
      "Poids 1: 2.912 - F1 Score: 0.6789\n",
      "Poids 1: 2.913 - F1 Score: 0.6789\n",
      "Poids 1: 2.914 - F1 Score: 0.6789\n",
      "Poids 1: 2.915 - F1 Score: 0.6789\n",
      "Poids 1: 2.916 - F1 Score: 0.6789\n",
      "Poids 1: 2.917 - F1 Score: 0.6789\n",
      "Poids 1: 2.918 - F1 Score: 0.6789\n",
      "Poids 1: 2.919 - F1 Score: 0.6789\n",
      "Poids 1: 2.920 - F1 Score: 0.6789\n",
      "Poids 1: 2.921 - F1 Score: 0.6789\n",
      "Poids 1: 2.922 - F1 Score: 0.6789\n",
      "Poids 1: 2.923 - F1 Score: 0.6789\n",
      "Poids 1: 2.924 - F1 Score: 0.6789\n",
      "Poids 1: 2.925 - F1 Score: 0.6789\n",
      "Poids 1: 2.926 - F1 Score: 0.6789\n",
      "Poids 1: 2.927 - F1 Score: 0.6789\n",
      "Poids 1: 2.928 - F1 Score: 0.6789\n",
      "Poids 1: 2.929 - F1 Score: 0.6789\n",
      "Poids 1: 2.930 - F1 Score: 0.6789\n",
      "Poids 1: 2.931 - F1 Score: 0.6789\n",
      "Poids 1: 2.932 - F1 Score: 0.6789\n",
      "Poids 1: 2.933 - F1 Score: 0.6789\n",
      "Poids 1: 2.934 - F1 Score: 0.6789\n",
      "Poids 1: 2.935 - F1 Score: 0.6789\n",
      "Poids 1: 2.936 - F1 Score: 0.6789\n",
      "Poids 1: 2.937 - F1 Score: 0.6789\n",
      "Poids 1: 2.938 - F1 Score: 0.6789\n",
      "Poids 1: 2.939 - F1 Score: 0.6789\n",
      "Poids 1: 2.940 - F1 Score: 0.6789\n",
      "Poids 1: 2.941 - F1 Score: 0.6789\n",
      "Poids 1: 2.942 - F1 Score: 0.6789\n",
      "Poids 1: 2.943 - F1 Score: 0.6789\n",
      "Poids 1: 2.944 - F1 Score: 0.6789\n",
      "Poids 1: 2.945 - F1 Score: 0.6789\n",
      "Poids 1: 2.946 - F1 Score: 0.6789\n",
      "Poids 1: 2.947 - F1 Score: 0.6789\n",
      "Poids 1: 2.948 - F1 Score: 0.6789\n",
      "Poids 1: 2.949 - F1 Score: 0.6789\n",
      "Poids 1: 2.950 - F1 Score: 0.6789\n",
      "Poids 1: 2.951 - F1 Score: 0.6789\n",
      "Poids 1: 2.952 - F1 Score: 0.6789\n",
      "Poids 1: 2.953 - F1 Score: 0.6789\n",
      "Poids 1: 2.954 - F1 Score: 0.6789\n",
      "Poids 1: 2.955 - F1 Score: 0.6789\n",
      "Poids 1: 2.956 - F1 Score: 0.6789\n",
      "Poids 1: 2.957 - F1 Score: 0.6789\n",
      "Poids 1: 2.958 - F1 Score: 0.6789\n",
      "Poids 1: 2.959 - F1 Score: 0.6789\n",
      "Poids 1: 2.960 - F1 Score: 0.6789\n",
      "Poids 1: 2.961 - F1 Score: 0.6789\n",
      "Poids 1: 2.962 - F1 Score: 0.6789\n",
      "Poids 1: 2.963 - F1 Score: 0.6789\n",
      "Poids 1: 2.964 - F1 Score: 0.6789\n",
      "Poids 1: 2.965 - F1 Score: 0.6789\n",
      "Poids 1: 2.966 - F1 Score: 0.6789\n",
      "Poids 1: 2.967 - F1 Score: 0.6789\n",
      "Poids 1: 2.968 - F1 Score: 0.6789\n",
      "Poids 1: 2.969 - F1 Score: 0.6789\n",
      "Poids 1: 2.970 - F1 Score: 0.6789\n",
      "Poids 1: 2.971 - F1 Score: 0.6789\n",
      "Poids 1: 2.972 - F1 Score: 0.6789\n",
      "Poids 1: 2.973 - F1 Score: 0.6789\n",
      "Poids 1: 2.974 - F1 Score: 0.6789\n",
      "Poids 1: 2.975 - F1 Score: 0.6789\n",
      "Poids 1: 2.976 - F1 Score: 0.6789\n",
      "Poids 1: 2.977 - F1 Score: 0.6789\n",
      "Poids 1: 2.978 - F1 Score: 0.6789\n",
      "Poids 1: 2.979 - F1 Score: 0.6789\n",
      "Poids 1: 2.980 - F1 Score: 0.6789\n",
      "Poids 1: 2.981 - F1 Score: 0.6789\n",
      "Poids 1: 2.982 - F1 Score: 0.6789\n",
      "Poids 1: 2.983 - F1 Score: 0.6789\n",
      "Poids 1: 2.984 - F1 Score: 0.6789\n",
      "Poids 1: 2.985 - F1 Score: 0.6789\n",
      "Poids 1: 2.986 - F1 Score: 0.6789\n",
      "Poids 1: 2.987 - F1 Score: 0.6789\n",
      "Poids 1: 2.988 - F1 Score: 0.6789\n",
      "Poids 1: 2.989 - F1 Score: 0.6789\n",
      "Poids 1: 2.990 - F1 Score: 0.6789\n",
      "Poids 1: 2.991 - F1 Score: 0.6789\n",
      "Poids 1: 2.992 - F1 Score: 0.6789\n",
      "Poids 1: 2.993 - F1 Score: 0.6789\n",
      "Poids 1: 2.994 - F1 Score: 0.6789\n",
      "Poids 1: 2.995 - F1 Score: 0.6789\n",
      "Poids 1: 2.996 - F1 Score: 0.6789\n",
      "Poids 1: 2.997 - F1 Score: 0.6789\n",
      "Poids 1: 2.998 - F1 Score: 0.6789\n",
      "Poids 1: 2.999 - F1 Score: 0.6789\n",
      "\n",
      "🏆 Meilleur modèle ultra-précis :\n",
      "F1 Score : 0.7381\n",
      "Poids optimaux : {0: 1, 1: np.float64(1.2519999999999722)}\n",
      "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
      "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
      "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
      "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
      "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
      "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
      "16            knn_balanced  0.715447   0.711274  0.715447          0.578313   \n",
      "17     logreg_best_weights  0.821138   0.819552  0.821138          0.738095   \n",
      "\n",
      "    F1 Score global weighted  \n",
      "0                   0.802499  \n",
      "1                   0.718675  \n",
      "2                   0.778541  \n",
      "3                   0.758723  \n",
      "4                   0.729328  \n",
      "5                   0.738346  \n",
      "6                   0.823837  \n",
      "7                   0.773504  \n",
      "8                   0.721993  \n",
      "9                   0.783727  \n",
      "10                  0.753123  \n",
      "11                  0.778541  \n",
      "12                  0.756098  \n",
      "13                  0.776626  \n",
      "14                  0.739837  \n",
      "15                  0.777565  \n",
      "16                  0.712923  \n",
      "17                  0.820113  \n"
     ]
    }
   ],
   "source": [
    "weights_range = np.arange(1, 3, 0.001)  # précision très fine\n",
    "\n",
    "best_model = None\n",
    "best_f1_score = -np.inf\n",
    "best_weights = None\n",
    "\n",
    "for weight in weights_range:\n",
    "    class_weights = {0: 1, 1: weight}\n",
    "    logreg = LogisticRegression(class_weight=class_weights, solver='liblinear', max_iter=1000, random_state=42)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred, pos_label=1)\n",
    "\n",
    "    print(f\"Poids 1: {weight:.3f} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        logreg_best_weights = logreg\n",
    "        best_weights = class_weights\n",
    "\n",
    "print(\"\\n🏆 Meilleur modèle ultra-précis :\")\n",
    "print(f\"F1 Score : {best_f1_score:.4f}\")\n",
    "print(f\"Poids optimaux : {best_weights}\")\n",
    "\n",
    "df_metrics_val = dataframe_metrics(logreg_best_weights, X_val, y_val, model_name=\"logreg_best_weights\", df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score class 1</th>\n",
       "      <th>F1 Score global weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>logreg_best_weights</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.819552</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.820113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear_svm</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.823837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svm_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.808667</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.777565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logistic_reg_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.792041</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.783727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adaboost_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.791050</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.776626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_reg</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.801960</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.802499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>random_forest_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgboost_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.756098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.775918</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.773504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision_tree_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.751839</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.753123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.758613</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.758723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm_balanced</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.739837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.737260</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.738346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.724255</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.718675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.727850</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.729328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.720801</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.721993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>knn_balanced</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.712923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
       "17     logreg_best_weights  0.821138   0.819552  0.821138          0.738095   \n",
       "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
       "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
       "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
       "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
       "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
       "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
       "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
       "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
       "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
       "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
       "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
       "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
       "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
       "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
       "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
       "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
       "16            knn_balanced  0.715447   0.711274  0.715447          0.578313   \n",
       "\n",
       "    F1 Score global weighted  \n",
       "17                  0.820113  \n",
       "6                   0.823837  \n",
       "15                  0.777565  \n",
       "9                   0.783727  \n",
       "13                  0.776626  \n",
       "0                   0.802499  \n",
       "2                   0.778541  \n",
       "11                  0.778541  \n",
       "12                  0.756098  \n",
       "7                   0.773504  \n",
       "10                  0.753123  \n",
       "3                   0.758723  \n",
       "14                  0.739837  \n",
       "5                   0.738346  \n",
       "1                   0.718675  \n",
       "4                   0.729328  \n",
       "8                   0.721993  \n",
       "16                  0.712923  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_val.sort_values(by='F1 Score class 1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids classe 1: 2.0 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.0 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.0 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.0 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.0 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.0 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.1 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.1 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.1 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.1 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.1 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.1 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.1 - F1 Score: 0.7010\n",
      "Poids classe 1: 2.1 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.1 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.1 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.2 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.2 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.2 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.2 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.2 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.2 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.2 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.2 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.2 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.2 - F1 Score: 0.7071\n",
      "Poids classe 1: 2.3 - F1 Score: 0.7216\n",
      "Poids classe 1: 2.3 - F1 Score: 0.7216\n",
      "Poids classe 1: 2.3 - F1 Score: 0.7216\n",
      "Poids classe 1: 2.3 - F1 Score: 0.7216\n",
      "Poids classe 1: 2.3 - F1 Score: 0.7216\n",
      "Poids classe 1: 2.3 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.3 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.3 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.3 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.3 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.4 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.4 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.4 - F1 Score: 0.7000\n",
      "Poids classe 1: 2.4 - F1 Score: 0.7000\n",
      "Poids classe 1: 2.4 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.4 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.4 - F1 Score: 0.7216\n",
      "Poids classe 1: 2.4 - F1 Score: 0.7216\n",
      "Poids classe 1: 2.4 - F1 Score: 0.7216\n",
      "Poids classe 1: 2.4 - F1 Score: 0.7216\n",
      "Poids classe 1: 2.5 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.5 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.5 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.5 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.5 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.5 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.5 - F1 Score: 0.7255\n",
      "Poids classe 1: 2.5 - F1 Score: 0.7255\n",
      "Poids classe 1: 2.5 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.5 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.6 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.6 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.6 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.6 - F1 Score: 0.7273\n",
      "Poids classe 1: 2.6 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.6 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.6 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.6 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.6 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.6 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.7 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.7 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.7 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.7 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.7 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.7 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.7 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.7 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.7 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.7 - F1 Score: 0.7115\n",
      "Poids classe 1: 2.8 - F1 Score: 0.7000\n",
      "Poids classe 1: 2.8 - F1 Score: 0.7000\n",
      "Poids classe 1: 2.8 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.8 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.8 - F1 Score: 0.7129\n",
      "Poids classe 1: 2.8 - F1 Score: 0.7184\n",
      "Poids classe 1: 2.8 - F1 Score: 0.7184\n",
      "Poids classe 1: 2.8 - F1 Score: 0.7184\n",
      "Poids classe 1: 2.8 - F1 Score: 0.7184\n",
      "Poids classe 1: 2.8 - F1 Score: 0.7184\n",
      "Poids classe 1: 2.9 - F1 Score: 0.7184\n",
      "Poids classe 1: 2.9 - F1 Score: 0.7184\n",
      "Poids classe 1: 2.9 - F1 Score: 0.7184\n",
      "Poids classe 1: 2.9 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.9 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.9 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.9 - F1 Score: 0.7059\n",
      "Poids classe 1: 2.9 - F1 Score: 0.7184\n",
      "Poids classe 1: 2.9 - F1 Score: 0.7184\n",
      "Poids classe 1: 2.9 - F1 Score: 0.7184\n",
      "Poids classe 1: 3.0 - F1 Score: 0.7184\n",
      "Poids classe 1: 3.0 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.0 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.0 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.0 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.0 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.0 - F1 Score: 0.7238\n",
      "Poids classe 1: 3.0 - F1 Score: 0.7238\n",
      "Poids classe 1: 3.0 - F1 Score: 0.7238\n",
      "Poids classe 1: 3.0 - F1 Score: 0.7184\n",
      "Poids classe 1: 3.1 - F1 Score: 0.7238\n",
      "Poids classe 1: 3.1 - F1 Score: 0.7238\n",
      "Poids classe 1: 3.1 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.1 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.1 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.1 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.1 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.1 - F1 Score: 0.7308\n",
      "Poids classe 1: 3.1 - F1 Score: 0.7115\n",
      "Poids classe 1: 3.1 - F1 Score: 0.7115\n",
      "Poids classe 1: 3.2 - F1 Score: 0.7115\n",
      "Poids classe 1: 3.2 - F1 Score: 0.7115\n",
      "Poids classe 1: 3.2 - F1 Score: 0.7184\n",
      "Poids classe 1: 3.2 - F1 Score: 0.6786\n",
      "Poids classe 1: 3.2 - F1 Score: 0.6847\n",
      "Poids classe 1: 3.2 - F1 Score: 0.6852\n",
      "Poids classe 1: 3.2 - F1 Score: 0.6852\n",
      "Poids classe 1: 3.2 - F1 Score: 0.6847\n",
      "Poids classe 1: 3.2 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.2 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.3 - F1 Score: 0.6727\n",
      "Poids classe 1: 3.3 - F1 Score: 0.6727\n",
      "Poids classe 1: 3.3 - F1 Score: 0.6727\n",
      "Poids classe 1: 3.3 - F1 Score: 0.6727\n",
      "Poids classe 1: 3.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.4 - F1 Score: 0.6789\n",
      "Poids classe 1: 3.4 - F1 Score: 0.6789\n",
      "Poids classe 1: 3.4 - F1 Score: 0.6727\n",
      "Poids classe 1: 3.4 - F1 Score: 0.6727\n",
      "Poids classe 1: 3.4 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.4 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.4 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.4 - F1 Score: 0.6667\n",
      "Poids classe 1: 3.4 - F1 Score: 0.6609\n",
      "Poids classe 1: 3.4 - F1 Score: 0.6609\n",
      "Poids classe 1: 3.5 - F1 Score: 0.6609\n",
      "Poids classe 1: 3.5 - F1 Score: 0.6609\n",
      "Poids classe 1: 3.5 - F1 Score: 0.6609\n",
      "Poids classe 1: 3.5 - F1 Score: 0.6609\n",
      "Poids classe 1: 3.5 - F1 Score: 0.6609\n",
      "Poids classe 1: 3.5 - F1 Score: 0.6789\n",
      "Poids classe 1: 3.5 - F1 Score: 0.6789\n",
      "Poids classe 1: 3.5 - F1 Score: 0.6789\n",
      "Poids classe 1: 3.5 - F1 Score: 0.6789\n",
      "Poids classe 1: 3.5 - F1 Score: 0.6727\n",
      "Poids classe 1: 3.6 - F1 Score: 0.6847\n",
      "Poids classe 1: 3.6 - F1 Score: 0.6847\n",
      "Poids classe 1: 3.6 - F1 Score: 0.6786\n",
      "Poids classe 1: 3.6 - F1 Score: 0.6847\n",
      "Poids classe 1: 3.6 - F1 Score: 0.6847\n",
      "Poids classe 1: 3.6 - F1 Score: 0.6847\n",
      "Poids classe 1: 3.6 - F1 Score: 0.6847\n",
      "Poids classe 1: 3.6 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.6 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.6 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.7 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.7 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.7 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.7 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.7 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.7 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.7 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.7 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.7 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.7 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.8 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.8 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.8 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.8 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.8 - F1 Score: 0.6903\n",
      "Poids classe 1: 3.8 - F1 Score: 0.6786\n",
      "Poids classe 1: 3.8 - F1 Score: 0.6786\n",
      "Poids classe 1: 3.8 - F1 Score: 0.6847\n",
      "Poids classe 1: 3.8 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.8 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.9 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.9 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.9 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.9 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.9 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.9 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.9 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.9 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.9 - F1 Score: 0.6726\n",
      "Poids classe 1: 3.9 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.0 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.0 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.0 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.0 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.0 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.0 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.0 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.0 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.0 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.0 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.1 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.1 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.1 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.1 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.1 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.1 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.1 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.1 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.1 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.1 - F1 Score: 0.6786\n",
      "Poids classe 1: 4.2 - F1 Score: 0.6786\n",
      "Poids classe 1: 4.2 - F1 Score: 0.6786\n",
      "Poids classe 1: 4.2 - F1 Score: 0.6786\n",
      "Poids classe 1: 4.2 - F1 Score: 0.6786\n",
      "Poids classe 1: 4.2 - F1 Score: 0.6786\n",
      "Poids classe 1: 4.2 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.2 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.2 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.2 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.2 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.3 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.3 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.3 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.4 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.4 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.4 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.4 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.4 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.4 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.4 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.4 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.4 - F1 Score: 0.6726\n",
      "Poids classe 1: 4.4 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.5 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.5 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.5 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.5 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.5 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.5 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.5 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.5 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.5 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.5 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.6 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.6 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.6 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.6 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.6 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.6 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.6 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.6 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.6 - F1 Score: 0.6609\n",
      "Poids classe 1: 4.6 - F1 Score: 0.6609\n",
      "Poids classe 1: 4.7 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.7 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.7 - F1 Score: 0.6609\n",
      "Poids classe 1: 4.7 - F1 Score: 0.6609\n",
      "Poids classe 1: 4.7 - F1 Score: 0.6609\n",
      "Poids classe 1: 4.7 - F1 Score: 0.6609\n",
      "Poids classe 1: 4.7 - F1 Score: 0.6609\n",
      "Poids classe 1: 4.7 - F1 Score: 0.6609\n",
      "Poids classe 1: 4.7 - F1 Score: 0.6609\n",
      "Poids classe 1: 4.7 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.8 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.8 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.8 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.8 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.8 - F1 Score: 0.6667\n",
      "Poids classe 1: 4.8 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.8 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.8 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.8 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.8 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.9 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.9 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.9 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.9 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.9 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.9 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.9 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.9 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.9 - F1 Score: 0.6555\n",
      "Poids classe 1: 4.9 - F1 Score: 0.6552\n",
      "Poids classe 1: 5.0 - F1 Score: 0.6552\n",
      "Poids classe 1: 5.0 - F1 Score: 0.6552\n",
      "Poids classe 1: 5.0 - F1 Score: 0.6667\n",
      "Poids classe 1: 5.0 - F1 Score: 0.6667\n",
      "\n",
      "✅ Meilleur modèle AdaBoost :\n",
      "F1 Score : 0.7308\n",
      "Poids optimaux : {0: 1, 1: np.float64(2.9699999999999793)}\n",
      "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
      "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
      "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
      "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
      "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
      "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
      "16            knn_balanced  0.715447   0.711274  0.715447          0.578313   \n",
      "17     logreg_best_weights  0.821138   0.819552  0.821138          0.738095   \n",
      "18   adaboost_best_weights  0.772358   0.815734  0.772358          0.730769   \n",
      "\n",
      "    F1 Score global weighted  \n",
      "0                   0.802499  \n",
      "1                   0.718675  \n",
      "2                   0.778541  \n",
      "3                   0.758723  \n",
      "4                   0.729328  \n",
      "5                   0.738346  \n",
      "6                   0.823837  \n",
      "7                   0.773504  \n",
      "8                   0.721993  \n",
      "9                   0.783727  \n",
      "10                  0.753123  \n",
      "11                  0.778541  \n",
      "12                  0.756098  \n",
      "13                  0.776626  \n",
      "14                  0.739837  \n",
      "15                  0.777565  \n",
      "16                  0.712923  \n",
      "17                  0.820113  \n",
      "18                  0.777630  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Liste des poids à tester entre 9.0 et 11.0 avec un pas de 0.5\n",
    "weights_range = np.arange(2, 5, 0.01)\n",
    "\n",
    "best_model = None\n",
    "best_f1_score = -np.inf\n",
    "best_weights = None\n",
    "\n",
    "for w in weights_range:\n",
    "    class_weights = {0: 1, 1: w}\n",
    "    sample_weight = compute_sample_weight(class_weight=class_weights, y=y_train)\n",
    "\n",
    "    ada_model = AdaBoostClassifier(n_estimators=50, random_state=42, learning_rate=1.0)\n",
    "    ada_model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "    y_pred = ada_model.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred, pos_label=1)\n",
    "\n",
    "    print(f\"Poids classe 1: {w:.1f} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        adaboost_best_weights = ada_model\n",
    "        best_weights = class_weights\n",
    "\n",
    "print(\"\\n✅ Meilleur modèle AdaBoost :\")\n",
    "print(f\"F1 Score : {best_f1_score:.4f}\")\n",
    "print(f\"Poids optimaux : {best_weights}\")\n",
    "\n",
    "# Évaluer et stocker les métriques du meilleur modèle\n",
    "df_metrics_val = dataframe_metrics(adaboost_best_weights, X_val, y_val, model_name=\"adaboost_best_weights\", df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score class 1</th>\n",
       "      <th>F1 Score global weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>logreg_best_weights</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.819552</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.820113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adaboost_best_weights</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.777630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear_svm</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.823837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svm_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.808667</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.777565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logistic_reg_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.792041</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.783727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adaboost_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.791050</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.776626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_reg</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.801960</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.802499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>random_forest_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgboost_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.756098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.775918</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.773504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision_tree_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.751839</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.753123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.758613</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.758723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm_balanced</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.739837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.737260</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.738346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.724255</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.718675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.727850</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.729328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.720801</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.721993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>knn_balanced</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.712923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
       "17     logreg_best_weights  0.821138   0.819552  0.821138          0.738095   \n",
       "18   adaboost_best_weights  0.772358   0.815734  0.772358          0.730769   \n",
       "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
       "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
       "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
       "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
       "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
       "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
       "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
       "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
       "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
       "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
       "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
       "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
       "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
       "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
       "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
       "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
       "16            knn_balanced  0.715447   0.711274  0.715447          0.578313   \n",
       "\n",
       "    F1 Score global weighted  \n",
       "17                  0.820113  \n",
       "18                  0.777630  \n",
       "6                   0.823837  \n",
       "15                  0.777565  \n",
       "9                   0.783727  \n",
       "13                  0.776626  \n",
       "0                   0.802499  \n",
       "11                  0.778541  \n",
       "2                   0.778541  \n",
       "12                  0.756098  \n",
       "7                   0.773504  \n",
       "10                  0.753123  \n",
       "3                   0.758723  \n",
       "14                  0.739837  \n",
       "5                   0.738346  \n",
       "1                   0.718675  \n",
       "4                   0.729328  \n",
       "8                   0.721993  \n",
       "16                  0.712923  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_val.sort_values(by='F1 Score class 1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids 1: 1.000 - F1 Score: 0.6494\n",
      "Poids 1: 1.010 - F1 Score: 0.6494\n",
      "Poids 1: 1.020 - F1 Score: 0.6494\n",
      "Poids 1: 1.030 - F1 Score: 0.6494\n",
      "Poids 1: 1.040 - F1 Score: 0.6494\n",
      "Poids 1: 1.050 - F1 Score: 0.6494\n",
      "Poids 1: 1.060 - F1 Score: 0.6667\n",
      "Poids 1: 1.070 - F1 Score: 0.6667\n",
      "Poids 1: 1.080 - F1 Score: 0.6667\n",
      "Poids 1: 1.090 - F1 Score: 0.6667\n",
      "Poids 1: 1.100 - F1 Score: 0.6667\n",
      "Poids 1: 1.110 - F1 Score: 0.6667\n",
      "Poids 1: 1.120 - F1 Score: 0.6914\n",
      "Poids 1: 1.130 - F1 Score: 0.6988\n",
      "Poids 1: 1.140 - F1 Score: 0.6988\n",
      "Poids 1: 1.150 - F1 Score: 0.6988\n",
      "Poids 1: 1.160 - F1 Score: 0.6988\n",
      "Poids 1: 1.170 - F1 Score: 0.6988\n",
      "Poids 1: 1.180 - F1 Score: 0.6988\n",
      "Poids 1: 1.190 - F1 Score: 0.6988\n",
      "Poids 1: 1.200 - F1 Score: 0.6824\n",
      "Poids 1: 1.210 - F1 Score: 0.6824\n",
      "Poids 1: 1.220 - F1 Score: 0.6824\n",
      "Poids 1: 1.230 - F1 Score: 0.6824\n",
      "Poids 1: 1.240 - F1 Score: 0.6824\n",
      "Poids 1: 1.250 - F1 Score: 0.6824\n",
      "Poids 1: 1.260 - F1 Score: 0.6824\n",
      "Poids 1: 1.270 - F1 Score: 0.6824\n",
      "Poids 1: 1.280 - F1 Score: 0.6824\n",
      "Poids 1: 1.290 - F1 Score: 0.6824\n",
      "Poids 1: 1.300 - F1 Score: 0.7126\n",
      "Poids 1: 1.310 - F1 Score: 0.7273\n",
      "Poids 1: 1.320 - F1 Score: 0.7273\n",
      "Poids 1: 1.330 - F1 Score: 0.7273\n",
      "Poids 1: 1.340 - F1 Score: 0.7273\n",
      "Poids 1: 1.350 - F1 Score: 0.7416\n",
      "Poids 1: 1.360 - F1 Score: 0.7416\n",
      "Poids 1: 1.370 - F1 Score: 0.7416\n",
      "Poids 1: 1.380 - F1 Score: 0.7416\n",
      "Poids 1: 1.390 - F1 Score: 0.7416\n",
      "Poids 1: 1.400 - F1 Score: 0.7333\n",
      "Poids 1: 1.410 - F1 Score: 0.7333\n",
      "Poids 1: 1.420 - F1 Score: 0.7333\n",
      "Poids 1: 1.430 - F1 Score: 0.7333\n",
      "Poids 1: 1.440 - F1 Score: 0.7473\n",
      "Poids 1: 1.450 - F1 Score: 0.7473\n",
      "Poids 1: 1.460 - F1 Score: 0.7473\n",
      "Poids 1: 1.470 - F1 Score: 0.7473\n",
      "Poids 1: 1.480 - F1 Score: 0.7391\n",
      "Poids 1: 1.490 - F1 Score: 0.7312\n",
      "Poids 1: 1.500 - F1 Score: 0.7312\n",
      "Poids 1: 1.510 - F1 Score: 0.7312\n",
      "Poids 1: 1.520 - F1 Score: 0.7312\n",
      "Poids 1: 1.530 - F1 Score: 0.7312\n",
      "Poids 1: 1.540 - F1 Score: 0.7312\n",
      "Poids 1: 1.550 - F1 Score: 0.7312\n",
      "Poids 1: 1.560 - F1 Score: 0.7312\n",
      "Poids 1: 1.570 - F1 Score: 0.7312\n",
      "Poids 1: 1.580 - F1 Score: 0.7447\n",
      "Poids 1: 1.590 - F1 Score: 0.7447\n",
      "Poids 1: 1.600 - F1 Score: 0.7447\n",
      "Poids 1: 1.610 - F1 Score: 0.7447\n",
      "Poids 1: 1.620 - F1 Score: 0.7447\n",
      "Poids 1: 1.630 - F1 Score: 0.7447\n",
      "Poids 1: 1.640 - F1 Score: 0.7447\n",
      "Poids 1: 1.650 - F1 Score: 0.7447\n",
      "Poids 1: 1.660 - F1 Score: 0.7292\n",
      "Poids 1: 1.670 - F1 Score: 0.7292\n",
      "Poids 1: 1.680 - F1 Score: 0.7292\n",
      "Poids 1: 1.690 - F1 Score: 0.7292\n",
      "Poids 1: 1.700 - F1 Score: 0.7216\n",
      "Poids 1: 1.710 - F1 Score: 0.7216\n",
      "Poids 1: 1.720 - F1 Score: 0.7143\n",
      "Poids 1: 1.730 - F1 Score: 0.7143\n",
      "Poids 1: 1.740 - F1 Score: 0.7143\n",
      "Poids 1: 1.750 - F1 Score: 0.7143\n",
      "Poids 1: 1.760 - F1 Score: 0.7071\n",
      "Poids 1: 1.770 - F1 Score: 0.7071\n",
      "Poids 1: 1.780 - F1 Score: 0.7200\n",
      "Poids 1: 1.790 - F1 Score: 0.7200\n",
      "Poids 1: 1.800 - F1 Score: 0.7200\n",
      "Poids 1: 1.810 - F1 Score: 0.7200\n",
      "Poids 1: 1.820 - F1 Score: 0.7327\n",
      "Poids 1: 1.830 - F1 Score: 0.7327\n",
      "Poids 1: 1.840 - F1 Score: 0.7327\n",
      "Poids 1: 1.850 - F1 Score: 0.7327\n",
      "Poids 1: 1.860 - F1 Score: 0.7255\n",
      "Poids 1: 1.870 - F1 Score: 0.7255\n",
      "Poids 1: 1.880 - F1 Score: 0.7255\n",
      "Poids 1: 1.890 - F1 Score: 0.7255\n",
      "Poids 1: 1.900 - F1 Score: 0.7255\n",
      "Poids 1: 1.910 - F1 Score: 0.7255\n",
      "Poids 1: 1.920 - F1 Score: 0.7255\n",
      "Poids 1: 1.930 - F1 Score: 0.7255\n",
      "Poids 1: 1.940 - F1 Score: 0.7255\n",
      "Poids 1: 1.950 - F1 Score: 0.7255\n",
      "Poids 1: 1.960 - F1 Score: 0.7255\n",
      "Poids 1: 1.970 - F1 Score: 0.7255\n",
      "Poids 1: 1.980 - F1 Score: 0.7255\n",
      "Poids 1: 1.990 - F1 Score: 0.7255\n",
      "\n",
      "🏆 Meilleur modèle SVM ultra-précis :\n",
      "F1 Score : 0.7473\n",
      "Poids optimaux : {0: 1, 1: np.float64(1.4400000000000004)}\n"
     ]
    }
   ],
   "source": [
    "weights_range = np.arange(1, 2, 0.01)  # précision très fine\n",
    "\n",
    "best_model = None\n",
    "best_f1_score = -np.inf\n",
    "best_weights = None\n",
    "\n",
    "for weight in weights_range:\n",
    "    class_weights = {0: 1, 1: weight}\n",
    "    svm = SVC(class_weight=class_weights, max_iter=10000, random_state=42, probability=True)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_val)\n",
    "    f1 = f1_score(y_val, y_pred, pos_label=1)\n",
    "\n",
    "    print(f\"Poids 1: {weight:.3f} - F1 Score: {f1:.4f}\")\n",
    "\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        svm_best_weights = svm\n",
    "        best_weights = class_weights\n",
    "\n",
    "print(\"\\n🏆 Meilleur modèle SVM ultra-précis :\")\n",
    "print(f\"F1 Score : {best_f1_score:.4f}\")\n",
    "print(f\"Poids optimaux : {best_weights}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
      "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
      "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
      "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
      "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
      "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
      "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
      "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
      "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
      "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
      "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
      "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
      "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
      "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
      "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
      "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
      "16            knn_balanced  0.715447   0.711274  0.715447          0.578313   \n",
      "17     logreg_best_weights  0.821138   0.819552  0.821138          0.738095   \n",
      "18   adaboost_best_weights  0.772358   0.815734  0.772358          0.730769   \n",
      "19        svm_best_weights  0.813008   0.819986  0.813008          0.747253   \n",
      "\n",
      "    F1 Score global weighted  \n",
      "0                   0.802499  \n",
      "1                   0.718675  \n",
      "2                   0.778541  \n",
      "3                   0.758723  \n",
      "4                   0.729328  \n",
      "5                   0.738346  \n",
      "6                   0.823837  \n",
      "7                   0.773504  \n",
      "8                   0.721993  \n",
      "9                   0.783727  \n",
      "10                  0.753123  \n",
      "11                  0.778541  \n",
      "12                  0.756098  \n",
      "13                  0.776626  \n",
      "14                  0.739837  \n",
      "15                  0.777565  \n",
      "16                  0.712923  \n",
      "17                  0.820113  \n",
      "18                  0.777630  \n",
      "19                  0.815129  \n"
     ]
    }
   ],
   "source": [
    "df_metrics_val = dataframe_metrics(svm_best_weights, X_val, y_val, model_name=\"svm_best_weights\", df_metrics=df_metrics_val)\n",
    "print(df_metrics_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best set of hyperparameters:  {'C': np.float64(1.408146893930582), 'penalty': 'l1'}\n",
      "Best score (mean CV accuracy):  0.6676080953546086\n",
      "Validation accuracy: 0.8130081300813008\n",
      "F1 score global (weighted): 0.8113495430171982\n",
      "F1 score class 1: 0.7228915662650602\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(\n",
    "    solver='saga',\n",
    "    max_iter=2000,\n",
    "    random_state=42,\n",
    "    class_weight={0: 1, 1: 1.25}\n",
    ")\n",
    "\n",
    "# Espace des hyperparamètres\n",
    "param_distributions = {\n",
    "    'C': loguniform(1e-3, 1e3),\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=logreg,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=cv,\n",
    "    scoring=make_scorer(f1_score, pos_label=1),\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Résultats\n",
    "print(\"Best set of hyperparameters: \", random_search.best_params_)\n",
    "print(\"Best score (mean CV accuracy): \", random_search.best_score_)\n",
    "\n",
    "logreg_best = random_search.best_estimator_\n",
    "y_pred = logreg_best.predict(X_val)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"F1 score global (weighted):\", f1_score(y_val, y_pred, average='weighted'))\n",
    "print(\"F1 score class 1:\", f1_score(y_val, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Meilleurs hyperparamètres : {'C': np.float64(1.0), 'penalty': 'l1'}\n",
      "Meilleur F1 (classe 1) moyenne en CV : 0.6676080953546086\n",
      "Validation accuracy: 0.8130081300813008\n",
      "F1 score global (weighted): 0.8113495430171982\n",
      "F1 score class 1: 0.7228915662650602\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logreg = LogisticRegression(\n",
    "    solver='saga',\n",
    "    max_iter=2000,\n",
    "    random_state=42,\n",
    "    class_weight={0: 1, 1: 1.25}\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.arange(1, 2, 0.01),\n",
    "\n",
    "}\n",
    "\n",
    "# F1 Score (positif sur la classe 1 sous-représentée)\n",
    "scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=logreg,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=scorer,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Résultats\n",
    "print(\"Meilleurs hyperparamètres :\", grid_search.best_params_)\n",
    "print(\"Meilleur F1 (classe 1) moyenne en CV :\", grid_search.best_score_)\n",
    "\n",
    "logreg_best = grid_search.best_estimator_\n",
    "y_pred = logreg_best.predict(X_val)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"F1 score global (weighted):\", f1_score(y_val, y_pred, average='weighted'))\n",
    "print(\"F1 score class 1:\", f1_score(y_val, y_pred, pos_label=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score class 1</th>\n",
       "      <th>F1 Score global weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>svm_best_weights</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.819986</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.815129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>logreg_best_weights</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.819552</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.820113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adaboost_best_weights</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.777630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear_svm</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.823837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svm_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.808667</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.777565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logreg_best</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.810731</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.811350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logistic_reg_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.792041</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.783727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adaboost_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.791050</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.776626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_reg</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.801960</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.802499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>random_forest_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgboost_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.756098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.775918</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.773504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision_tree_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.751839</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.753123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.758613</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.758723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm_balanced</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.739837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.737260</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.738346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.724255</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.718675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.727850</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.729328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.720801</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.721993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>knn_balanced</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.712923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
       "19        svm_best_weights  0.813008   0.819986  0.813008          0.747253   \n",
       "17     logreg_best_weights  0.821138   0.819552  0.821138          0.738095   \n",
       "18   adaboost_best_weights  0.772358   0.815734  0.772358          0.730769   \n",
       "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
       "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
       "20             logreg_best  0.813008   0.810731  0.813008          0.722892   \n",
       "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
       "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
       "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
       "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
       "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
       "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
       "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
       "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
       "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
       "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
       "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
       "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
       "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
       "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
       "16            knn_balanced  0.715447   0.711274  0.715447          0.578313   \n",
       "\n",
       "    F1 Score global weighted  \n",
       "19                  0.815129  \n",
       "17                  0.820113  \n",
       "18                  0.777630  \n",
       "6                   0.823837  \n",
       "15                  0.777565  \n",
       "20                  0.811350  \n",
       "9                   0.783727  \n",
       "13                  0.776626  \n",
       "0                   0.802499  \n",
       "11                  0.778541  \n",
       "2                   0.778541  \n",
       "12                  0.756098  \n",
       "7                   0.773504  \n",
       "10                  0.753123  \n",
       "3                   0.758723  \n",
       "14                  0.739837  \n",
       "5                   0.738346  \n",
       "1                   0.718675  \n",
       "4                   0.729328  \n",
       "8                   0.721993  \n",
       "16                  0.712923  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_val = dataframe_metrics(logreg_best, X_val, y_val, model_name=\"logreg_best\", df_metrics=df_metrics_val)\n",
    "df_metrics_val.sort_values(by='F1 Score class 1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utilisateur\\miniconda3\\envs\\ml_project\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres :\n",
      "{'n_estimators': 500, 'learning_rate': 0.5}\n",
      "Validation accuracy: 0.7398373983739838\n",
      "F1 score global (weighted): 0.7457979970635286\n",
      "F1 score class 1: 0.6981132075471698\n"
     ]
    }
   ],
   "source": [
    "class_weights = {0: 1, 1: 2.97}  # Poids manuels pour chaque classe\n",
    "\n",
    "# Calculer les poids d'échantillons à l'aide des poids de classe\n",
    "sample_weight = compute_sample_weight(class_weight=class_weights, y=y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Définir les hyperparamètres à tester dans RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [400, 500, 600, 700, 800],        # Nombre d'estimateurs\n",
    "    'learning_rate': [0.1, 0.5, 1.0, 1.5],      # Taux d'apprentissage\n",
    "}\n",
    "\n",
    "# Créer un modèle AdaBoost\n",
    "ada_model = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Configurer RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=ada_model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=30,               # Nombre d'itérations aléatoires\n",
    "    scoring=scorer,      # Mesure de la performance (ici, l'accuracy)\n",
    "    cv=cv,                    # Cross-validation avec 5 splits\n",
    "    random_state=42,         # Fixer la graine pour la reproductibilité\n",
    "    n_jobs=-1                # Utiliser tous les cœurs du CPU\n",
    ")\n",
    "\n",
    "# Effectuer la recherche aléatoire avec les poids d'échantillons\n",
    "random_search.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "# Afficher les meilleurs hyperparamètres trouvés\n",
    "print(\"Meilleurs hyperparamètres :\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "adaboost_best = random_search.best_estimator_\n",
    "y_pred = adaboost_best.predict(X_val)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"F1 score global (weighted):\", f1_score(y_val, y_pred, average='weighted'))\n",
    "print(\"F1 score class 1:\", f1_score(y_val, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamètres : {'learning_rate': np.float64(0.5), 'n_estimators': np.int64(550)}\n",
      "Meilleur F1 (classe 1) moyenne en CV : 0.6838950937801511\n",
      "Validation accuracy: 0.7398373983739838\n",
      "F1 score global (weighted): 0.7457979970635286\n",
      "F1 score class 1: 0.6981132075471698\n"
     ]
    }
   ],
   "source": [
    "class_weights = {0: 1, 1: 2.97}  # Poids manuels pour chaque classe\n",
    "\n",
    "# Calculer les poids d'échantillons à l'aide des poids de classe\n",
    "sample_weight = compute_sample_weight(class_weight=class_weights, y=y_train)\n",
    "\n",
    "# Définir les hyperparamètres à tester dans GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': np.arange(400, 600, 25),        # Nombre d'estimateurs\n",
    "    'learning_rate': np.arange(0.5, 1.5, 0.25),      # Taux d'apprentissage\n",
    "}\n",
    "\n",
    "# Créer un modèle AdaBoost\n",
    "ada_model = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Configurer GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ada_model, \n",
    "    param_grid=param_grid, \n",
    "    scoring=scorer,      # Mesure de la performance (ici, l'accuracy)\n",
    "    cv=cv,                    # Cross-validation avec 5 splits\n",
    "    n_jobs=-1,               # Utiliser tous les cœurs du CPU\n",
    ")\n",
    "\n",
    "# Effectuer la recherche en grille avec les poids d'échantillons\n",
    "grid_search.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Résultats\n",
    "print(\"Meilleurs hyperparamètres :\", grid_search.best_params_)\n",
    "print(\"Meilleur F1 (classe 1) moyenne en CV :\", grid_search.best_score_)\n",
    "\n",
    "adaboost_best = grid_search.best_estimator_\n",
    "y_pred = adaboost_best.predict(X_val)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"F1 score global (weighted):\", f1_score(y_val, y_pred, average='weighted'))\n",
    "print(\"F1 score class 1:\", f1_score(y_val, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score class 1</th>\n",
       "      <th>F1 Score global weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>svm_best_weights</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.819986</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.815129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>logreg_best_weights</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.819552</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.820113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adaboost_best_weights</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.777630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear_svm</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.823837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svm_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.808667</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.777565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logreg_best</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.810731</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.811350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logistic_reg_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.792041</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.783727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adaboost_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.791050</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.776626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_reg</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.801960</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.802499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>adaboost_best</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.790683</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.745798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>random_forest_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgboost_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.756098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.775918</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.773504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision_tree_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.751839</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.753123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.758613</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.758723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm_balanced</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.739837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.737260</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.738346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.724255</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.718675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.727850</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.729328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.720801</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.721993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>knn_balanced</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.712923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
       "19        svm_best_weights  0.813008   0.819986  0.813008          0.747253   \n",
       "17     logreg_best_weights  0.821138   0.819552  0.821138          0.738095   \n",
       "18   adaboost_best_weights  0.772358   0.815734  0.772358          0.730769   \n",
       "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
       "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
       "20             logreg_best  0.813008   0.810731  0.813008          0.722892   \n",
       "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
       "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
       "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
       "21           adaboost_best  0.739837   0.790683  0.739837          0.698113   \n",
       "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
       "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
       "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
       "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
       "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
       "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
       "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
       "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
       "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
       "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
       "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
       "16            knn_balanced  0.715447   0.711274  0.715447          0.578313   \n",
       "\n",
       "    F1 Score global weighted  \n",
       "19                  0.815129  \n",
       "17                  0.820113  \n",
       "18                  0.777630  \n",
       "6                   0.823837  \n",
       "15                  0.777565  \n",
       "20                  0.811350  \n",
       "9                   0.783727  \n",
       "13                  0.776626  \n",
       "0                   0.802499  \n",
       "21                  0.745798  \n",
       "2                   0.778541  \n",
       "11                  0.778541  \n",
       "12                  0.756098  \n",
       "7                   0.773504  \n",
       "10                  0.753123  \n",
       "3                   0.758723  \n",
       "14                  0.739837  \n",
       "5                   0.738346  \n",
       "1                   0.718675  \n",
       "4                   0.729328  \n",
       "8                   0.721993  \n",
       "16                  0.712923  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_val = dataframe_metrics(adaboost_best, X_val, y_val, model_name=\"adaboost_best\", df_metrics=df_metrics_val)\n",
    "df_metrics_val.sort_values(by='F1 Score class 1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Meilleurs hyperparamètres :\n",
      "{'kernel': 'rbf', 'gamma': 'scale', 'C': np.float64(5.264105263157894)}\n",
      "Validation accuracy: 0.7886178861788617\n",
      "F1 score global (weighted): 0.7906052393857272\n",
      "F1 score class 1: 0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "custom_weights = {0: 1, 1: 1.44}\n",
    "\n",
    "# Définir le modèle de base\n",
    "svm = SVC(class_weight=custom_weights, random_state=42, probability=True)\n",
    "\n",
    "# Définir l'espace de recherche\n",
    "param_dist = {\n",
    "    'C': np.linspace(0.001, 100, 20),  # de 0.001 à 100\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']  # utilisé pour certains kernels\n",
    "}\n",
    "\n",
    "# Scorer basé sur le F1 score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=svm,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,  # augmente si tu veux une recherche plus exhaustive\n",
    "    scoring=f1_scorer,\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Lancer la recherche\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Meilleurs hyperparamètres :\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "svm_best = random_search.best_estimator_\n",
    "y_pred = svm_best.predict(X_val)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"F1 score global (weighted):\", f1_score(y_val, y_pred, average='weighted'))\n",
    "print(\"F1 score class 1:\", f1_score(y_val, y_pred, pos_label=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Validation accuracy: 0.8048780487804879\n",
      "F1 score global (weighted): 0.8058603946223581\n",
      "F1 score class 1: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "custom_weights = {0: 1, 1: 1.44}\n",
    "\n",
    "# Définir le modèle de base\n",
    "svm = SVC(class_weight=custom_weights, random_state=42, probability=True)\n",
    "\n",
    "# Définir l'espace de recherche (plus restreint que RandomizedSearch, sinon c'est trop long)\n",
    "param_grid = {\n",
    "    'C':np.arange(4, 6, 0.1),  # de 0.001 à 100 avec un pas de 0.1\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']  # gamma n'est utile que pour 'rbf', mais pas grave ici\n",
    "}\n",
    "\n",
    "# Scorer basé sur le F1 score\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    scoring=f1_scorer,\n",
    "    cv=cv,  # défini plus haut dans ton script\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Lancer la recherche\n",
    "grid_search.fit(X_train, y_train)\n",
    "svm_best = grid_search.best_estimator_\n",
    "y_pred = svm_best.predict(X_val)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"F1 score global (weighted):\", f1_score(y_val, y_pred, average='weighted'))\n",
    "print(\"F1 score class 1:\", f1_score(y_val, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score class 1</th>\n",
       "      <th>F1 Score global weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>svm_best_weights</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.819986</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.815129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>logreg_best_weights</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.819552</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.820113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adaboost_best_weights</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.815734</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.777630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>svm_best</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.807282</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.805860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linear_svm</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.823837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svm_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.808667</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.777565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logreg_best</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.810731</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.811350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logistic_reg_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.792041</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.783727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adaboost_balanced</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.791050</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.776626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_reg</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.801960</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.802499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>adaboost_best</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.790683</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.745798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>random_forest_balanced</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.778541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgboost_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>0.756098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.775918</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.773504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>decision_tree_balanced</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.751839</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.753123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaboost</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.758613</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.758723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lgbm_balanced</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.739837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lgbm</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.737260</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.738346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.724255</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.718675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.727850</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.729328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.720801</td>\n",
       "      <td>0.723577</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.721993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>knn_balanced</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.711274</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.712923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
       "19        svm_best_weights  0.813008   0.819986  0.813008          0.747253   \n",
       "17     logreg_best_weights  0.821138   0.819552  0.821138          0.738095   \n",
       "18   adaboost_best_weights  0.772358   0.815734  0.772358          0.730769   \n",
       "22                svm_best  0.804878   0.807282  0.804878          0.727273   \n",
       "6               linear_svm  0.829268   0.828688  0.829268          0.727273   \n",
       "15            svm_balanced  0.772358   0.808667  0.772358          0.725490   \n",
       "20             logreg_best  0.813008   0.810731  0.813008          0.722892   \n",
       "9    logistic_reg_balanced  0.780488   0.792041  0.780488          0.709677   \n",
       "13       adaboost_balanced  0.772358   0.791050  0.772358          0.708333   \n",
       "0             logistic_reg  0.804878   0.801960  0.804878          0.707317   \n",
       "21           adaboost_best  0.739837   0.790683  0.739837          0.698113   \n",
       "2            random_forest  0.780488   0.777579  0.780488          0.674699   \n",
       "11  random_forest_balanced  0.780488   0.777579  0.780488          0.674699   \n",
       "12        xgboost_balanced  0.756098   0.756098  0.756098          0.651163   \n",
       "7                      svm  0.780488   0.775918  0.780488          0.649351   \n",
       "10  decision_tree_balanced  0.756098   0.751839  0.756098          0.634146   \n",
       "3                 adaboost  0.764228   0.758613  0.764228          0.632911   \n",
       "14           lgbm_balanced  0.739837   0.739837  0.739837          0.627907   \n",
       "5                     lgbm  0.739837   0.737260  0.739837          0.619048   \n",
       "1            decision_tree  0.715447   0.724255  0.715447          0.615385   \n",
       "4                  xgboost  0.731707   0.727850  0.731707          0.602410   \n",
       "8                      KNN  0.723577   0.720801  0.723577          0.595238   \n",
       "16            knn_balanced  0.715447   0.711274  0.715447          0.578313   \n",
       "\n",
       "    F1 Score global weighted  \n",
       "19                  0.815129  \n",
       "17                  0.820113  \n",
       "18                  0.777630  \n",
       "22                  0.805860  \n",
       "6                   0.823837  \n",
       "15                  0.777565  \n",
       "20                  0.811350  \n",
       "9                   0.783727  \n",
       "13                  0.776626  \n",
       "0                   0.802499  \n",
       "21                  0.745798  \n",
       "2                   0.778541  \n",
       "11                  0.778541  \n",
       "12                  0.756098  \n",
       "7                   0.773504  \n",
       "10                  0.753123  \n",
       "3                   0.758723  \n",
       "14                  0.739837  \n",
       "5                   0.738346  \n",
       "1                   0.718675  \n",
       "4                   0.729328  \n",
       "8                   0.721993  \n",
       "16                  0.712923  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics_val = dataframe_metrics(svm_best, X_val, y_val, model_name=\"svm_best\", df_metrics=df_metrics_val)\n",
    "df_metrics_val.sort_values(by='F1 Score class 1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validar con el dataset de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_test= pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score class 1', 'F1 Score global weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0  adaboost_best  0.766234   0.802387  0.766234           0.71875   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                   0.77151  \n",
      "                   Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0          adaboost_best  0.766234   0.802387  0.766234           0.71875   \n",
      "1  adaboost_best_weights  0.740260   0.776676  0.740260           0.68750   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.771510  \n",
      "1                  0.746122  \n",
      "                   Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0          adaboost_best  0.766234   0.802387  0.766234          0.718750   \n",
      "1  adaboost_best_weights  0.740260   0.776676  0.740260          0.687500   \n",
      "2      adaboost_balanced  0.753247   0.755500  0.753247          0.654545   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.771510  \n",
      "1                  0.746122  \n",
      "2                  0.754244  \n",
      "                   Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0          adaboost_best  0.766234   0.802387  0.766234          0.718750   \n",
      "1  adaboost_best_weights  0.740260   0.776676  0.740260          0.687500   \n",
      "2      adaboost_balanced  0.753247   0.755500  0.753247          0.654545   \n",
      "3            logreg_best  0.714286   0.711969  0.714286          0.584906   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.771510  \n",
      "1                  0.746122  \n",
      "2                  0.754244  \n",
      "3                  0.713005  \n",
      "                   Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0          adaboost_best  0.766234   0.802387  0.766234          0.718750   \n",
      "1  adaboost_best_weights  0.740260   0.776676  0.740260          0.687500   \n",
      "2      adaboost_balanced  0.753247   0.755500  0.753247          0.654545   \n",
      "3            logreg_best  0.714286   0.711969  0.714286          0.584906   \n",
      "4    logreg_best_weights  0.707792   0.706576  0.707792          0.579439   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.771510  \n",
      "1                  0.746122  \n",
      "2                  0.754244  \n",
      "3                  0.713005  \n",
      "4                  0.707154  \n",
      "                   Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0          adaboost_best  0.766234   0.802387  0.766234          0.718750   \n",
      "1  adaboost_best_weights  0.740260   0.776676  0.740260          0.687500   \n",
      "2      adaboost_balanced  0.753247   0.755500  0.753247          0.654545   \n",
      "3            logreg_best  0.714286   0.711969  0.714286          0.584906   \n",
      "4    logreg_best_weights  0.707792   0.706576  0.707792          0.579439   \n",
      "5  logistic_reg_balanced  0.733766   0.750299  0.733766          0.655462   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.771510  \n",
      "1                  0.746122  \n",
      "2                  0.754244  \n",
      "3                  0.713005  \n",
      "4                  0.707154  \n",
      "5                  0.738324  \n",
      "                   Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0          adaboost_best  0.766234   0.802387  0.766234          0.718750   \n",
      "1  adaboost_best_weights  0.740260   0.776676  0.740260          0.687500   \n",
      "2      adaboost_balanced  0.753247   0.755500  0.753247          0.654545   \n",
      "3            logreg_best  0.714286   0.711969  0.714286          0.584906   \n",
      "4    logreg_best_weights  0.707792   0.706576  0.707792          0.579439   \n",
      "5  logistic_reg_balanced  0.733766   0.750299  0.733766          0.655462   \n",
      "6               svm_best  0.701299   0.694621  0.701299          0.549020   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.771510  \n",
      "1                  0.746122  \n",
      "2                  0.754244  \n",
      "3                  0.713005  \n",
      "4                  0.707154  \n",
      "5                  0.738324  \n",
      "6                  0.696863  \n",
      "                   Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0          adaboost_best  0.766234   0.802387  0.766234          0.718750   \n",
      "1  adaboost_best_weights  0.740260   0.776676  0.740260          0.687500   \n",
      "2      adaboost_balanced  0.753247   0.755500  0.753247          0.654545   \n",
      "3            logreg_best  0.714286   0.711969  0.714286          0.584906   \n",
      "4    logreg_best_weights  0.707792   0.706576  0.707792          0.579439   \n",
      "5  logistic_reg_balanced  0.733766   0.750299  0.733766          0.655462   \n",
      "6               svm_best  0.701299   0.694621  0.701299          0.549020   \n",
      "7       svm_best_weights  0.681818   0.689157  0.681818          0.566372   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.771510  \n",
      "1                  0.746122  \n",
      "2                  0.754244  \n",
      "3                  0.713005  \n",
      "4                  0.707154  \n",
      "5                  0.738324  \n",
      "6                  0.696863  \n",
      "7                  0.684778  \n",
      "                   Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
      "0          adaboost_best  0.766234   0.802387  0.766234          0.718750   \n",
      "1  adaboost_best_weights  0.740260   0.776676  0.740260          0.687500   \n",
      "2      adaboost_balanced  0.753247   0.755500  0.753247          0.654545   \n",
      "3            logreg_best  0.714286   0.711969  0.714286          0.584906   \n",
      "4    logreg_best_weights  0.707792   0.706576  0.707792          0.579439   \n",
      "5  logistic_reg_balanced  0.733766   0.750299  0.733766          0.655462   \n",
      "6               svm_best  0.701299   0.694621  0.701299          0.549020   \n",
      "7       svm_best_weights  0.681818   0.689157  0.681818          0.566372   \n",
      "8           svm_balanced  0.688312   0.707792  0.688312          0.600000   \n",
      "\n",
      "   F1 Score global weighted  \n",
      "0                  0.771510  \n",
      "1                  0.746122  \n",
      "2                  0.754244  \n",
      "3                  0.713005  \n",
      "4                  0.707154  \n",
      "5                  0.738324  \n",
      "6                  0.696863  \n",
      "7                  0.684778  \n",
      "8                  0.693949  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\AppData\\Local\\Temp\\ipykernel_36364\\679556832.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metrics = pd.concat([df_metrics, results], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score class 1</th>\n",
       "      <th>F1 Score global weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adaboost_best</td>\n",
       "      <td>0.766234</td>\n",
       "      <td>0.802387</td>\n",
       "      <td>0.766234</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.771510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaboost_best_weights</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.776676</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.746122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logistic_reg_balanced</td>\n",
       "      <td>0.733766</td>\n",
       "      <td>0.750299</td>\n",
       "      <td>0.733766</td>\n",
       "      <td>0.655462</td>\n",
       "      <td>0.738324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adaboost_balanced</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.755500</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.754244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>svm_balanced</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.693949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg_best</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.711969</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.713005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logreg_best_weights</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.706576</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.707154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>svm_best_weights</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.689157</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.684778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>svm_best</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.694621</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.696863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy  Precision    Recall  F1 Score class 1  \\\n",
       "0          adaboost_best  0.766234   0.802387  0.766234          0.718750   \n",
       "1  adaboost_best_weights  0.740260   0.776676  0.740260          0.687500   \n",
       "5  logistic_reg_balanced  0.733766   0.750299  0.733766          0.655462   \n",
       "2      adaboost_balanced  0.753247   0.755500  0.753247          0.654545   \n",
       "8           svm_balanced  0.688312   0.707792  0.688312          0.600000   \n",
       "3            logreg_best  0.714286   0.711969  0.714286          0.584906   \n",
       "4    logreg_best_weights  0.707792   0.706576  0.707792          0.579439   \n",
       "7       svm_best_weights  0.681818   0.689157  0.681818          0.566372   \n",
       "6               svm_best  0.701299   0.694621  0.701299          0.549020   \n",
       "\n",
       "   F1 Score global weighted  \n",
       "0                  0.771510  \n",
       "1                  0.746122  \n",
       "5                  0.738324  \n",
       "2                  0.754244  \n",
       "8                  0.693949  \n",
       "3                  0.713005  \n",
       "4                  0.707154  \n",
       "7                  0.684778  \n",
       "6                  0.696863  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "    \"adaboost_best\": adaboost_best,\n",
    "    \"adaboost_best_weights\": adaboost_best_weights,\n",
    "    \"adaboost_balanced\": adaboost_balanced,\n",
    "    \"logreg_best\": logreg_best,\n",
    "    \"logreg_best_weights\": logreg_best_weights,\n",
    "    \"logistic_reg_balanced\": logistic_reg_balanced,\n",
    "    \"svm_best\": svm_best,\n",
    "    \"svm_best_weights\": svm_best_weights,\n",
    "    \"svm_balanced\": svm_balanced\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    df_metrics_test = dataframe_metrics(model, X_test, y_test, model_name=model_name, df_metrics=df_metrics_test)\n",
    "    print(df_metrics_test)\n",
    "df_metrics_test.sort_values(by='F1 Score class 1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟠 AdaBoost - best - AUC: 0.8284\n",
      "🟠 AdaBoost - best_weights - AUC: 0.8138\n",
      "🟠 AdaBoost - balanced - AUC: 0.8286\n",
      "🔺 LogReg - best - AUC: 0.8056\n",
      "🔺 LogReg - best_weights - AUC: 0.8059\n",
      "🔺 LogReg - balanced - AUC: 0.8065\n",
      "🔷 SVM - best - AUC: 0.8009\n",
      "🔷 SVM - best_weights - AUC: 0.7880\n",
      "🔷 SVM - balanced - AUC: 0.7784\n",
      "\n",
      "🏆 Résultats AUC :\n",
      "                     Model       AUC\n",
      "2      AdaBoost - balanced  0.828611\n",
      "0          AdaBoost - best  0.828426\n",
      "1  AdaBoost - best_weights  0.813796\n",
      "5        LogReg - balanced  0.806481\n",
      "4    LogReg - best_weights  0.805926\n",
      "3            LogReg - best  0.805556\n",
      "6               SVM - best  0.800926\n",
      "7       SVM - best_weights  0.787963\n",
      "8           SVM - balanced  0.778426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\AppData\\Local\\Temp\\ipykernel_36364\\646342652.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_auc = pd.concat([df_auc, pd.DataFrame({'Model': [name], 'AUC': [auc_ada]})], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Création du DataFrame pour stocker les résultats\n",
    "df_auc = pd.DataFrame(columns=['Model', 'AUC'])\n",
    "\n",
    "# === ADA BOOST ===\n",
    "adaboost_models = [adaboost_best, adaboost_best_weights, adaboost_balanced]\n",
    "adaboost_names = [\"AdaBoost - best\", \"AdaBoost - best_weights\", \"AdaBoost - balanced\"]\n",
    "\n",
    "for name, model in zip(adaboost_names, adaboost_models):\n",
    "    y_probs_ada = model.predict_proba(X_test)[:, 1]\n",
    "    auc_ada = roc_auc_score(y_test, y_probs_ada)\n",
    "    print(f\"🟠 {name} - AUC: {auc_ada:.4f}\")\n",
    "    # Ajout des résultats dans le DataFrame\n",
    "    df_auc = pd.concat([df_auc, pd.DataFrame({'Model': [name], 'AUC': [auc_ada]})], ignore_index=True)\n",
    "\n",
    "# === REGRESSION LOGISTIQUE ===\n",
    "regression_logistic_models = [logreg_best, logreg_best_weights, logistic_reg_balanced]\n",
    "logreg_names = [\"LogReg - best\", \"LogReg - best_weights\", \"LogReg - balanced\"]\n",
    "\n",
    "for name, model in zip(logreg_names, regression_logistic_models):\n",
    "    y_probs_logreg = model.predict_proba(X_test)[:, 1]\n",
    "    auc_logreg = roc_auc_score(y_test, y_probs_logreg)\n",
    "    print(f\"🔺 {name} - AUC: {auc_logreg:.4f}\")\n",
    "    # Ajout des résultats dans le DataFrame\n",
    "    df_auc = pd.concat([df_auc, pd.DataFrame({'Model': [name], 'AUC': [auc_logreg]})], ignore_index=True)\n",
    "\n",
    "# === SVM ===\n",
    "svm_models = [svm_best, svm_best_weights, svm_balanced]\n",
    "svm_names = [\"SVM - best\", \"SVM - best_weights\", \"SVM - balanced\"]\n",
    "\n",
    "for name, model in zip(svm_names, svm_models):\n",
    "    y_probs_svm = model.predict_proba(X_test)[:, 1]\n",
    "    auc_svm = roc_auc_score(y_test, y_probs_svm)\n",
    "    print(f\"🔷 {name} - AUC: {auc_svm:.4f}\")\n",
    "    # Ajout des résultats dans le DataFrame\n",
    "    df_auc = pd.concat([df_auc, pd.DataFrame({'Model': [name], 'AUC': [auc_svm]})], ignore_index=True)\n",
    "\n",
    "# Affichage du DataFrame complet et trié par AUC\n",
    "print(\"\\n🏆 Résultats AUC :\")\n",
    "df_auc_sorted = df_auc.sort_values(by='AUC', ascending=False)\n",
    "print(df_auc_sorted)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
